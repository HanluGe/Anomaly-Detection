{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230eab1a",
   "metadata": {},
   "source": [
    "## HW4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c01e0e",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd34ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training GAN for 0050 =====\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.117348]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.269022]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.107848]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.259430]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.107934]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.259470]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 1.107752]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.259352]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.106480]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.258148]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 1.105823]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249934][G train loss: 1.257526]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 1.105642]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249900][G train loss: 1.257354]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 1.105422]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249856][G train loss: 1.257124]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249829][G eval loss: 1.104922]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249791][G train loss: 1.256597]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249755][G eval loss: 1.103952]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249704][G train loss: 1.255591]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249656][G eval loss: 1.102809]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249587][G train loss: 1.254401]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249521][G eval loss: 1.101852]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249428][G train loss: 1.253398]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249352][G eval loss: 1.101090]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249232][G train loss: 1.252594]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249129][G eval loss: 1.100708]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248976][G train loss: 1.252176]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248830][G eval loss: 1.100744]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248635][G train loss: 1.252164]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248435][G eval loss: 1.101221]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248189][G train loss: 1.252581]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247928][G eval loss: 1.102093]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247619][G train loss: 1.253393]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247369][G eval loss: 1.102612]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246991][G train loss: 1.253839]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246538][G eval loss: 1.101883]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246085][G train loss: 1.253011]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245511][G eval loss: 1.100232]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244968][G train loss: 1.251205]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244334][G eval loss: 1.097981]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243709][G train loss: 1.248718]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242917][G eval loss: 1.095047]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242209][G train loss: 1.245433]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241217][G eval loss: 1.090575]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240423][G train loss: 1.240480]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239243][G eval loss: 1.083274]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238351][G train loss: 1.232568]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236957][G eval loss: 1.072087]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235960][G train loss: 1.220541]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234424][G eval loss: 1.055990]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.233300][G train loss: 1.203394]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231631][G eval loss: 1.034966]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.230363][G train loss: 1.180851]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.228569][G eval loss: 1.008484]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.227142][G train loss: 1.152276]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.225200][G eval loss: 0.974675]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.223656][G train loss: 1.116490]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.221947][G eval loss: 0.932603]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.220142][G train loss: 1.073285]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.219502][G eval loss: 0.883689]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.217126][G train loss: 1.024095]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.218658][G eval loss: 0.832171]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.215622][G train loss: 0.971668]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.221429][G eval loss: 0.781474]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.217015][G train loss: 0.918553]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.228506][G eval loss: 0.735036]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.222167][G train loss: 0.866600]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.238255][G eval loss: 0.696277]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.230798][G train loss: 0.817407]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250014][G eval loss: 0.665469]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.242000][G train loss: 0.774323]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.263483][G eval loss: 0.639508]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.254340][G train loss: 0.738828]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.277342][G eval loss: 0.618302]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.267042][G train loss: 0.709377]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.290981][G eval loss: 0.597570]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.279433][G train loss: 0.682120]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.302760][G eval loss: 0.575034]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.290966][G train loss: 0.655376]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.312883][G eval loss: 0.549432]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.301308][G train loss: 0.628705]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.321247][G eval loss: 0.525353]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.309518][G train loss: 0.603509]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.327861][G eval loss: 0.505094]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.315519][G train loss: 0.582509]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.332504][G eval loss: 0.488767]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.319466][G train loss: 0.565839]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.334148][G eval loss: 0.477095]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.321850][G train loss: 0.553709]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.334225][G eval loss: 0.468145]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.323206][G train loss: 0.544595]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.333293][G eval loss: 0.461388]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.323749][G train loss: 0.537106]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.331393][G eval loss: 0.457375]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.322852][G train loss: 0.532372]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.328569][G eval loss: 0.455950]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.320276][G train loss: 0.530057]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.325559][G eval loss: 0.455366]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.317726][G train loss: 0.527674]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.322156][G eval loss: 0.454873]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.315422][G train loss: 0.524987]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.318218][G eval loss: 0.453617]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.312949][G train loss: 0.521860]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.314182][G eval loss: 0.450924]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.309992][G train loss: 0.517351]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.309999][G eval loss: 0.446332]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.307085][G train loss: 0.510987]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.304943][G eval loss: 0.443888]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.303634][G train loss: 0.506479]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.299970][G eval loss: 0.441739]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299643][G train loss: 0.501919]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.295039][G eval loss: 0.441201]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.295083][G train loss: 0.499082]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.290655][G eval loss: 0.443324]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.290636][G train loss: 0.499080]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.286974][G eval loss: 0.443465]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.286837][G train loss: 0.498879]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.283545][G eval loss: 0.444566]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.283169][G train loss: 0.500298]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.279999][G eval loss: 0.445908]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.279103][G train loss: 0.502009]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.276858][G eval loss: 0.445695]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.275081][G train loss: 0.502841]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.274115][G eval loss: 0.446005]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.271605][G train loss: 0.504608]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.271596][G eval loss: 0.446998]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.269004][G train loss: 0.506768]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.269076][G eval loss: 0.448190]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.266708][G train loss: 0.508424]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.266521][G eval loss: 0.449291]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.264536][G train loss: 0.509336]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.263590][G eval loss: 0.449091]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.262477][G train loss: 0.509300]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.260577][G eval loss: 0.447475]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.260595][G train loss: 0.508215]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.257835][G eval loss: 0.446247]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.258982][G train loss: 0.507135]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.256090][G eval loss: 0.446100]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.257574][G train loss: 0.507109]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.254893][G eval loss: 0.445716]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.256320][G train loss: 0.507197]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.253912][G eval loss: 0.445059]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.255211][G train loss: 0.506519]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.253114][G eval loss: 0.445002]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.254254][G train loss: 0.505375]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.252473][G eval loss: 0.446369]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.253438][G train loss: 0.504807]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.251914][G eval loss: 0.448215]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "===== Training GAN for 0056 =====\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.980660]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.083180]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.975456]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.078601]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.971864]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.075342]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.971814]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.075296]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.973313]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.076672]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.974509]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.077710]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.974701]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249940][G train loss: 1.077758]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249938][G eval loss: 0.973502]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076436]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.971456]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.074301]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 0.969577]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249865][G train loss: 1.072361]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249866][G eval loss: 0.968497]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249829][G train loss: 1.071252]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.967944]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249782][G train loss: 1.070699]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249777][G eval loss: 0.967536]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249720][G train loss: 1.070304]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249711][G eval loss: 0.967422]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249641][G train loss: 1.070208]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249632][G eval loss: 0.967517]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249548][G train loss: 1.070334]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249541][G eval loss: 0.967339]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249442][G train loss: 1.070200]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249440][G eval loss: 0.966562]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249326][G train loss: 1.069419]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249326][G eval loss: 0.964654]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249200][G train loss: 1.067420]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249204][G eval loss: 0.960490]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249075][G train loss: 1.063098]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249069][G eval loss: 0.951554]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248952][G train loss: 1.053839]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248940][G eval loss: 0.934691]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248867][G train loss: 1.036220]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248901][G eval loss: 0.913240]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248916][G train loss: 1.013468]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249012][G eval loss: 0.900804]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249111][G train loss: 0.999367]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249110][G eval loss: 0.904692]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249231][G train loss: 1.001020]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.248946][G eval loss: 0.900952]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249142][G train loss: 0.995242]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.248488][G eval loss: 0.882598]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.248896][G train loss: 0.974309]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.248012][G eval loss: 0.858508]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.248526][G train loss: 0.948989]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.247573][G eval loss: 0.838108]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.248162][G train loss: 0.929007]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.247278][G eval loss: 0.816444]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.247924][G train loss: 0.907744]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.247154][G eval loss: 0.786735]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.247891][G train loss: 0.877355]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.247397][G eval loss: 0.748519]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.248266][G train loss: 0.840046]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.248070][G eval loss: 0.711532]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.248912][G train loss: 0.800759]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.248689][G eval loss: 0.675359]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249272][G train loss: 0.762817]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.248681][G eval loss: 0.634926]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.249293][G train loss: 0.719836]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.248150][G eval loss: 0.581364]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248780][G train loss: 0.667915]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.247107][G eval loss: 0.530172]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.247631][G train loss: 0.615378]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.245213][G eval loss: 0.511087]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.245343][G train loss: 0.594116]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.243898][G eval loss: 0.535064]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.242972][G train loss: 0.625966]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.244054][G eval loss: 0.578054]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.242212][G train loss: 0.677520]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.245452][G eval loss: 0.617088]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.242844][G train loss: 0.704916]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.248241][G eval loss: 0.614690]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.244810][G train loss: 0.695443]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.252632][G eval loss: 0.582589]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.248001][G train loss: 0.667134]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.256386][G eval loss: 0.564990]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.251460][G train loss: 0.640003]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.258983][G eval loss: 0.566527]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.253893][G train loss: 0.634402]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.260693][G eval loss: 0.564644]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.255172][G train loss: 0.634009]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.261423][G eval loss: 0.554116]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.255643][G train loss: 0.625716]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.261239][G eval loss: 0.538302]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.255424][G train loss: 0.611935]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.259804][G eval loss: 0.523859]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.254530][G train loss: 0.600399]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.257379][G eval loss: 0.516479]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.253272][G train loss: 0.594538]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.256147][G eval loss: 0.513956]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.251923][G train loss: 0.592473]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.255167][G eval loss: 0.514866]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.251343][G train loss: 0.590985]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.254492][G eval loss: 0.514660]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.250978][G train loss: 0.589785]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.254063][G eval loss: 0.509897]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.250745][G train loss: 0.585879]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.253776][G eval loss: 0.501001]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.250685][G train loss: 0.577415]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.253507][G eval loss: 0.489221]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.250692][G train loss: 0.566382]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.253193][G eval loss: 0.481979]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.250647][G train loss: 0.558270]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.252827][G eval loss: 0.481994]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.250517][G train loss: 0.557839]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.252594][G eval loss: 0.485138]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.250374][G train loss: 0.562464]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.252443][G eval loss: 0.487020]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.250150][G train loss: 0.566145]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.252304][G eval loss: 0.484233]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.249931][G train loss: 0.564961]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.252153][G eval loss: 0.478397]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.249737][G train loss: 0.558417]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.252086][G eval loss: 0.470607]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.249556][G train loss: 0.550406]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.252051][G eval loss: 0.463682]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.249408][G train loss: 0.543487]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.252041][G eval loss: 0.455410]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.249252][G train loss: 0.535301]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.251970][G eval loss: 0.445360]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.249016][G train loss: 0.524311]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.251701][G eval loss: 0.437013]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.248667][G train loss: 0.514887]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.251231][G eval loss: 0.435543]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.248220][G train loss: 0.512076]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.250745][G eval loss: 0.435805]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.247681][G train loss: 0.511667]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.250193][G eval loss: 0.433646]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.247054][G train loss: 0.508309]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.249591][G eval loss: 0.428734]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.246418][G train loss: 0.501576]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.248997][G eval loss: 0.424169]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.245869][G train loss: 0.494497]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.248459][G eval loss: 0.422996]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.245302][G train loss: 0.490862]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.247896][G eval loss: 0.423845]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.244702][G train loss: 0.489711]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.247386][G eval loss: 0.423571]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.244060][G train loss: 0.488092]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.246866][G eval loss: 0.421075]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.243385][G train loss: 0.484489]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.246165][G eval loss: 0.420909]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.242692][G train loss: 0.480754]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.245302][G eval loss: 0.422815]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.241801][G train loss: 0.479334]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.244514][G eval loss: 0.424385]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "===== Training GAN for 2330 =====\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.484525]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 0.448827]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.479276]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 0.443842]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.479288]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 0.444022]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.479723]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249974][G train loss: 0.444383]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.480315]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249960][G train loss: 0.444815]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 0.480766]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249932][G train loss: 0.445111]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 0.479945]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249895][G train loss: 0.444212]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249879][G eval loss: 0.478011]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249845][G train loss: 0.442314]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249827][G eval loss: 0.475982]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249780][G train loss: 0.440378]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249756][G eval loss: 0.474508]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249691][G train loss: 0.438966]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249653][G eval loss: 0.473451]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249563][G train loss: 0.437884]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249504][G eval loss: 0.473018]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249382][G train loss: 0.437387]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249310][G eval loss: 0.473312]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249139][G train loss: 0.437638]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249056][G eval loss: 0.474171]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248815][G train loss: 0.438479]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248715][G eval loss: 0.475124]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248392][G train loss: 0.439442]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248359][G eval loss: 0.476063]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247936][G train loss: 0.440412]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247913][G eval loss: 0.477107]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247366][G train loss: 0.441496]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247357][G eval loss: 0.478206]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246656][G train loss: 0.442616]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246818][G eval loss: 0.479134]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245927][G train loss: 0.443540]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.246035][G eval loss: 0.479696]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244923][G train loss: 0.444131]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244981][G eval loss: 0.480510]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243604][G train loss: 0.444977]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243711][G eval loss: 0.481497]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242011][G train loss: 0.445954]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.242201][G eval loss: 0.482513]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240130][G train loss: 0.446940]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.240406][G eval loss: 0.483647]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237910][G train loss: 0.448038]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.238285][G eval loss: 0.484812]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n"
     ]
    }
   ],
   "source": [
    "from LOB_GAN_training_raw import train_lob_gan\n",
    "\n",
    "stock_list = [\"0050\", \"0056\", \"2330\"]\n",
    "\n",
    "lr_config = {\n",
    "    \"0050\": {\"lr_g\": 0.004, \"lr_d\": 0.0008},\n",
    "    \"0056\": {\"lr_g\": 0.006, \"lr_d\": 0.0005},\n",
    "    \"2330\": {\"lr_g\": 0.006, \"lr_d\": 0.0008},\n",
    "}\n",
    "\n",
    "batch_config = {\n",
    "    \"0050\": 50,\n",
    "    \"0056\": 50,\n",
    "    \"2330\": 50,   \n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"===== Training GAN for {stock} =====\")\n",
    "    cfg = lr_config[stock]\n",
    "    bs = batch_config[stock]\n",
    "    res = train_lob_gan(\n",
    "        stock=stock,\n",
    "        lr_g=cfg[\"lr_g\"],\n",
    "        lr_d=cfg[\"lr_d\"],\n",
    "        batch_size=bs,\n",
    "        seed=307,\n",
    "    )\n",
    "    results[stock] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d028d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Grid search for 0050 ==========\n",
      "\n",
      "----- 0050: lr_g=0.002, lr_d=0.0003 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.129697]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.281367]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.119200]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249996][G train loss: 1.270856]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.112270]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.263892]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 1.108468]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.260047]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.107101]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.258657]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 1.106778]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249979][G train loss: 1.258338]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 1.106335]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.257919]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 1.105613]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.257228]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 1.104867]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249959][G train loss: 1.256511]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249960][G eval loss: 1.104331]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249951][G train loss: 1.255991]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 1.104054]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249941][G train loss: 1.255722]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249944][G eval loss: 1.103984]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249930][G train loss: 1.255655]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249934][G eval loss: 1.104033]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249918][G train loss: 1.255704]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249923][G eval loss: 1.104125]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249904][G train loss: 1.255794]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249910][G eval loss: 1.104210]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249888][G train loss: 1.255873]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 1.104255]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249870][G train loss: 1.255911]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249877][G eval loss: 1.104245]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249849][G train loss: 1.255891]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249856][G eval loss: 1.104176]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249824][G train loss: 1.255811]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 1.104051]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249796][G train loss: 1.255675]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249804][G eval loss: 1.103889]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249763][G train loss: 1.255499]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249772][G eval loss: 1.103713]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249726][G train loss: 1.255309]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249736][G eval loss: 1.103534]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249685][G train loss: 1.255116]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249696][G eval loss: 1.103351]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249638][G train loss: 1.254921]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249652][G eval loss: 1.103182]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249587][G train loss: 1.254739]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.249603][G eval loss: 1.102969]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249530][G train loss: 1.254512]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.249549][G eval loss: 1.102802]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249468][G train loss: 1.254334]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.249489][G eval loss: 1.102696]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.249398][G train loss: 1.254216]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.249421][G eval loss: 1.102644]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.249320][G train loss: 1.254158]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.249344][G eval loss: 1.102632]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.249231][G train loss: 1.254131]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.249257][G eval loss: 1.102636]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.249131][G train loss: 1.254108]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.249160][G eval loss: 1.102631]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.249020][G train loss: 1.254074]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.249051][G eval loss: 1.102600]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.248894][G train loss: 1.254010]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.248918][G eval loss: 1.102565]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.248743][G train loss: 1.253945]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.248767][G eval loss: 1.102525]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.248574][G train loss: 1.253872]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.248597][G eval loss: 1.102442]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248385][G train loss: 1.253749]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.248409][G eval loss: 1.102264]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.248177][G train loss: 1.253525]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.248205][G eval loss: 1.101949]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.247949][G train loss: 1.253157]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.247982][G eval loss: 1.101453]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.247700][G train loss: 1.252599]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.247740][G eval loss: 1.100741]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.247430][G train loss: 1.251815]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.247478][G eval loss: 1.099796]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.247140][G train loss: 1.250784]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.247236][G eval loss: 1.098550]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.246867][G train loss: 1.249424]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.246963][G eval loss: 1.096868]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.246561][G train loss: 1.247609]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.246649][G eval loss: 1.094705]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.246213][G train loss: 1.245307]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.246291][G eval loss: 1.091948]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.245823][G train loss: 1.242396]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.245915][G eval loss: 1.088431]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.245416][G train loss: 1.238699]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.245526][G eval loss: 1.084131]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.245000][G train loss: 1.234173]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.245100][G eval loss: 1.079002]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.244548][G train loss: 1.228723]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.244641][G eval loss: 1.072867]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.244062][G train loss: 1.222148]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.244151][G eval loss: 1.065506]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.243544][G train loss: 1.214203]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.243635][G eval loss: 1.056672]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.243002][G train loss: 1.204590]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.243102][G eval loss: 1.046074]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.242446][G train loss: 1.193056]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.242561][G eval loss: 1.033595]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.241886][G train loss: 1.179580]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.242016][G eval loss: 1.019217]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.241332][G train loss: 1.164185]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.241474][G eval loss: 1.002810]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.240787][G train loss: 1.146777]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.240933][G eval loss: 0.984373]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.240256][G train loss: 1.127406]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.240394][G eval loss: 0.964091]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.239740][G train loss: 1.106297]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.239846][G eval loss: 0.942277]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.239243][G train loss: 1.083757]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.239295][G eval loss: 0.919417]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.238782][G train loss: 1.060123]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.238752][G eval loss: 0.896124]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.238353][G train loss: 1.035939]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.238221][G eval loss: 0.872835]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.237958][G train loss: 1.011604]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.237727][G eval loss: 0.849750]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.237628][G train loss: 0.987115]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.237320][G eval loss: 0.826872]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.237388][G train loss: 0.962461]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.237076][G eval loss: 0.804351]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.237276][G train loss: 0.937615]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.237037][G eval loss: 0.783017]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.237322][G train loss: 0.912939]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.237241][G eval loss: 0.763899]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.237522][G train loss: 0.889170]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.237650][G eval loss: 0.747949]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.237833][G train loss: 0.867311]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.238142][G eval loss: 0.735905]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.238215][G train loss: 0.848116]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.238616][G eval loss: 0.727653]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.238625][G train loss: 0.831623]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.239062][G eval loss: 0.723913]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.239062][G train loss: 0.818091]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.239644][G eval loss: 0.724075]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.239632][G train loss: 0.808312]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.240643][G eval loss: 0.726338]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.002 lr_d=0.0003 -> score=0.966982\n",
      "\n",
      "----- 0050: lr_g=0.002, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.128389]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.280059]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.118938]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.270594]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.113048]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.264669]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.109923]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.261502]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 1.108615]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.260172]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249964][G eval loss: 1.107783]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.259344]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249950][G eval loss: 1.106579]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249937][G train loss: 1.258164]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249932][G eval loss: 1.105102]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249916][G train loss: 1.256718]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249911][G eval loss: 1.103748]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249890][G train loss: 1.255392]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249887][G eval loss: 1.102831]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249860][G train loss: 1.254491]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249859][G eval loss: 1.102444]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249826][G train loss: 1.254112]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249826][G eval loss: 1.102538]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249785][G train loss: 1.254210]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249784][G eval loss: 1.102990]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249733][G train loss: 1.254663]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249729][G eval loss: 1.103650]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249668][G train loss: 1.255320]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249661][G eval loss: 1.104395]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249587][G train loss: 1.256060]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249576][G eval loss: 1.105135]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249488][G train loss: 1.256793]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249477][G eval loss: 1.105799]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249372][G train loss: 1.257449]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249367][G eval loss: 1.106342]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249241][G train loss: 1.257981]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249221][G eval loss: 1.106677]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249072][G train loss: 1.258304]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249060][G eval loss: 1.106870]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248884][G train loss: 1.258485]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248868][G eval loss: 1.106971]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248663][G train loss: 1.258572]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248639][G eval loss: 1.106971]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248401][G train loss: 1.258558]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248381][G eval loss: 1.106868]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248105][G train loss: 1.258442]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248069][G eval loss: 1.106686]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247750][G train loss: 1.258250]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247682][G eval loss: 1.106655]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247318][G train loss: 1.258205]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247221][G eval loss: 1.106875]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246807][G train loss: 1.258414]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246710][G eval loss: 1.106940]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246242][G train loss: 1.258467]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246101][G eval loss: 1.107036]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245570][G train loss: 1.258563]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245414][G eval loss: 1.107358]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244809][G train loss: 1.258871]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.244657][G eval loss: 1.107843]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243974][G train loss: 1.259331]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.243754][G eval loss: 1.108318]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.242993][G train loss: 1.259779]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.242694][G eval loss: 1.108862]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.241853][G train loss: 1.260295]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.241476][G eval loss: 1.109538]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.240552][G train loss: 1.260944]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.240093][G eval loss: 1.110363]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.239084][G train loss: 1.261741]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.238539][G eval loss: 1.111291]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.237454][G train loss: 1.262631]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.236895][G eval loss: 1.112081]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.235725][G train loss: 1.263376]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.235036][G eval loss: 1.112892]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.233786][G train loss: 1.264130]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.232954][G eval loss: 1.113677]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.231660][G train loss: 1.264848]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.230694][G eval loss: 1.114368]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.229360][G train loss: 1.265456]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.228293][G eval loss: 1.114843]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.226918][G train loss: 1.265836]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.225695][G eval loss: 1.115039]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.224307][G train loss: 1.265910]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.222848][G eval loss: 1.115005]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.221477][G train loss: 1.265729]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.219690][G eval loss: 1.114766]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.218382][G train loss: 1.265326]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.216163][G eval loss: 1.114357]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.214923][G train loss: 1.264727]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.212421][G eval loss: 1.113372]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.211256][G train loss: 1.263497]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.208331][G eval loss: 1.111914]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.207292][G train loss: 1.261655]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.203951][G eval loss: 1.109806]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.203043][G train loss: 1.258983]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.199332][G eval loss: 1.106673]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.198571][G train loss: 1.255105]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.194536][G eval loss: 1.102335]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.193879][G train loss: 1.249773]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.189539][G eval loss: 1.096413]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.189054][G train loss: 1.242550]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.184438][G eval loss: 1.088722]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.184255][G train loss: 1.233166]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.179477][G eval loss: 1.078894]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.180297][G train loss: 1.220009]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.175552][G eval loss: 1.065282]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.178788][G train loss: 1.200233]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.175016][G eval loss: 1.044088]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.181082][G train loss: 1.173448]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.181811][G eval loss: 1.010577]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.185829][G train loss: 1.143550]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.195167][G eval loss: 0.970081]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.193892][G train loss: 1.109580]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.208100][G eval loss: 0.939378]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.205720][G train loss: 1.072341]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.216817][G eval loss: 0.922786]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.222321][G train loss: 1.033526]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.223661][G eval loss: 0.911531]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.238683][G train loss: 1.000434]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.229938][G eval loss: 0.893712]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.252096][G train loss: 0.968491]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.237057][G eval loss: 0.866633]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.263253][G train loss: 0.935759]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.243954][G eval loss: 0.832280]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.271827][G train loss: 0.901411]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.249819][G eval loss: 0.794007]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.278443][G train loss: 0.866486]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.254919][G eval loss: 0.755497]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.283884][G train loss: 0.833335]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.259632][G eval loss: 0.720994]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.288488][G train loss: 0.804620]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.265258][G eval loss: 0.693855]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.293453][G train loss: 0.782011]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.272782][G eval loss: 0.673289]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.299405][G train loss: 0.765045]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.282866][G eval loss: 0.654849]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.306683][G train loss: 0.750671]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.293789][G eval loss: 0.638820]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.314606][G train loss: 0.736419]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.305242][G eval loss: 0.622275]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.322585][G train loss: 0.721210]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.319831][G eval loss: 0.602844]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.330597][G train loss: 0.705559]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.337009][G eval loss: 0.584090]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.337846][G train loss: 0.690396]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.351080][G eval loss: 0.563167]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.344680][G train loss: 0.677006]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.369079][G eval loss: 0.542160]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.348596][G train loss: 0.669039]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.375153][G eval loss: 0.538488]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.350837][G train loss: 0.662831]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.378170][G eval loss: 0.536511]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.002 lr_d=0.0005 -> score=0.914681\n",
      "\n",
      "----- 0050: lr_g=0.002, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250008][G eval loss: 1.126463]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250007][G train loss: 1.278132]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.118653]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.270310]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 1.114070]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249980][G train loss: 1.265692]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 1.111499]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.263078]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249962][G eval loss: 1.109951]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249952][G train loss: 1.261508]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249934][G eval loss: 1.108557]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.260117]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 1.106729]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249872][G train loss: 1.258314]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249845][G eval loss: 1.104694]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249813][G train loss: 1.256309]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249786][G eval loss: 1.103010]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249740][G train loss: 1.254655]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249711][G eval loss: 1.101938]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249649][G train loss: 1.253599]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249610][G eval loss: 1.101740]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249526][G train loss: 1.253410]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249472][G eval loss: 1.102090]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249361][G train loss: 1.253764]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249292][G eval loss: 1.102836]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249146][G train loss: 1.254512]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249047][G eval loss: 1.103821]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248860][G train loss: 1.255495]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248717][G eval loss: 1.104984]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248475][G train loss: 1.256655]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248288][G eval loss: 1.106256]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247978][G train loss: 1.257921]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247731][G eval loss: 1.107492]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247344][G train loss: 1.259150]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247084][G eval loss: 1.108600]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246618][G train loss: 1.260248]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246220][G eval loss: 1.109033]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245688][G train loss: 1.260671]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245147][G eval loss: 1.109160]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244549][G train loss: 1.260789]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.243875][G eval loss: 1.109481]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243199][G train loss: 1.261099]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242321][G eval loss: 1.110200]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241554][G train loss: 1.261805]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.240469][G eval loss: 1.111211]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239599][G train loss: 1.262809]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238274][G eval loss: 1.112624]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237281][G train loss: 1.264210]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.235676][G eval loss: 1.114357]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.234541][G train loss: 1.265932]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.232770][G eval loss: 1.116165]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.231465][G train loss: 1.267728]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.229367][G eval loss: 1.118226]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.227911][G train loss: 1.269785]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.225444][G eval loss: 1.120442]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.223872][G train loss: 1.271999]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.220751][G eval loss: 1.123127]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.219117][G train loss: 1.274668]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.215330][G eval loss: 1.126161]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.213633][G train loss: 1.277675]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.209121][G eval loss: 1.129623]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.207413][G train loss: 1.281109]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.202134][G eval loss: 1.133631]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.200435][G train loss: 1.285085]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.194375][G eval loss: 1.138164]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.192689][G train loss: 1.289580]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.186052][G eval loss: 1.142878]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.184386][G train loss: 1.294252]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.177063][G eval loss: 1.147897]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.175446][G train loss: 1.299206]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.167382][G eval loss: 1.153647]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.165865][G train loss: 1.304857]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.157164][G eval loss: 1.160308]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.155849][G train loss: 1.311400]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.146440][G eval loss: 1.168570]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.145454][G train loss: 1.319430]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.135786][G eval loss: 1.177997]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.135141][G train loss: 1.328590]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.125532][G eval loss: 1.188409]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.125255][G train loss: 1.338605]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.115921][G eval loss: 1.199684]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.116037][G train loss: 1.349253]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.107220][G eval loss: 1.211307]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.107773][G train loss: 1.359966]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.099848][G eval loss: 1.222074]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.101160][G train loss: 1.368747]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.095601][G eval loss: 1.226475]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.099503][G train loss: 1.366625]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.118476][G eval loss: 1.179466]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.135532][G train loss: 1.305124]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.192480][G eval loss: 1.088773]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.192860][G train loss: 1.231058]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.234081][G eval loss: 1.056534]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.244928][G train loss: 1.173219]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.247212][G eval loss: 1.045832]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.302770][G train loss: 1.103712]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.285363][G eval loss: 0.979439]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.339762][G train loss: 1.066509]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.331537][G eval loss: 0.938623]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.386002][G train loss: 1.014109]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.363316][G eval loss: 0.886052]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.398305][G train loss: 1.006614]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.399271][G eval loss: 0.857444]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.403924][G train loss: 1.002698]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.404807][G eval loss: 0.856625]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.407948][G train loss: 1.000839]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.410013][G eval loss: 0.857143]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.002 lr_d=0.0008 -> score=1.267156\n",
      "\n",
      "----- 0050: lr_g=0.002, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250052][G eval loss: 1.122333]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250051][G train loss: 1.274002]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 1.117848]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249987][G train loss: 1.269504]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 1.116332]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249965][G train loss: 1.267953]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 1.114994]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.266572]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 1.113124]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249929][G train loss: 1.264681]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249868][G eval loss: 1.111050]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249838][G train loss: 1.262610]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249758][G eval loss: 1.108876]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249709][G train loss: 1.260461]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249593][G eval loss: 1.106557]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249513][G train loss: 1.258175]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249328][G eval loss: 1.104604]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249198][G train loss: 1.256251]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248838][G eval loss: 1.103240]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248592][G train loss: 1.254905]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.247896][G eval loss: 1.102323]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.247563][G train loss: 1.253998]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.246875][G eval loss: 1.101918]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246429][G train loss: 1.253600]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.245429][G eval loss: 1.102661]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.244819][G train loss: 1.254350]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.243216][G eval loss: 1.104873]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242355][G train loss: 1.256564]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.240069][G eval loss: 1.108647]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.238861][G train loss: 1.260343]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.236303][G eval loss: 1.112973]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.234661][G train loss: 1.264663]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.231318][G eval loss: 1.117815]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.229241][G train loss: 1.269508]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.224532][G eval loss: 1.124002]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.222110][G train loss: 1.275694]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.215901][G eval loss: 1.130911]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.213329][G train loss: 1.282594]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.204909][G eval loss: 1.138410]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.202597][G train loss: 1.290082]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.192098][G eval loss: 1.145444]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.190132][G train loss: 1.297103]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.177032][G eval loss: 1.154105]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.175390][G train loss: 1.305751]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.160340][G eval loss: 1.165008]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.158950][G train loss: 1.316638]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.142699][G eval loss: 1.179579]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.141786][G train loss: 1.331186]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.125314][G eval loss: 1.198511]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.124842][G train loss: 1.350101]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.109227][G eval loss: 1.220759]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.109061][G train loss: 1.372344]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.094736][G eval loss: 1.245634]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.094784][G train loss: 1.397205]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.081978][G eval loss: 1.272202]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.082159][G train loss: 1.423754]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.070915][G eval loss: 1.299546]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.071182][G train loss: 1.451106]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.061387][G eval loss: 1.327032]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.061740][G train loss: 1.478580]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.053169][G eval loss: 1.354393]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.053617][G train loss: 1.505917]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.046064][G eval loss: 1.381552]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.046626][G train loss: 1.533043]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.039889][G eval loss: 1.408419]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.040563][G train loss: 1.559859]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.034514][G eval loss: 1.434841]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.035244][G train loss: 1.586222]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.029849][G eval loss: 1.460659]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.030558][G train loss: 1.611830]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.057286][G eval loss: 1.435299]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.059160][G train loss: 1.582664]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.194761][G eval loss: 1.249556]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.171029][G train loss: 1.431741]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.233544][G eval loss: 1.211144]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.246752][G train loss: 1.331630]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.271485][G eval loss: 1.167644]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.324741][G train loss: 1.228715]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.273680][G eval loss: 1.176457]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.375462][G train loss: 1.173046]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.387342][G eval loss: 1.007557]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.441259][G train loss: 1.082415]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.465450][G eval loss: 0.888277]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.479010][G train loss: 1.039759]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.467956][G eval loss: 0.886483]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.481872][G train loss: 1.037797]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.469643][G eval loss: 0.883444]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.483114][G train loss: 1.034617]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.470965][G eval loss: 0.880107]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.478597][G train loss: 1.031123]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.472015][G eval loss: 0.877043]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.479495][G train loss: 1.027858]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.477527][G eval loss: 0.874595]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.479991][G train loss: 1.025116]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.505723][G eval loss: 0.873357]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.002 lr_d=0.0015 -> score=1.379080\n",
      "\n",
      "----- 0050: lr_g=0.002, lr_d=0.002 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250109][G eval loss: 1.119377]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250107][G train loss: 1.271047]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 1.117313]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.268969]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.117739]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249957][G train loss: 1.269361]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 1.116404]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.267983]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249906][G eval loss: 1.114857]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249877][G train loss: 1.266414]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249727][G eval loss: 1.113019]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249669][G train loss: 1.264580]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249390][G eval loss: 1.110764]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249276][G train loss: 1.262351]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.248788][G eval loss: 1.109430]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248570][G train loss: 1.261050]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.247818][G eval loss: 1.108969]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.247469][G train loss: 1.260621]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.246279][G eval loss: 1.109720]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.245749][G train loss: 1.261391]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.243978][G eval loss: 1.111383]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.243219][G train loss: 1.263068]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.240318][G eval loss: 1.114683]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.239240][G train loss: 1.266378]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.234898][G eval loss: 1.119833]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.233479][G train loss: 1.271536]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.227487][G eval loss: 1.126261]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.225765][G train loss: 1.277967]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.216678][G eval loss: 1.134710]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.215031][G train loss: 1.286426]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.202487][G eval loss: 1.145276]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.201177][G train loss: 1.296996]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.185172][G eval loss: 1.158570]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.184218][G train loss: 1.310295]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.165008][G eval loss: 1.176815]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.164154][G train loss: 1.328539]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.142762][G eval loss: 1.202152]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.142228][G train loss: 1.353860]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.120755][G eval loss: 1.233358]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.120597][G train loss: 1.385048]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.100201][G eval loss: 1.268866]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.100375][G train loss: 1.420561]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.082195][G eval loss: 1.306606]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.082667][G train loss: 1.458321]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.066776][G eval loss: 1.345267]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.067555][G train loss: 1.497014]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.053798][G eval loss: 1.383847]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.054854][G train loss: 1.535643]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.042856][G eval loss: 1.422575]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.044040][G train loss: 1.574399]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.033879][G eval loss: 1.459994]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.034795][G train loss: 1.611830]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.026541][G eval loss: 1.496205]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.027046][G train loss: 1.648038]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.020616][G eval loss: 1.530916]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.020813][G train loss: 1.682735]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.015895][G eval loss: 1.563800]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.015950][G train loss: 1.715592]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.012193][G eval loss: 1.594595]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.012196][G train loss: 1.746345]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.009311][G eval loss: 1.623074]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.009312][G train loss: 1.774771]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.007081][G eval loss: 1.649132]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.007081][G train loss: 1.800768]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.005370][G eval loss: 1.672727]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.005370][G train loss: 1.824299]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.004066][G eval loss: 1.693915]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.004066][G train loss: 1.845420]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.003076][G eval loss: 1.712817]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.003076][G train loss: 1.864251]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.002328][G eval loss: 1.729578]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.002328][G train loss: 1.880941]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.001763][G eval loss: 1.744368]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.001763][G train loss: 1.895659]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.001336][G eval loss: 1.757349]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.001336][G train loss: 1.908567]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.001013][G eval loss: 1.768675]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.001013][G train loss: 1.919824]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.000769][G eval loss: 1.778493]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.000769][G train loss: 1.929579]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.000583][G eval loss: 1.786947]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.000583][G train loss: 1.937978]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.000443][G eval loss: 1.794188]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.000443][G train loss: 1.945164]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.000336][G eval loss: 1.800327]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.000336][G train loss: 1.951255]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.000256][G eval loss: 1.805455]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.000256][G train loss: 1.956339]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.000194][G eval loss: 1.809672]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.000194][G train loss: 1.960510]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.000147][G eval loss: 1.813082]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.000147][G train loss: 1.963854]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.000112][G eval loss: 1.815773]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.000112][G train loss: 1.966472]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.000084][G eval loss: 1.817688]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.000084][G train loss: 1.968295]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.000064][G eval loss: 1.818791]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.000064][G train loss: 1.969294]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.000048][G eval loss: 1.819062]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.000048][G train loss: 1.969460]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.000037][G eval loss: 1.818500]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.000037][G train loss: 1.968771]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.000028][G eval loss: 1.817014]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.000028][G train loss: 1.967150]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.000021][G eval loss: 1.814459]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.000021][G train loss: 1.964428]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.000016][G eval loss: 1.810667]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.000016][G train loss: 1.960392]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.000012][G eval loss: 1.805480]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.000012][G train loss: 1.954920]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.000009][G eval loss: 1.798768]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.000009][G train loss: 1.947900]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.000007][G eval loss: 1.790369]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.000007][G train loss: 1.939138]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.000005][G eval loss: 1.780121]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.000005][G train loss: 1.928490]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.000004][G eval loss: 1.767951]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.000004][G train loss: 1.915964]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.000003][G eval loss: 1.753851]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.000003][G train loss: 1.901572]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.000002][G eval loss: 1.737900]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.000002][G train loss: 1.885400]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.000002][G eval loss: 1.720300]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.000002][G train loss: 1.867578]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.000002][G eval loss: 1.701235]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.000002][G train loss: 1.848428]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.000001][G eval loss: 1.680922]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.000001][G train loss: 1.828122]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.000001][G eval loss: 1.659431]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.009999][G train loss: 1.786843]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.000006][G eval loss: 1.635717]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.099977][G train loss: 1.585053]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.166635][G eval loss: 1.281408]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.229988][G train loss: 1.299983]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.333279][G eval loss: 0.927007]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.319947][G train loss: 1.101387]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.333789][G eval loss: 0.889317]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.369964][G train loss: 0.978091]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.291685][G eval loss: 0.965049]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.389958][G train loss: 0.920639]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.291636][G eval loss: 0.957377]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.379998][G train loss: 0.921345]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.291648][G eval loss: 0.942791]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.410064][G train loss: 0.844701]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.291644][G eval loss: 0.932613]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.389986][G train loss: 0.872509]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.333309][G eval loss: 0.839291]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.389976][G train loss: 0.859315]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.374976][G eval loss: 0.744802]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.370065][G train loss: 0.880942]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.374978][G eval loss: 0.738792]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.360004][G train loss: 0.890540]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.369706][G eval loss: 0.737401]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.359912][G train loss: 0.884580]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.374991][G eval loss: 0.730547]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.350014][G train loss: 0.891601]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.333328][G eval loss: 0.817613]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.340212][G train loss: 0.900985]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.291678][G eval loss: 0.903490]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.330055][G train loss: 0.919763]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.333325][G eval loss: 0.830733]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.340105][G train loss: 0.900109]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.374992][G eval loss: 0.755904]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.379989][G train loss: 0.825853]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.458322][G eval loss: 0.596244]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.389990][G train loss: 0.808057]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.458323][G eval loss: 0.598407]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.419990][G train loss: 0.749907]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.458325][G eval loss: 0.596442]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.439991][G train loss: 0.710113]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.464621][G eval loss: 0.542632]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.469991][G train loss: 0.647897]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.499991][G eval loss: 0.508023]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.489991][G train loss: 0.603235]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.499992][G eval loss: 0.502057]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.499992][G train loss: 0.576045]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.499993][G eval loss: 0.494430]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.499993][G train loss: 0.569148]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.499993][G eval loss: 0.485475]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.499993][G train loss: 0.560928]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.499994][G eval loss: 0.475803]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.499994][G train loss: 0.552084]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.499994][G eval loss: 0.464465]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.499994][G train loss: 0.542648]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.499995][G eval loss: 0.450765]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.499995][G train loss: 0.531444]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.499995][G eval loss: 0.435547]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.499995][G train loss: 0.518424]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.499996][G eval loss: 0.419497]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.499996][G train loss: 0.503708]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.499996][G eval loss: 0.402866]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.499996][G train loss: 0.487853]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.499996][G eval loss: 0.386348]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.499996][G train loss: 0.472297]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.499997][G eval loss: 0.373637]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.499997][G train loss: 0.458364]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.499997][G eval loss: 0.364387]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.499997][G train loss: 0.446539]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.499997][G eval loss: 0.359466]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.499997][G train loss: 0.438002]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.499998][G eval loss: 0.359743]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.499997][G train loss: 0.433640]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.499998][G eval loss: 0.361362]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.002 lr_d=0.002 -> score=0.861360\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.0003 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.120584]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.272257]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.108384]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.259966]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.106111]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.257648]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 1.104693]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.256293]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 1.103605]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.255273]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 1.104040]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249979][G train loss: 1.255742]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 1.105283]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.256994]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 1.106422]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.258123]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 1.106839]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249962][G train loss: 1.258513]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249962][G eval loss: 1.106355]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249953][G train loss: 1.257992]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249954][G eval loss: 1.105251]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249943][G train loss: 1.256840]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249944][G eval loss: 1.103874]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249932][G train loss: 1.255417]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249932][G eval loss: 1.102424]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249918][G train loss: 1.253924]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 1.101144]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249904][G train loss: 1.252608]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249906][G eval loss: 1.100097]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249886][G train loss: 1.251510]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249889][G eval loss: 1.099323]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249866][G train loss: 1.250672]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249868][G eval loss: 1.098883]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249843][G train loss: 1.250166]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249845][G eval loss: 1.098336]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249816][G train loss: 1.249546]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249819][G eval loss: 1.097126]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249785][G train loss: 1.248225]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249789][G eval loss: 1.095208]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249751][G train loss: 1.246122]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249756][G eval loss: 1.092570]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249712][G train loss: 1.243227]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249717][G eval loss: 1.088854]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249668][G train loss: 1.239160]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249675][G eval loss: 1.083245]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249620][G train loss: 1.233100]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249628][G eval loss: 1.074722]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249566][G train loss: 1.224015]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.249578][G eval loss: 1.062243]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249509][G train loss: 1.210747]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.249528][G eval loss: 1.044807]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249450][G train loss: 1.192318]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.249481][G eval loss: 1.022245]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.249392][G train loss: 1.168331]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.249436][G eval loss: 0.993972]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.249333][G train loss: 1.138047]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.249397][G eval loss: 0.958386]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.249277][G train loss: 1.100570]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.249357][G eval loss: 0.915696]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.249221][G train loss: 1.056314]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.249319][G eval loss: 0.868458]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.249165][G train loss: 1.007607]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.249295][G eval loss: 0.821338]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.249115][G train loss: 0.958123]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.249288][G eval loss: 0.780502]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249073][G train loss: 0.911840]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.249279][G eval loss: 0.749536]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.249027][G train loss: 0.870974]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.249253][G eval loss: 0.732002]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248967][G train loss: 0.839354]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.249215][G eval loss: 0.730306]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.248899][G train loss: 0.820803]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.249189][G eval loss: 0.740952]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.248839][G train loss: 0.812632]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.249204][G eval loss: 0.752795]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.0003 -> score=1.001999\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.119275]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.270949]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.108129]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.259711]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.106901]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249986][G train loss: 1.258437]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.106161]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.257762]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.105135]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.256803]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 1.105068]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249959][G train loss: 1.256771]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 1.105553]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249944][G train loss: 1.257264]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 1.105930]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249926][G train loss: 1.257632]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 1.105726]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249904][G train loss: 1.257400]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249899][G eval loss: 1.104848]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249876][G train loss: 1.256486]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249869][G eval loss: 1.103633]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249840][G train loss: 1.255223]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 1.102427]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249797][G train loss: 1.253971]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249786][G eval loss: 1.101383]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249743][G train loss: 1.252883]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249729][G eval loss: 1.100673]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249678][G train loss: 1.252138]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249659][G eval loss: 1.100297]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249597][G train loss: 1.251712]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249577][G eval loss: 1.100231]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249501][G train loss: 1.251582]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249477][G eval loss: 1.100348]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249387][G train loss: 1.251635]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249357][G eval loss: 1.100292]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249250][G train loss: 1.251506]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249217][G eval loss: 1.099502]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249089][G train loss: 1.250608]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249054][G eval loss: 1.097937]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248903][G train loss: 1.248861]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248861][G eval loss: 1.095587]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248682][G train loss: 1.246260]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248626][G eval loss: 1.092116]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248418][G train loss: 1.242439]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248327][G eval loss: 1.086703]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248088][G train loss: 1.236572]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.247977][G eval loss: 1.078333]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247706][G train loss: 1.227635]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247595][G eval loss: 1.065985]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247284][G train loss: 1.214485]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247200][G eval loss: 1.048447]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246843][G train loss: 1.195944]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246778][G eval loss: 1.025737]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246371][G train loss: 1.171796]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246377][G eval loss: 0.997435]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245899][G train loss: 1.141487]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.246000][G eval loss: 0.961977]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.245434][G train loss: 1.104153]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245643][G eval loss: 0.919482]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.244990][G train loss: 1.060108]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.245313][G eval loss: 0.872464]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.244558][G train loss: 1.011675]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.245055][G eval loss: 0.825606]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.244149][G train loss: 0.962630]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.244898][G eval loss: 0.785120]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.243767][G train loss: 0.917053]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.244772][G eval loss: 0.754281]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.243367][G train loss: 0.876966]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.244608][G eval loss: 0.736057]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.242894][G train loss: 0.845592]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.244392][G eval loss: 0.732642]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.242360][G train loss: 0.826491]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.244304][G eval loss: 0.740996]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.241911][G train loss: 0.817197]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.244569][G eval loss: 0.750104]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.0005 -> score=0.994673\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.117348]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.269022]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.107848]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.259430]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.107934]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.259470]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 1.107752]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.259352]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.106480]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.258148]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 1.105823]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249934][G train loss: 1.257526]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 1.105642]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249900][G train loss: 1.257354]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 1.105422]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249856][G train loss: 1.257124]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249829][G eval loss: 1.104922]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249791][G train loss: 1.256597]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249755][G eval loss: 1.103952]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249704][G train loss: 1.255591]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249656][G eval loss: 1.102809]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249587][G train loss: 1.254401]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249521][G eval loss: 1.101852]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249428][G train loss: 1.253398]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249352][G eval loss: 1.101090]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249232][G train loss: 1.252594]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249129][G eval loss: 1.100708]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248976][G train loss: 1.252176]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248830][G eval loss: 1.100744]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248635][G train loss: 1.252164]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248435][G eval loss: 1.101221]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248189][G train loss: 1.252581]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247928][G eval loss: 1.102093]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247619][G train loss: 1.253393]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247369][G eval loss: 1.102612]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246991][G train loss: 1.253839]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246538][G eval loss: 1.101883]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246085][G train loss: 1.253011]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245511][G eval loss: 1.100232]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244968][G train loss: 1.251205]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244334][G eval loss: 1.097981]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243709][G train loss: 1.248718]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242917][G eval loss: 1.095047]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242209][G train loss: 1.245433]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241217][G eval loss: 1.090575]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240423][G train loss: 1.240480]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239243][G eval loss: 1.083274]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238351][G train loss: 1.232568]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236957][G eval loss: 1.072087]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235960][G train loss: 1.220541]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234424][G eval loss: 1.055990]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.233300][G train loss: 1.203394]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231631][G eval loss: 1.034966]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.230363][G train loss: 1.180851]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.228569][G eval loss: 1.008484]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.227142][G train loss: 1.152276]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.225200][G eval loss: 0.974675]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.223656][G train loss: 1.116490]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.221947][G eval loss: 0.932603]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.220142][G train loss: 1.073285]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.219502][G eval loss: 0.883689]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.217126][G train loss: 1.024095]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.218658][G eval loss: 0.832171]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.215622][G train loss: 0.971668]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.221429][G eval loss: 0.781474]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.217015][G train loss: 0.918553]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.228506][G eval loss: 0.735036]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.222167][G train loss: 0.866600]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.238255][G eval loss: 0.696277]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.230798][G train loss: 0.817407]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250014][G eval loss: 0.665469]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.242000][G train loss: 0.774323]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.263483][G eval loss: 0.639508]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.254340][G train loss: 0.738828]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.277342][G eval loss: 0.618302]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.267042][G train loss: 0.709377]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.290981][G eval loss: 0.597570]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.279433][G train loss: 0.682120]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.302760][G eval loss: 0.575034]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.290966][G train loss: 0.655376]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.312883][G eval loss: 0.549432]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.301308][G train loss: 0.628705]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.321247][G eval loss: 0.525353]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.309518][G train loss: 0.603509]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.327861][G eval loss: 0.505094]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.315519][G train loss: 0.582509]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.332504][G eval loss: 0.488767]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.319466][G train loss: 0.565839]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.334148][G eval loss: 0.477095]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.321850][G train loss: 0.553709]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.334225][G eval loss: 0.468145]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.323206][G train loss: 0.544595]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.333293][G eval loss: 0.461388]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.323749][G train loss: 0.537106]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.331393][G eval loss: 0.457375]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.322852][G train loss: 0.532372]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.328569][G eval loss: 0.455950]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.320276][G train loss: 0.530057]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.325559][G eval loss: 0.455366]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.317726][G train loss: 0.527674]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.322156][G eval loss: 0.454873]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.315422][G train loss: 0.524987]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.318218][G eval loss: 0.453617]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.312949][G train loss: 0.521860]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.314182][G eval loss: 0.450924]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.309992][G train loss: 0.517351]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.309999][G eval loss: 0.446332]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.307085][G train loss: 0.510987]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.304943][G eval loss: 0.443888]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.303634][G train loss: 0.506479]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.299970][G eval loss: 0.441739]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299643][G train loss: 0.501919]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.295039][G eval loss: 0.441201]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.295083][G train loss: 0.499082]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.290655][G eval loss: 0.443324]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.290636][G train loss: 0.499080]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.286974][G eval loss: 0.443465]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.286837][G train loss: 0.498879]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.283545][G eval loss: 0.444566]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.283169][G train loss: 0.500298]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.279999][G eval loss: 0.445908]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.279103][G train loss: 0.502009]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.276858][G eval loss: 0.445695]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.275081][G train loss: 0.502841]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.274115][G eval loss: 0.446005]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.271605][G train loss: 0.504608]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.271596][G eval loss: 0.446998]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.269004][G train loss: 0.506768]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.269076][G eval loss: 0.448190]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.266708][G train loss: 0.508424]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.266521][G eval loss: 0.449291]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.264536][G train loss: 0.509336]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.263590][G eval loss: 0.449091]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.262477][G train loss: 0.509300]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.260577][G eval loss: 0.447475]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.260595][G train loss: 0.508215]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.257835][G eval loss: 0.446247]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.258982][G train loss: 0.507135]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.256090][G eval loss: 0.446100]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.257574][G train loss: 0.507109]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.254893][G eval loss: 0.445716]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.256320][G train loss: 0.507197]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.253912][G eval loss: 0.445059]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.255211][G train loss: 0.506519]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.253114][G eval loss: 0.445002]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.254254][G train loss: 0.505375]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.252473][G eval loss: 0.446369]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.253438][G train loss: 0.504807]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.251914][G eval loss: 0.448215]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.0008 -> score=0.700129\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250054][G eval loss: 1.113217]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250053][G train loss: 1.264891]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 1.107029]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.258610]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 1.110167]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.261703]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.111211]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.262812]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249963][G eval loss: 1.109636]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249947][G train loss: 1.261304]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 1.108336]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249872][G train loss: 1.260039]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249808][G eval loss: 1.107621]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249763][G train loss: 1.259333]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249665][G eval loss: 1.106924]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249594][G train loss: 1.258627]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249422][G eval loss: 1.105941]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249315][G train loss: 1.257618]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249033][G eval loss: 1.104727]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248860][G train loss: 1.256370]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248368][G eval loss: 1.103388]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248055][G train loss: 1.254986]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247142][G eval loss: 1.102237]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246718][G train loss: 1.253790]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.245595][G eval loss: 1.101206]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245038][G train loss: 1.252722]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.243424][G eval loss: 1.101515]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242681][G train loss: 1.253005]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.240444][G eval loss: 1.103434]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239431][G train loss: 1.254897]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.236690][G eval loss: 1.106631]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235310][G train loss: 1.258050]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.231729][G eval loss: 1.111530]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.229965][G train loss: 1.262901]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.225296][G eval loss: 1.117790]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223194][G train loss: 1.269083]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.217657][G eval loss: 1.123288]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.215350][G train loss: 1.274465]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.207882][G eval loss: 1.128696]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.205754][G train loss: 1.279687]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.195873][G eval loss: 1.133526]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.194127][G train loss: 1.284265]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.181508][G eval loss: 1.138912]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.180162][G train loss: 1.289283]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.165275][G eval loss: 1.144581]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.164174][G train loss: 1.294391]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.147368][G eval loss: 1.150960]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.146567][G train loss: 1.299954]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.129551][G eval loss: 1.155985]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.129201][G train loss: 1.303856]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.113171][G eval loss: 1.157951]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.113257][G train loss: 1.304318]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.098906][G eval loss: 1.156969]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.099378][G train loss: 1.300939]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.087202][G eval loss: 1.150542]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.088375][G train loss: 1.289992]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.080925][G eval loss: 1.126542]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.108618][G train loss: 1.214186]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.207755][G eval loss: 0.900253]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.189389][G train loss: 1.065079]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.338125][G eval loss: 0.733485]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.351582][G train loss: 0.840911]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.384060][G eval loss: 0.680492]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.386134][G train loss: 0.804374]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.393316][G eval loss: 0.636666]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.396813][G train loss: 0.762393]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.398859][G eval loss: 0.586896]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.403454][G train loss: 0.718370]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.402271][G eval loss: 0.554728]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.407521][G train loss: 0.687568]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.404116][G eval loss: 0.551520]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.409728][G train loss: 0.677411]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.404718][G eval loss: 0.555532]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.410562][G train loss: 0.671395]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.404275][G eval loss: 0.547668]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.410295][G train loss: 0.654396]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.402922][G eval loss: 0.526905]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.409080][G train loss: 0.626463]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.400842][G eval loss: 0.502423]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.407007][G train loss: 0.595407]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.417123][G eval loss: 0.486622]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.404229][G train loss: 0.570505]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.418365][G eval loss: 0.488019]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.401758][G train loss: 0.557492]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.415567][G eval loss: 0.492055]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.0015 -> score=0.907621\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.002 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250110][G eval loss: 1.110262]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250108][G train loss: 1.261935]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 1.106506]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249987][G train loss: 1.258088]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.111608]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.263144]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 1.112632]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.264232]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 1.111344]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249896][G train loss: 1.263013]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249783][G eval loss: 1.110202]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249729][G train loss: 1.261906]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249504][G eval loss: 1.109565]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249402][G train loss: 1.261279]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.248983][G eval loss: 1.110032]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248787][G train loss: 1.261738]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.248170][G eval loss: 1.110551]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.247844][G train loss: 1.262233]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.246806][G eval loss: 1.111223]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.246293][G train loss: 1.262874]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.244719][G eval loss: 1.112296]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.243944][G train loss: 1.263904]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.241278][G eval loss: 1.114897]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.240190][G train loss: 1.266464]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.236297][G eval loss: 1.119256]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.234783][G train loss: 1.270784]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.229704][G eval loss: 1.124829]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.227769][G train loss: 1.276336]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.219985][G eval loss: 1.133289]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.217726][G train loss: 1.284761]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.206657][G eval loss: 1.143823]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.204686][G train loss: 1.295249]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.189225][G eval loss: 1.157889]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.188199][G train loss: 1.309257]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.168368][G eval loss: 1.178090]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.167954][G train loss: 1.329335]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.145926][G eval loss: 1.202942]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.145604][G train loss: 1.353986]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.123269][G eval loss: 1.231923]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.123317][G train loss: 1.382667]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.102427][G eval loss: 1.263264]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.102892][G train loss: 1.413577]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.084166][G eval loss: 1.295323]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.084935][G train loss: 1.444944]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.068650][G eval loss: 1.325830]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.069734][G train loss: 1.474069]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.067561][G eval loss: 1.321965]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.086264][G train loss: 1.438835]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.171063][G eval loss: 1.154728]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.173591][G train loss: 1.289866]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.208303][G eval loss: 1.120484]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.248504][G train loss: 1.193560]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.242114][G eval loss: 1.077136]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.302457][G train loss: 1.114896]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.285803][G eval loss: 0.976652]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.366479][G train loss: 1.033196]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.347813][G eval loss: 0.944270]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.417153][G train loss: 0.984823]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.381513][G eval loss: 0.917757]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.431480][G train loss: 0.997742]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.416251][G eval loss: 0.867202]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.430530][G train loss: 0.996199]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.445688][G eval loss: 0.849084]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.431306][G train loss: 0.978300]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.476396][G eval loss: 0.816241]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.430773][G train loss: 0.948092]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.476793][G eval loss: 0.777580]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.429975][G train loss: 0.913454]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.473632][G eval loss: 0.741583]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.427801][G train loss: 0.881067]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.468912][G eval loss: 0.712744]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.423214][G train loss: 0.854555]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.462626][G eval loss: 0.691441]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.416740][G train loss: 0.835008]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.454806][G eval loss: 0.678563]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.408536][G train loss: 0.822729]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.392875][G eval loss: 0.764410]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.398229][G train loss: 0.816470]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.376884][G eval loss: 0.798804]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.002 -> score=1.175688\n",
      "\n",
      "----- 0050: lr_g=0.006, lr_d=0.0003 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.114403]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.266077]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.108505]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.260014]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.104090]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.255714]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 1.102475]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.254182]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.104319]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.256053]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 1.106412]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249979][G train loss: 1.258137]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 1.107033]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.258707]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 1.106154]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.257760]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.104549]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249961][G train loss: 1.256092]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249961][G eval loss: 1.102967]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249952][G train loss: 1.254485]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249952][G eval loss: 1.101750]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249942][G train loss: 1.253228]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 1.101007]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249929][G train loss: 1.252402]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249929][G eval loss: 1.100206]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249915][G train loss: 1.251505]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 1.098376]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249899][G train loss: 1.249529]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249899][G eval loss: 1.095652]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249881][G train loss: 1.246582]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249881][G eval loss: 1.091395]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249860][G train loss: 1.241994]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249861][G eval loss: 1.084267]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249839][G train loss: 1.234446]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249841][G eval loss: 1.072319]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249816][G train loss: 1.221831]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249819][G eval loss: 1.052218]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249792][G train loss: 1.200363]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249799][G eval loss: 1.020147]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249769][G train loss: 1.165902]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249781][G eval loss: 0.973285]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249747][G train loss: 1.115869]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249769][G eval loss: 0.912311]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249730][G train loss: 1.051672]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249757][G eval loss: 0.844965]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249713][G train loss: 0.980580]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249738][G eval loss: 0.777770]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249690][G train loss: 0.907906]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.249715][G eval loss: 0.727658]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249660][G train loss: 0.847082]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.249700][G eval loss: 0.724691]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249633][G train loss: 0.819604]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.249702][G eval loss: 0.744887]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.249625][G train loss: 0.817711]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.249723][G eval loss: 0.758214]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.006 lr_d=0.0003 -> score=1.007936\n",
      "\n",
      "----- 0050: lr_g=0.006, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 1.113095]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.264768]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.108249]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.259759]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.104881]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.256505]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.103945]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249978][G train loss: 1.255651]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 1.105849]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.257584]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 1.107439]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249961][G train loss: 1.259164]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 1.107299]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249946][G train loss: 1.258973]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 1.105654]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249926][G train loss: 1.257260]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 1.103426]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249903][G train loss: 1.254969]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249896][G eval loss: 1.101453]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249875][G train loss: 1.252971]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249866][G eval loss: 1.100129]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249839][G train loss: 1.251607]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249827][G eval loss: 1.099560]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249793][G train loss: 1.250955]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249777][G eval loss: 1.099172]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249736][G train loss: 1.250471]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249715][G eval loss: 1.097919]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249666][G train loss: 1.249072]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249640][G eval loss: 1.095869]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249581][G train loss: 1.246799]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249565][G eval loss: 1.092316]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249495][G train loss: 1.242914]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249454][G eval loss: 1.085780]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249373][G train loss: 1.235953]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249341][G eval loss: 1.074352]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249250][G train loss: 1.223854]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249218][G eval loss: 1.054722]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249116][G train loss: 1.202847]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249088][G eval loss: 1.023046]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248976][G train loss: 1.168766]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248962][G eval loss: 0.976513]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248839][G train loss: 1.119036]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248847][G eval loss: 0.915743]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248705][G train loss: 1.055002]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248728][G eval loss: 0.848670]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248574][G train loss: 0.984132]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248565][G eval loss: 0.781544]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.248403][G train loss: 0.911524]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.248355][G eval loss: 0.730797]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.248177][G train loss: 0.850342]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.248203][G eval loss: 0.726845]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.247957][G train loss: 0.822213]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.248179][G eval loss: 0.746319]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.247872][G train loss: 0.819597]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.248305][G eval loss: 0.759073]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.006 lr_d=0.0005 -> score=1.007377\n",
      "\n",
      "----- 0050: lr_g=0.006, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 1.111166]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250009][G train loss: 1.262840]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.107972]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249996][G train loss: 1.259482]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.105922]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.257545]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 1.105539]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.257245]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 1.107187]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249965][G train loss: 1.258921]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 1.108173]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249942][G train loss: 1.259898]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249926][G eval loss: 1.107350]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249909][G train loss: 1.259023]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249889][G eval loss: 1.105084]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249865][G train loss: 1.256690]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249839][G eval loss: 1.102539]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249807][G train loss: 1.254083]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249773][G eval loss: 1.100468]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249729][G train loss: 1.251986]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249681][G eval loss: 1.099229]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249620][G train loss: 1.250708]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249551][G eval loss: 1.098883]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249470][G train loss: 1.250280]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249372][G eval loss: 1.098823]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249268][G train loss: 1.250124]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249147][G eval loss: 1.097980]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249016][G train loss: 1.249133]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248866][G eval loss: 1.096417]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248701][G train loss: 1.247346]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248494][G eval loss: 1.093417]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248292][G train loss: 1.244009]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.248041][G eval loss: 1.087673]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247802][G train loss: 1.237830]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247582][G eval loss: 1.076681]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247297][G train loss: 1.226157]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246879][G eval loss: 1.056919]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246571][G train loss: 1.204938]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.246107][G eval loss: 1.024786]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.245800][G train loss: 1.170260]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.245383][G eval loss: 0.977703]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.245085][G train loss: 1.119810]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.244761][G eval loss: 0.916623]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.244458][G train loss: 1.055274]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.244181][G eval loss: 0.850070]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.243915][G train loss: 0.984561]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.243327][G eval loss: 0.783700]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.243169][G train loss: 0.912349]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.242068][G eval loss: 0.731762]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.242021][G train loss: 0.850607]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.241014][G eval loss: 0.724576]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.240839][G train loss: 0.820718]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.241122][G eval loss: 0.740573]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.240740][G train loss: 0.814943]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.242881][G eval loss: 0.748825]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.006 lr_d=0.0008 -> score=0.991706\n",
      "\n",
      "----- 0050: lr_g=0.006, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250056][G eval loss: 1.107035]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250054][G train loss: 1.258709]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.107148]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 1.258657]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.108145]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.259769]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 1.108987]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.260694]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 1.110341]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.262076]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249912][G eval loss: 1.110699]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249887][G train loss: 1.262425]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249817][G eval loss: 1.109284]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249778][G train loss: 1.260959]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249676][G eval loss: 1.106526]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249614][G train loss: 1.258133]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249457][G eval loss: 1.103448]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249364][G train loss: 1.254993]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249103][G eval loss: 1.101057]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248951][G train loss: 1.252576]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248400][G eval loss: 1.099514]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248134][G train loss: 1.250994]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247233][G eval loss: 1.098744]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246870][G train loss: 1.250146]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.245737][G eval loss: 1.098804]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245259][G train loss: 1.250108]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.243577][G eval loss: 1.099130]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242946][G train loss: 1.250275]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.240738][G eval loss: 1.099777]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239880][G train loss: 1.250675]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.237287][G eval loss: 1.099474]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.236113][G train loss: 1.250004]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.232656][G eval loss: 1.097256]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.231198][G train loss: 1.247226]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.226547][G eval loss: 1.091720]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.224884][G train loss: 1.240812]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.219401][G eval loss: 1.077572]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.217677][G train loss: 1.224880]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.210549][G eval loss: 1.051592]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.209275][G train loss: 1.195735]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.200415][G eval loss: 1.010489]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.201789][G train loss: 1.146439]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.197074][G eval loss: 0.945082]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.202819][G train loss: 1.071075]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.212550][G eval loss: 0.857504]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.206904][G train loss: 0.996381]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.221670][G eval loss: 0.798925]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.218614][G train loss: 0.918486]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.227976][G eval loss: 0.743909]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235262][G train loss: 0.837168]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.244395][G eval loss: 0.681982]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.263494][G train loss: 0.755896]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.279297][G eval loss: 0.613446]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.290107][G train loss: 0.692213]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.308846][G eval loss: 0.604062]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.315781][G train loss: 0.675237]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.339690][G eval loss: 0.595155]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.006 lr_d=0.0015 -> score=0.934845\n",
      "\n",
      "----- 0050: lr_g=0.006, lr_d=0.002 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250112][G eval loss: 1.104080]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250110][G train loss: 1.255754]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 1.106627]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.258136]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 1.109594]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249961][G train loss: 1.261217]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 1.110401]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.262107]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249945][G eval loss: 1.112034]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249921][G train loss: 1.263769]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249811][G eval loss: 1.112483]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249762][G train loss: 1.264210]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249532][G eval loss: 1.111107]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249444][G train loss: 1.262783]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249057][G eval loss: 1.109473]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248892][G train loss: 1.261082]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.248306][G eval loss: 1.107892]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.248034][G train loss: 1.259439]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.247048][G eval loss: 1.107367]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.246624][G train loss: 1.258883]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.245022][G eval loss: 1.108360]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.244373][G train loss: 1.259841]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.241755][G eval loss: 1.111427]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.240849][G train loss: 1.262837]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.237039][G eval loss: 1.116171]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.235787][G train loss: 1.267468]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.230770][G eval loss: 1.121281]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.229138][G train loss: 1.272406]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.221595][G eval loss: 1.128065]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.219720][G train loss: 1.278910]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.209237][G eval loss: 1.134838]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.207656][G train loss: 1.285219]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.192995][G eval loss: 1.141885]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.192315][G train loss: 1.291556]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.173385][G eval loss: 1.149126]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.173618][G train loss: 1.297244]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.152433][G eval loss: 1.151850]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.153483][G train loss: 1.296167]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.135500][G eval loss: 1.135491]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.150598][G train loss: 1.249697]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.199894][G eval loss: 0.982275]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.185617][G train loss: 1.147410]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.216340][G eval loss: 0.961989]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.226988][G train loss: 1.067916]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.227720][G eval loss: 1.003059]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.281913][G train loss: 1.036260]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.263257][G eval loss: 0.978159]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.307170][G train loss: 1.015079]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.286918][G eval loss: 0.941792]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.326667][G train loss: 0.975839]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.304129][G eval loss: 0.873766]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.334419][G train loss: 0.905871]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.328229][G eval loss: 0.784009]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.339674][G train loss: 0.822364]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.330680][G eval loss: 0.707563]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.341733][G train loss: 0.757295]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.327775][G eval loss: 0.680898]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.331348][G train loss: 0.736703]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.328466][G eval loss: 0.683550]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.328518][G train loss: 0.745127]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.340138][G eval loss: 0.684596]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.324264][G train loss: 0.757446]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.333894][G eval loss: 0.686176]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.320342][G train loss: 0.742347]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.347146][G eval loss: 0.637452]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.318748][G train loss: 0.729586]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.339161][G eval loss: 0.619173]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.311185][G train loss: 0.701709]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.330682][G eval loss: 0.592729]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.300771][G train loss: 0.669511]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.322106][G eval loss: 0.573793]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.289832][G train loss: 0.641461]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.314925][G eval loss: 0.559566]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.292585][G train loss: 0.623113]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.308779][G eval loss: 0.551605]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.286396][G train loss: 0.619031]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.303489][G eval loss: 0.558927]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.276476][G train loss: 0.630572]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.299475][G eval loss: 0.570319]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.267196][G train loss: 0.643988]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.296213][G eval loss: 0.580409]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.263860][G train loss: 0.652289]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.293546][G eval loss: 0.588137]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.260132][G train loss: 0.657484]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.291587][G eval loss: 0.596867]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.263413][G train loss: 0.662784]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.290283][G eval loss: 0.613285]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.263719][G train loss: 0.672924]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.289510][G eval loss: 0.637780]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.265090][G train loss: 0.691418]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.272272][G eval loss: 0.666048]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.266097][G train loss: 0.715903]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.273517][G eval loss: 0.685573]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.266811][G train loss: 0.736434]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.274242][G eval loss: 0.691569]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.266814][G train loss: 0.748171]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.274913][G eval loss: 0.692554]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.266951][G train loss: 0.756676]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.275085][G eval loss: 0.688793]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.267660][G train loss: 0.758044]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.274784][G eval loss: 0.682104]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.267794][G train loss: 0.753363]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.274102][G eval loss: 0.673961]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.267471][G train loss: 0.744258]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.273076][G eval loss: 0.665758]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.266774][G train loss: 0.732988]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.271776][G eval loss: 0.659839]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.264460][G train loss: 0.727705]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.270449][G eval loss: 0.656812]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.263869][G train loss: 0.719453]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.268974][G eval loss: 0.653095]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.263518][G train loss: 0.709991]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.267350][G eval loss: 0.644006]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.262146][G train loss: 0.700500]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.265645][G eval loss: 0.631209]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.260653][G train loss: 0.687692]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.263884][G eval loss: 0.616473]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.259110][G train loss: 0.674519]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.262121][G eval loss: 0.601491]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.257557][G train loss: 0.661786]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.260413][G eval loss: 0.587277]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.256062][G train loss: 0.647335]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.258798][G eval loss: 0.572771]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.254673][G train loss: 0.630147]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.257313][G eval loss: 0.556291]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.253438][G train loss: 0.612674]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.255977][G eval loss: 0.541292]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.252407][G train loss: 0.595110]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.250647][G eval loss: 0.526947]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.251543][G train loss: 0.578663]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.247935][G eval loss: 0.513629]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.250782][G train loss: 0.563618]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.250005][G eval loss: 0.499328]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.249772][G train loss: 0.549806]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.250145][G eval loss: 0.485405]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.250345][G train loss: 0.537595]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.250419][G eval loss: 0.471541]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.250345][G train loss: 0.527444]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.250777][G eval loss: 0.459156]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.250176][G train loss: 0.517957]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.251181][G eval loss: 0.449695]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.250270][G train loss: 0.510301]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.251562][G eval loss: 0.442423]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.250863][G train loss: 0.503094]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.251865][G eval loss: 0.438850]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.251387][G train loss: 0.497983]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.252068][G eval loss: 0.435366]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.251826][G train loss: 0.494250]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.252181][G eval loss: 0.431651]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.252084][G train loss: 0.489653]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.252155][G eval loss: 0.431650]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.252032][G train loss: 0.486471]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.252020][G eval loss: 0.432336]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.251892][G train loss: 0.485627]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.251804][G eval loss: 0.435605]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.006 lr_d=0.002 -> score=0.687410\n",
      "\n",
      "----- 0050: lr_g=0.008, lr_d=0.0003 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.111300]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249999][G train loss: 1.262979]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.110633]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.262101]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.102638]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.254337]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 1.104812]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.256608]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.106525]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.258304]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 1.106614]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249980][G train loss: 1.258307]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 1.105937]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.257519]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 1.104512]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.255993]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 1.102760]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249960][G train loss: 1.254165]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249959][G eval loss: 1.101748]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249951][G train loss: 1.253114]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249950][G eval loss: 1.100450]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249941][G train loss: 1.251754]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 1.098195]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249928][G train loss: 1.249318]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249927][G eval loss: 1.094290]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249914][G train loss: 1.245054]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249913][G eval loss: 1.086372]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249899][G train loss: 1.236590]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249899][G eval loss: 1.070751]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249884][G train loss: 1.220033]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249889][G eval loss: 1.042466]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249871][G train loss: 1.189763]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249882][G eval loss: 0.996634]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249862][G train loss: 1.140934]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249878][G eval loss: 0.932774]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249856][G train loss: 1.073661]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249870][G eval loss: 0.856461]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249845][G train loss: 0.993649]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249848][G eval loss: 0.780191]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249823][G train loss: 0.911508]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249815][G eval loss: 0.739740]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249795][G train loss: 0.851127]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249804][G eval loss: 0.755401]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249779][G train loss: 0.830996]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249821][G eval loss: 0.768131]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.008 lr_d=0.0003 -> score=1.017952\n",
      "\n",
      "----- 0050: lr_g=0.008, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 1.109991]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249999][G train loss: 1.261670]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.110373]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.261841]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.103427]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.255126]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 1.106280]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249979][G train loss: 1.258077]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 1.108053]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.259832]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 1.107638]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249961][G train loss: 1.259330]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 1.106200]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249945][G train loss: 1.257782]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249938][G eval loss: 1.104013]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249925][G train loss: 1.255495]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 1.101643]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249901][G train loss: 1.253048]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249893][G eval loss: 1.100241]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249872][G train loss: 1.251607]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249862][G eval loss: 1.098828]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249836][G train loss: 1.250133]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249823][G eval loss: 1.096733]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249790][G train loss: 1.247854]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249773][G eval loss: 1.093221]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249734][G train loss: 1.243982]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249712][G eval loss: 1.085849]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249665][G train loss: 1.236062]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249643][G eval loss: 1.070856]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249589][G train loss: 1.220127]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249577][G eval loss: 1.043101]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249515][G train loss: 1.190376]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249520][G eval loss: 0.997727]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249450][G train loss: 1.141990]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249475][G eval loss: 0.934330]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249396][G train loss: 1.075156]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249415][G eval loss: 0.858528]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249329][G train loss: 0.995636]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249290][G eval loss: 0.782703]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249210][G train loss: 0.913912]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249096][G eval loss: 0.742339]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249045][G train loss: 0.853711]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248992][G eval loss: 0.757850]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248926][G train loss: 0.833470]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249036][G eval loss: 0.770348]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.008 lr_d=0.0005 -> score=1.019384\n",
      "\n",
      "----- 0050: lr_g=0.008, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250011][G eval loss: 1.108062]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250010][G train loss: 1.259740]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.110099]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249996][G train loss: 1.261567]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.104470]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.256169]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249985][G eval loss: 1.107875]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249980][G train loss: 1.259672]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.109393]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.261172]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 1.108370]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249944][G train loss: 1.260063]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249926][G eval loss: 1.106239]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249910][G train loss: 1.257822]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249888][G eval loss: 1.103428]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249866][G train loss: 1.254910]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249839][G eval loss: 1.100726]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249809][G train loss: 1.252131]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249773][G eval loss: 1.099216]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249731][G train loss: 1.250583]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249683][G eval loss: 1.097890]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249625][G train loss: 1.249196]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249557][G eval loss: 1.096013]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249481][G train loss: 1.247137]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249384][G eval loss: 1.092836]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249286][G train loss: 1.243598]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249172][G eval loss: 1.085860]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249053][G train loss: 1.236067]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248914][G eval loss: 1.071330]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248770][G train loss: 1.220582]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248612][G eval loss: 1.044127]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248448][G train loss: 1.191351]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.248326][G eval loss: 0.999334]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.248139][G train loss: 1.143497]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.248125][G eval loss: 0.936340]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247915][G train loss: 1.076950]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.247812][G eval loss: 0.860317]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.247601][G train loss: 0.997089]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.247200][G eval loss: 0.784067]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.247057][G train loss: 0.914798]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.246275][G eval loss: 0.742524]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.246318][G train loss: 0.853603]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.245764][G eval loss: 0.755805]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.245796][G train loss: 0.831495]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.245996][G eval loss: 0.766853]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.008 lr_d=0.0008 -> score=1.012849\n",
      "\n",
      "----- 0050: lr_g=0.008, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250057][G eval loss: 1.103930]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250056][G train loss: 1.255609]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.109273]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.260741]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 1.106685]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.258384]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.111312]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.263108]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 1.112530]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.264310]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 1.110859]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249898][G train loss: 1.262552]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249830][G eval loss: 1.108146]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249796][G train loss: 1.259729]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249701][G eval loss: 1.104887]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249648][G train loss: 1.256370]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249499][G eval loss: 1.101681]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249411][G train loss: 1.253088]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248978][G eval loss: 1.099555]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248820][G train loss: 1.250924]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248430][G eval loss: 1.096695]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248216][G train loss: 1.248003]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247601][G eval loss: 1.093751]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247324][G train loss: 1.244875]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246365][G eval loss: 1.090280]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246009][G train loss: 1.241023]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244503][G eval loss: 1.084300]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.244049][G train loss: 1.234425]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.242136][G eval loss: 1.071603]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.241552][G train loss: 1.220615]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.239349][G eval loss: 1.046755]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.238614][G train loss: 1.193407]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.236449][G eval loss: 1.004501]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.235735][G train loss: 1.147170]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.234909][G eval loss: 0.943515]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.234182][G train loss: 1.081162]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.234638][G eval loss: 0.869925]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.232956][G train loss: 1.003813]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.231991][G eval loss: 0.795442]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.229540][G train loss: 0.924918]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.223021][G eval loss: 0.754316]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.224113][G train loss: 0.862570]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.225097][G eval loss: 0.745147]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.222137][G train loss: 0.830139]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.233978][G eval loss: 0.735295]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.227390][G train loss: 0.806090]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.241749][G eval loss: 0.709484]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.753925]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.278550][G eval loss: 0.657139]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.275607][G train loss: 0.727751]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.296262][G eval loss: 0.598213]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.293684][G train loss: 0.660514]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.310288][G eval loss: 0.532755]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.301067][G train loss: 0.589953]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.312422][G eval loss: 0.532567]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.303153][G train loss: 0.585789]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.314156][G eval loss: 0.538703]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.008 lr_d=0.0015 -> score=0.852859\n",
      "\n",
      "----- 0050: lr_g=0.008, lr_d=0.002 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250113][G eval loss: 1.100975]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250111][G train loss: 1.252654]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 1.108755]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.260223]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 1.108144]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.259843]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 1.112734]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249980][G train loss: 1.264531]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249954][G eval loss: 1.114220]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249932][G train loss: 1.266000]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249819][G eval loss: 1.112639]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249775][G train loss: 1.264333]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249545][G eval loss: 1.109924]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249464][G train loss: 1.261509]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249097][G eval loss: 1.107688]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248948][G train loss: 1.259172]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.248370][G eval loss: 1.105860]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.248123][G train loss: 1.257268]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.247133][G eval loss: 1.105828]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.246747][G train loss: 1.257200]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.245186][G eval loss: 1.106606]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.244591][G train loss: 1.257916]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.242104][G eval loss: 1.108175]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.241265][G train loss: 1.259286]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.237661][G eval loss: 1.109718]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.236521][G train loss: 1.260400]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.231619][G eval loss: 1.108833]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.230118][G train loss: 1.258858]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.223142][G eval loss: 1.102160]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.221528][G train loss: 1.250794]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.212237][G eval loss: 1.083028]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.211312][G train loss: 1.228299]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.202367][G eval loss: 1.040571]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.207183][G train loss: 1.173059]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.219116][G eval loss: 0.949870]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.213950][G train loss: 1.094835]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.224013][G eval loss: 0.906333]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.226450][G train loss: 1.020290]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.225122][G eval loss: 0.864860]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.235439][G train loss: 0.950793]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.224624][G eval loss: 0.787821]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.234927][G train loss: 0.870308]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.225252][G eval loss: 0.733687]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.232374][G train loss: 0.811871]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.242609][G eval loss: 0.728870]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.252382][G train loss: 0.774979]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.275348][G eval loss: 0.693225]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.268919][G train loss: 0.753060]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.287269][G eval loss: 0.669223]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.291925][G train loss: 0.705729]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.341530][G eval loss: 0.563109]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.316398][G train loss: 0.664579]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.343227][G eval loss: 0.564852]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.328604][G train loss: 0.650287]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.341473][G eval loss: 0.558936]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.328367][G train loss: 0.635191]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.338212][G eval loss: 0.563825]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.325286][G train loss: 0.617993]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.333939][G eval loss: 0.581405]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.008 lr_d=0.002 -> score=0.915344\n",
      "\n",
      "----- 0050: lr_g=0.01, lr_d=0.0003 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.111486]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249999][G train loss: 1.263190]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.107692]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.259232]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.103308]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.255038]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 1.105086]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.256840]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.105441]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.257074]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 1.104617]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249979][G train loss: 1.256139]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 1.103702]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.255172]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 1.102315]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.253736]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 1.101027]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249959][G train loss: 1.252315]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249957][G eval loss: 1.099241]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249950][G train loss: 1.250301]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249947][G eval loss: 1.092689]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249939][G train loss: 1.243088]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249937][G eval loss: 1.078345]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249927][G train loss: 1.227456]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249926][G eval loss: 1.047472]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249915][G train loss: 1.194633]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.990711]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249903][G train loss: 1.134367]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249907][G eval loss: 0.907179]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249895][G train loss: 1.044509]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249896][G eval loss: 0.804781]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249885][G train loss: 0.929876]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 0.736772]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249869][G train loss: 0.843424]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249887][G eval loss: 0.751473]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249859][G train loss: 0.820571]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 0.761201]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.01 lr_d=0.0003 -> score=1.011101\n",
      "\n",
      "----- 0050: lr_g=0.01, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 1.110177]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250000][G train loss: 1.261881]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.107424]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.258964]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 1.104090]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249986][G train loss: 1.255820]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 1.106550]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249980][G train loss: 1.258303]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 1.106967]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.258601]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.105644]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249960][G train loss: 1.257166]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 1.103973]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249946][G train loss: 1.255443]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249938][G eval loss: 1.101827]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249925][G train loss: 1.253247]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 1.099922]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249899][G train loss: 1.251209]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249888][G eval loss: 1.097749]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249867][G train loss: 1.248807]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249854][G eval loss: 1.091087]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249829][G train loss: 1.241485]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249815][G eval loss: 1.076896]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249785][G train loss: 1.226009]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249767][G eval loss: 1.046439]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249732][G train loss: 1.193592]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249712][G eval loss: 0.990271]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249673][G train loss: 1.133917]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249662][G eval loss: 0.907487]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249620][G train loss: 1.044827]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249603][G eval loss: 0.805895]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249561][G train loss: 0.931038]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249533][G eval loss: 0.738307]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249480][G train loss: 0.845022]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249524][G eval loss: 0.752661]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249412][G train loss: 0.822126]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249578][G eval loss: 0.763468]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.01 lr_d=0.0005 -> score=1.013047\n",
      "\n",
      "----- 0050: lr_g=0.01, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250012][G eval loss: 1.108247]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250011][G train loss: 1.259951]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.107148]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.258688]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.105129]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.256859]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 1.108136]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249979][G train loss: 1.259890]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 1.108293]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.259926]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249952][G eval loss: 1.106363]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249941][G train loss: 1.257884]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249923][G eval loss: 1.104011]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249908][G train loss: 1.255480]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 1.101264]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249861][G train loss: 1.252684]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 1.099016]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249797][G train loss: 1.250301]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249754][G eval loss: 1.096728]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249713][G train loss: 1.247784]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249650][G eval loss: 1.090176]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249596][G train loss: 1.240570]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249510][G eval loss: 1.076183]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249441][G train loss: 1.225297]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249331][G eval loss: 1.046019]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249249][G train loss: 1.193136]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249115][G eval loss: 0.990182]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249022][G train loss: 1.133817]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248890][G eval loss: 0.907479]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248794][G train loss: 1.044926]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248638][G eval loss: 0.807133]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248544][G train loss: 0.932238]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.248345][G eval loss: 0.740359]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.248200][G train loss: 0.846718]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.248299][G eval loss: 0.753878]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247940][G train loss: 0.823660]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.248494][G eval loss: 0.763046]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.01 lr_d=0.0008 -> score=1.011540\n",
      "\n",
      "----- 0050: lr_g=0.01, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250058][G eval loss: 1.104115]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250057][G train loss: 1.255819]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.106322]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 1.257862]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 1.107331]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.259061]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.111569]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.263323]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 1.111430]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249960][G train loss: 1.263063]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 1.108840]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249895][G train loss: 1.260361]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249836][G eval loss: 1.105933]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249807][G train loss: 1.257401]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249713][G eval loss: 1.102682]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249669][G train loss: 1.254100]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249489][G eval loss: 1.099897]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249404][G train loss: 1.251176]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248968][G eval loss: 1.096605]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248846][G train loss: 1.247644]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248343][G eval loss: 1.089095]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248190][G train loss: 1.239463]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247439][G eval loss: 1.074647]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247247][G train loss: 1.223719]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246210][G eval loss: 1.044775]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245985][G train loss: 1.191697]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244473][G eval loss: 0.990166]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.244248][G train loss: 1.133468]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.242412][G eval loss: 0.909368]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.242166][G train loss: 1.046412]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.240064][G eval loss: 0.812727]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.239751][G train loss: 0.937256]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.237505][G eval loss: 0.746313]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.236800][G train loss: 0.852003]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.237002][G eval loss: 0.747413]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.234582][G train loss: 0.821783]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.243394][G eval loss: 0.747752]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.01 lr_d=0.0015 -> score=0.991145\n",
      "\n",
      "----- 0050: lr_g=0.01, lr_d=0.002 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250114][G eval loss: 1.101161]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250112][G train loss: 1.252865]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 1.105803]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.257343]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 1.108801]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.260531]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.113005]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.264758]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 1.113113]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249928][G train loss: 1.264746]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249817][G eval loss: 1.110632]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249777][G train loss: 1.262152]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249553][G eval loss: 1.107708]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249482][G train loss: 1.259171]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249095][G eval loss: 1.105510]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248971][G train loss: 1.256916]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.248299][G eval loss: 1.104229]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.248098][G train loss: 1.255489]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.247014][G eval loss: 1.103506]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.246708][G train loss: 1.254507]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.245043][G eval loss: 1.099272]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.244584][G train loss: 1.249598]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.242000][G eval loss: 1.088635]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.241389][G train loss: 1.237668]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.237684][G eval loss: 1.063464]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.236902][G train loss: 1.210081]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.231966][G eval loss: 1.013106]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.230989][G train loss: 1.156114]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.224366][G eval loss: 0.937365]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.223662][G train loss: 1.073738]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.215145][G eval loss: 0.848279]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.215330][G train loss: 0.971101]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.209497][G eval loss: 0.775997]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.211980][G train loss: 0.876758]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.248096][G eval loss: 0.692426]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242523][G train loss: 0.781433]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.272152][G eval loss: 0.656691]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.268560][G train loss: 0.733698]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.281675][G eval loss: 0.612606]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.277941][G train loss: 0.694355]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.287073][G eval loss: 0.567305]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.283381][G train loss: 0.642623]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.289164][G eval loss: 0.540499]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.284270][G train loss: 0.601163]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.288783][G eval loss: 0.518055]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.282182][G train loss: 0.571494]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.285903][G eval loss: 0.497450]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.280553][G train loss: 0.554724]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.281557][G eval loss: 0.493995]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.277987][G train loss: 0.553404]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.276385][G eval loss: 0.484633]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.273484][G train loss: 0.546252]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.270278][G eval loss: 0.489666]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.268342][G train loss: 0.556506]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.264016][G eval loss: 0.491390]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.264501][G train loss: 0.558614]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.259616][G eval loss: 0.480744]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.261787][G train loss: 0.549643]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.256844][G eval loss: 0.488666]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.259312][G train loss: 0.556726]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.254553][G eval loss: 0.486113]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.256736][G train loss: 0.555091]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.253039][G eval loss: 0.482512]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.254552][G train loss: 0.550706]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.252101][G eval loss: 0.483261]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.252905][G train loss: 0.549026]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.251395][G eval loss: 0.476436]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.251586][G train loss: 0.538251]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.250929][G eval loss: 0.472171]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.250905][G train loss: 0.529995]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250570][G eval loss: 0.472780]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.250552][G train loss: 0.526357]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.250319][G eval loss: 0.472626]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.250307][G train loss: 0.523897]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.250182][G eval loss: 0.469418]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.250179][G train loss: 0.519727]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.250312][G eval loss: 0.474114]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.250289][G train loss: 0.526094]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.250309][G eval loss: 0.474307]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.250271][G train loss: 0.529701]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.250334][G eval loss: 0.472105]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.250294][G train loss: 0.531650]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.250409][G eval loss: 0.473301]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.250377][G train loss: 0.534778]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.250515][G eval loss: 0.474866]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.250497][G train loss: 0.536291]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.250582][G eval loss: 0.476892]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.250557][G train loss: 0.537848]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.250641][G eval loss: 0.477157]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.250619][G train loss: 0.539212]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.250660][G eval loss: 0.479023]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.250637][G train loss: 0.542087]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.250616][G eval loss: 0.475029]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.250605][G train loss: 0.539117]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.250585][G eval loss: 0.472919]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.250576][G train loss: 0.537972]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.250620][G eval loss: 0.473374]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.250596][G train loss: 0.537739]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.250570][G eval loss: 0.469089]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.250538][G train loss: 0.534259]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.250498][G eval loss: 0.468325]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.250472][G train loss: 0.532274]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.250592][G eval loss: 0.473834]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.250551][G train loss: 0.533070]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.250680][G eval loss: 0.473262]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.250630][G train loss: 0.530736]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.250630][G eval loss: 0.466906]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.250595][G train loss: 0.524924]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.250505][G eval loss: 0.464018]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.250437][G train loss: 0.528949]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.250399][G eval loss: 0.464502]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.250315][G train loss: 0.530959]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.250361][G eval loss: 0.459148]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.250281][G train loss: 0.523324]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.250421][G eval loss: 0.467604]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.250380][G train loss: 0.526948]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.250363][G eval loss: 0.473726]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.250366][G train loss: 0.532513]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.250160][G eval loss: 0.463210]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.250143][G train loss: 0.521827]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.249620][G eval loss: 0.459757]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.249424][G train loss: 0.517739]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.249074][G eval loss: 0.463129]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.248068][G train loss: 0.523093]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.248753][G eval loss: 0.459908]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.248234][G train loss: 0.515737]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.248820][G eval loss: 0.456099]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.248835][G train loss: 0.511046]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.248597][G eval loss: 0.457324]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.248713][G train loss: 0.511360]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.247584][G eval loss: 0.455486]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.247172][G train loss: 0.508816]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.243246][G eval loss: 0.462427]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.241249][G train loss: 0.519950]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.240537][G eval loss: 0.470722]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.239657][G train loss: 0.524819]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.240037][G eval loss: 0.469830]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.241274][G train loss: 0.517789]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.242437][G eval loss: 0.467371]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.244468][G train loss: 0.510148]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.247483][G eval loss: 0.469569]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.248379][G train loss: 0.513655]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.247785][G eval loss: 0.484863]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.248852][G train loss: 0.524558]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.247826][G eval loss: 0.482566]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.248882][G train loss: 0.521297]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.245750][G eval loss: 0.471824]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.247770][G train loss: 0.510625]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.241408][G eval loss: 0.467089]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.242968][G train loss: 0.511011]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.232610][G eval loss: 0.489707]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.241346][G train loss: 0.516697]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.227492][G eval loss: 0.501323]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.240054][G train loss: 0.515310]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.232321][G eval loss: 0.488273]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.241878][G train loss: 0.508702]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.241490][G eval loss: 0.472714]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.245253][G train loss: 0.508985]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.241740][G eval loss: 0.474347]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.245858][G train loss: 0.510970]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.238350][G eval loss: 0.475549]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.244778][G train loss: 0.508647]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.240339][G eval loss: 0.468403]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.246024][G train loss: 0.506448]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.236822][G eval loss: 0.472942]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.244468][G train loss: 0.505289]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.241681][G eval loss: 0.458340]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.244751][G train loss: 0.505145]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.240027][G eval loss: 0.464569]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.245266][G train loss: 0.505965]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.239162][G eval loss: 0.463728]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.244220][G train loss: 0.508007]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.237499][G eval loss: 0.471867]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.244458][G train loss: 0.513289]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.239210][G eval loss: 0.471786]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.244376][G train loss: 0.516665]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.238149][G eval loss: 0.476910]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.242229][G train loss: 0.524874]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.237879][G eval loss: 0.481669]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.239110][G train loss: 0.540361]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.237983][G eval loss: 0.489641]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.239341][G train loss: 0.557401]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.230219][G eval loss: 0.528765]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.235946][G train loss: 0.578094]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.227465][G eval loss: 0.544510]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.230577][G train loss: 0.597763]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.235109][G eval loss: 0.528731]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.228533][G train loss: 0.603259]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.253754][G eval loss: 0.528887]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.238588][G train loss: 0.602255]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.266561][G eval loss: 0.552278]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.243217][G train loss: 0.639288]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.252345][G eval loss: 0.636008]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.232940][G train loss: 0.708905]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.235780][G eval loss: 0.712293]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.217033][G train loss: 0.779459]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.217427][G eval loss: 0.774879]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.201644][G train loss: 0.831702]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.200234][G eval loss: 0.822497]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.192268][G train loss: 0.864947]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.203712][G eval loss: 0.838808]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.205380][G train loss: 0.863896]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.231170][G eval loss: 0.800142]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.238550][G train loss: 0.821760]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.248139][G eval loss: 0.763217]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.248779][G train loss: 0.865501]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.239034][G eval loss: 0.868872]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.246390][G train loss: 0.976392]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.233536][G eval loss: 0.963113]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.246533][G train loss: 1.060587]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.238700][G eval loss: 0.970387]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.240856][G train loss: 1.104545]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.230855][G eval loss: 0.985557]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.231358][G train loss: 1.105036]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.236535][G eval loss: 0.968299]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.216857][G train loss: 1.107531]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.238956][G eval loss: 0.971862]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.211885][G train loss: 1.094917]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.238818][G eval loss: 0.975514]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.01 lr_d=0.002 -> score=1.214332\n",
      "\n",
      ">>> Best config for 0050: lr_g=0.006, lr_d=0.002, score=0.687410\n",
      "\n",
      "========== Grid search for 0056 ==========\n",
      "\n",
      "----- 0056: lr_g=0.002, lr_d=0.0003 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.997419]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249996][G train loss: 1.099867]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.986769]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.089282]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.979795]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.082401]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.975982]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249987][G train loss: 1.078711]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.974410]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.077299]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.973818]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.076875]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.973274]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.076471]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.972644]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249964][G train loss: 1.075937]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.971983]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.075336]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249961][G eval loss: 0.971464]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249946][G train loss: 1.074849]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 0.971166]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249936][G train loss: 1.074557]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249943][G eval loss: 0.971077]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249924][G train loss: 1.074452]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249933][G eval loss: 0.971139]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249910][G train loss: 1.074481]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 0.971282]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249895][G train loss: 1.074579]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249908][G eval loss: 0.971441]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249878][G train loss: 1.074684]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249892][G eval loss: 0.971563]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249858][G train loss: 1.074748]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249874][G eval loss: 0.971614]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249834][G train loss: 1.074740]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249853][G eval loss: 0.971587]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249807][G train loss: 1.074656]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.971489]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249776][G train loss: 1.074505]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249799][G eval loss: 0.971336]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249741][G train loss: 1.074307]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249766][G eval loss: 0.971158]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249701][G train loss: 1.074089]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249729][G eval loss: 0.970978]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249656][G train loss: 1.073878]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249688][G eval loss: 0.970816]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249605][G train loss: 1.073690]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249643][G eval loss: 0.970677]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249550][G train loss: 1.073535]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.249592][G eval loss: 0.970503]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249488][G train loss: 1.073353]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.249537][G eval loss: 0.970391]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249421][G train loss: 1.073240]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.249475][G eval loss: 0.970352]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.249346][G train loss: 1.073207]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.249405][G eval loss: 0.970390]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.249261][G train loss: 1.073254]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.249330][G eval loss: 0.970482]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.249169][G train loss: 1.073354]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.249242][G eval loss: 0.970595]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.249062][G train loss: 1.073478]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.249139][G eval loss: 0.970726]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.248939][G train loss: 1.073616]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.249027][G eval loss: 0.970873]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.248804][G train loss: 1.073766]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.248900][G eval loss: 0.971040]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.248654][G train loss: 1.073935]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.248758][G eval loss: 0.971225]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.248486][G train loss: 1.074115]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.248601][G eval loss: 0.971414]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248301][G train loss: 1.074294]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.248426][G eval loss: 0.971598]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.248095][G train loss: 1.074461]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.248221][G eval loss: 0.971791]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.247856][G train loss: 1.074633]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.247987][G eval loss: 0.971967]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.247587][G train loss: 1.074782]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.247729][G eval loss: 0.972111]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.247292][G train loss: 1.074891]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.247451][G eval loss: 0.972201]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.246975][G train loss: 1.074944]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.247191][G eval loss: 0.972167]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.246667][G train loss: 1.074873]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.246898][G eval loss: 0.971980]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.246332][G train loss: 1.074636]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.246571][G eval loss: 0.971687]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.245960][G train loss: 1.074283]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.246215][G eval loss: 0.971266]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.245559][G train loss: 1.073794]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.245838][G eval loss: 0.970729]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.245135][G train loss: 1.073174]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.245433][G eval loss: 0.970079]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.244684][G train loss: 1.072428]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.244990][G eval loss: 0.969308]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.244195][G train loss: 1.071540]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.244504][G eval loss: 0.968372]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.243663][G train loss: 1.070459]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.243974][G eval loss: 0.967191]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.243087][G train loss: 1.069123]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.243401][G eval loss: 0.965663]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.242467][G train loss: 1.067430]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.242784][G eval loss: 0.963678]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.241804][G train loss: 1.065253]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.242123][G eval loss: 0.961115]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.241107][G train loss: 1.062427]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.241438][G eval loss: 0.957808]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.240396][G train loss: 1.058746]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.240749][G eval loss: 0.953413]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.239715][G train loss: 1.053884]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.240086][G eval loss: 0.947491]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.239114][G train loss: 1.047360]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.239468][G eval loss: 0.939721]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.238646][G train loss: 1.038720]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.238938][G eval loss: 0.929852]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.238385][G train loss: 1.027712]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.238612][G eval loss: 0.917903]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.238430][G train loss: 1.014434]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.238567][G eval loss: 0.904513]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.238921][G train loss: 0.999399]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.238864][G eval loss: 0.890906]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.239925][G train loss: 0.983725]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.239505][G eval loss: 0.878939]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.241268][G train loss: 0.969531]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.240422][G eval loss: 0.870355]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.242671][G train loss: 0.958934]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.241342][G eval loss: 0.865036]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.243818][G train loss: 0.951917]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.241868][G eval loss: 0.860584]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.244445][G train loss: 0.946019]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.241987][G eval loss: 0.854511]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.244606][G train loss: 0.938942]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.241711][G eval loss: 0.845913]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.244373][G train loss: 0.929688]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.241061][G eval loss: 0.835199]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.243786][G train loss: 0.918640]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.239951][G eval loss: 0.824483]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.242851][G train loss: 0.907320]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.238197][G eval loss: 0.816169]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.241622][G train loss: 0.897445]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.236275][G eval loss: 0.810346]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.240166][G train loss: 0.890161]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.234817][G eval loss: 0.805612]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.238844][G train loss: 0.884481]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.233842][G eval loss: 0.800234]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.237989][G train loss: 0.878115]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.233467][G eval loss: 0.792312]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.237752][G train loss: 0.869262]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.233793][G eval loss: 0.780557]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.238170][G train loss: 0.857028]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.234913][G eval loss: 0.764744]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.239203][G train loss: 0.841201]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.236944][G eval loss: 0.745561]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.240672][G train loss: 0.822432]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.239808][G eval loss: 0.724572]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.242575][G train loss: 0.801533]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.242650][G eval loss: 0.704789]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.244962][G train loss: 0.779681]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.245297][G eval loss: 0.687722]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.248210][G train loss: 0.757829]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.248879][G eval loss: 0.671886]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.251746][G train loss: 0.738215]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.252762][G eval loss: 0.657584]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.254994][G train loss: 0.722307]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.256213][G eval loss: 0.643758]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.257785][G train loss: 0.709227]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.258839][G eval loss: 0.630742]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.260044][G train loss: 0.697336]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.260749][G eval loss: 0.617682]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.261956][G train loss: 0.684836]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.262108][G eval loss: 0.603052]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.263628][G train loss: 0.670680]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.263016][G eval loss: 0.586394]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.264898][G train loss: 0.655166]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.263421][G eval loss: 0.568940]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.265705][G train loss: 0.638940]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.263374][G eval loss: 0.551639]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.266070][G train loss: 0.622527]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.262902][G eval loss: 0.535989]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.266081][G train loss: 0.607020]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.261961][G eval loss: 0.523836]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.265795][G train loss: 0.593401]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.260625][G eval loss: 0.515359]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.265301][G train loss: 0.582506]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.259071][G eval loss: 0.509945]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.264685][G train loss: 0.574792]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.257836][G eval loss: 0.506427]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.264081][G train loss: 0.569916]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.257453][G eval loss: 0.502922]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.263660][G train loss: 0.566620]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.258054][G eval loss: 0.499064]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.263405][G train loss: 0.563881]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.258816][G eval loss: 0.495345]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.263199][G train loss: 0.561070]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.260097][G eval loss: 0.489403]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.263031][G train loss: 0.557837]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.262078][G eval loss: 0.480815]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.262878][G train loss: 0.553411]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.263644][G eval loss: 0.472659]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.262719][G train loss: 0.548378]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.264809][G eval loss: 0.464469]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.262551][G train loss: 0.542815]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.265733][G eval loss: 0.456682]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.262364][G train loss: 0.536892]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.266232][G eval loss: 0.450536]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.262120][G train loss: 0.531405]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.266363][G eval loss: 0.444715]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.261806][G train loss: 0.526700]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.266329][G eval loss: 0.441309]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.261441][G train loss: 0.523331]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.266171][G eval loss: 0.440631]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.261026][G train loss: 0.521582]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.265987][G eval loss: 0.441082]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.260625][G train loss: 0.521026]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.265779][G eval loss: 0.441907]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.002 lr_d=0.0003 -> score=0.707686\n",
      "\n",
      "----- 0056: lr_g=0.002, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.996111]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.098559]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.986498]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.089012]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.980560]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.083166]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.977426]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.080155]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.975924]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249965][G train loss: 1.078813]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249965][G eval loss: 0.974840]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249951][G train loss: 1.077897]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249950][G eval loss: 0.973551]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249932][G train loss: 1.076749]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249932][G eval loss: 0.972179]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249909][G train loss: 1.075472]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249911][G eval loss: 0.970914]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249882][G train loss: 1.074267]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249886][G eval loss: 0.970006]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249849][G train loss: 1.073391]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249856][G eval loss: 0.969581]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249810][G train loss: 1.072972]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249820][G eval loss: 0.969628]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249763][G train loss: 1.073002]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249775][G eval loss: 0.970052]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249706][G train loss: 1.073393]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249719][G eval loss: 0.970731]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249635][G train loss: 1.074028]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249648][G eval loss: 0.971533]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249547][G train loss: 1.074775]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249562][G eval loss: 0.972344]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249440][G train loss: 1.075528]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249459][G eval loss: 0.973084]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249314][G train loss: 1.076208]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249350][G eval loss: 0.973691]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249176][G train loss: 1.076757]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249196][G eval loss: 0.974100]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.248989][G train loss: 1.077114]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249031][G eval loss: 0.974348]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248786][G train loss: 1.077316]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248839][G eval loss: 0.974491]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248550][G train loss: 1.077418]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248618][G eval loss: 0.974528]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248278][G train loss: 1.077423]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248358][G eval loss: 0.974486]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.247961][G train loss: 1.077355]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248051][G eval loss: 0.974364]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247589][G train loss: 1.077216]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247679][G eval loss: 0.974314]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247144][G train loss: 1.077156]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247235][G eval loss: 0.974522]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246617][G train loss: 1.077361]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246713][G eval loss: 0.974598]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246013][G train loss: 1.077441]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246085][G eval loss: 0.974749]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245301][G train loss: 1.077594]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245386][G eval loss: 0.975186]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244493][G train loss: 1.078035]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.244624][G eval loss: 0.975839]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243595][G train loss: 1.078698]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.243738][G eval loss: 0.976505]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.242569][G train loss: 1.079365]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.242688][G eval loss: 0.977281]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.241373][G train loss: 1.080137]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.241482][G eval loss: 0.978180]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.240016][G train loss: 1.081029]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.240127][G eval loss: 0.979261]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.238507][G train loss: 1.082096]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.238621][G eval loss: 0.980517]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.236831][G train loss: 1.083332]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.236999][G eval loss: 0.981782]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.235032][G train loss: 1.084554]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.235167][G eval loss: 0.983216]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.233024][G train loss: 1.085923]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.233114][G eval loss: 0.984834]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.230790][G train loss: 1.087467]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.230912][G eval loss: 0.986518]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.228382][G train loss: 1.089078]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.228604][G eval loss: 0.988139]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.225863][G train loss: 1.090613]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.226138][G eval loss: 0.989758]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.223174][G train loss: 1.092131]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.223413][G eval loss: 0.991496]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.220263][G train loss: 1.093688]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.220393][G eval loss: 0.993481]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.217089][G train loss: 1.095426]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.217121][G eval loss: 0.995656]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.213678][G train loss: 1.097323]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.213674][G eval loss: 0.998067]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.210032][G train loss: 1.099351]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.210013][G eval loss: 1.000725]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.206205][G train loss: 1.101463]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.206235][G eval loss: 1.003505]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.202256][G train loss: 1.103508]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.202499][G eval loss: 1.006060]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.198374][G train loss: 1.105057]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.198938][G eval loss: 1.008029]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.194868][G train loss: 1.105236]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.195753][G eval loss: 1.008941]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.192402][G train loss: 1.102749]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.193802][G eval loss: 1.007262]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.192444][G train loss: 1.094667]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.193711][G eval loss: 1.001388]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.197151][G train loss: 1.077778]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.197243][G eval loss: 0.988130]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.205579][G train loss: 1.054675]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.203517][G eval loss: 0.970392]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.215649][G train loss: 1.028835]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.209482][G eval loss: 0.953903]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.226838][G train loss: 1.001848]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.216783][G eval loss: 0.935313]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.238279][G train loss: 0.976721]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.229190][G eval loss: 0.909480]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.249630][G train loss: 0.953796]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.241802][G eval loss: 0.890527]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.262889][G train loss: 0.931495]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.251966][G eval loss: 0.881966]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.276142][G train loss: 0.916252]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.262331][G eval loss: 0.878978]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.285975][G train loss: 0.913887]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.271833][G eval loss: 0.880459]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.292783][G train loss: 0.917767]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.281596][G eval loss: 0.883157]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.300299][G train loss: 0.920286]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.290198][G eval loss: 0.889682]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.307830][G train loss: 0.923440]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.295651][G eval loss: 0.897628]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.313608][G train loss: 0.925443]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.298910][G eval loss: 0.903858]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.317604][G train loss: 0.927610]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.300660][G eval loss: 0.905903]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.318969][G train loss: 0.928381]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.301294][G eval loss: 0.903521]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.319139][G train loss: 0.924862]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.300882][G eval loss: 0.897446]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.318492][G train loss: 0.917285]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.299504][G eval loss: 0.888276]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.316832][G train loss: 0.906598]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.297083][G eval loss: 0.877193]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.314497][G train loss: 0.893679]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.293634][G eval loss: 0.865002]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.311318][G train loss: 0.880105]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.289203][G eval loss: 0.852864]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.307217][G train loss: 0.867085]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.284036][G eval loss: 0.842317]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.302132][G train loss: 0.856329]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.279081][G eval loss: 0.832866]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.296995][G train loss: 0.847229]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.274416][G eval loss: 0.825464]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.291336][G train loss: 0.841563]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.270456][G eval loss: 0.820684]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.285201][G train loss: 0.839669]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.267517][G eval loss: 0.818252]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.276961][G train loss: 0.845214]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.265380][G eval loss: 0.817542]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.271601][G train loss: 0.852764]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.263688][G eval loss: 0.817459]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.269107][G train loss: 0.855382]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.262447][G eval loss: 0.817530]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.266923][G train loss: 0.856844]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.261706][G eval loss: 0.817375]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.265206][G train loss: 0.857059]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.261447][G eval loss: 0.816790]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.264888][G train loss: 0.854154]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.261672][G eval loss: 0.815939]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.265043][G train loss: 0.851287]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.261716][G eval loss: 0.814690]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.264718][G train loss: 0.847751]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.261049][G eval loss: 0.813552]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.265142][G train loss: 0.842753]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.260494][G eval loss: 0.811927]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.266319][G train loss: 0.836799]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.260472][G eval loss: 0.809323]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.266314][G train loss: 0.832529]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.260961][G eval loss: 0.806359]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.267160][G train loss: 0.826591]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.261675][G eval loss: 0.803672]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.266790][G train loss: 0.824975]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.262457][G eval loss: 0.801868]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.265818][G train loss: 0.824272]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.262612][G eval loss: 0.801345]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.264638][G train loss: 0.824054]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.262165][G eval loss: 0.801282]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.263332][G train loss: 0.823779]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.261263][G eval loss: 0.801116]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.261914][G train loss: 0.822911]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.260051][G eval loss: 0.800423]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.260489][G train loss: 0.821113]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.258663][G eval loss: 0.799212]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.259129][G train loss: 0.818289]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.257189][G eval loss: 0.797349]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.257872][G train loss: 0.814349]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.255885][G eval loss: 0.794505]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.256946][G train loss: 0.808989]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.254646][G eval loss: 0.791107]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.256269][G train loss: 0.802615]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.253497][G eval loss: 0.786932]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.255632][G train loss: 0.795834]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.252406][G eval loss: 0.781011]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.254759][G train loss: 0.788845]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.251366][G eval loss: 0.773890]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.253787][G train loss: 0.781536]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.250272][G eval loss: 0.765648]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.252766][G train loss: 0.773894]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.249319][G eval loss: 0.756450]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.251781][G train loss: 0.765748]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.248513][G eval loss: 0.746129]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.250465][G train loss: 0.758757]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.247758][G eval loss: 0.736711]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.248883][G train loss: 0.752356]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.247259][G eval loss: 0.727447]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.247603][G train loss: 0.745644]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.247044][G eval loss: 0.718042]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.246399][G train loss: 0.739779]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.247488][G eval loss: 0.709089]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.245040][G train loss: 0.735685]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.249272][G eval loss: 0.697327]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.243827][G train loss: 0.732633]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.258333][G eval loss: 0.668370]\n",
      "[Epoch 111/200][Batch 1/1][D train loss: 0.242789][G train loss: 0.730583]\n",
      "[Epoch 111/200][Batch 1/1][D eval loss: 0.258928][G eval loss: 0.663785]\n",
      "[Epoch 112/200][Batch 1/1][D train loss: 0.241985][G train loss: 0.729742]\n",
      "[Epoch 112/200][Batch 1/1][D eval loss: 0.259859][G eval loss: 0.661805]\n",
      "[Epoch 113/200][Batch 1/1][D train loss: 0.241494][G train loss: 0.730516]\n",
      "[Epoch 113/200][Batch 1/1][D eval loss: 0.261338][G eval loss: 0.660147]\n",
      "[Epoch 114/200][Batch 1/1][D train loss: 0.241169][G train loss: 0.729780]\n",
      "[Epoch 114/200][Batch 1/1][D eval loss: 0.263214][G eval loss: 0.657424]\n",
      "[Epoch 115/200][Batch 1/1][D train loss: 0.240361][G train loss: 0.731455]\n",
      "[Epoch 115/200][Batch 1/1][D eval loss: 0.264549][G eval loss: 0.656395]\n",
      "[Epoch 116/200][Batch 1/1][D train loss: 0.239471][G train loss: 0.733967]\n",
      "[Epoch 116/200][Batch 1/1][D eval loss: 0.264540][G eval loss: 0.660192]\n",
      "[Epoch 117/200][Batch 1/1][D train loss: 0.238971][G train loss: 0.734575]\n",
      "[Epoch 117/200][Batch 1/1][D eval loss: 0.264097][G eval loss: 0.667191]\n",
      "[Epoch 118/200][Batch 1/1][D train loss: 0.239030][G train loss: 0.732910]\n",
      "[Epoch 118/200][Batch 1/1][D eval loss: 0.264735][G eval loss: 0.669508]\n",
      "[Epoch 119/200][Batch 1/1][D train loss: 0.241690][G train loss: 0.722603]\n",
      "[Epoch 119/200][Batch 1/1][D eval loss: 0.265924][G eval loss: 0.670533]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.002 lr_d=0.0005 -> score=0.936457\n",
      "\n",
      "----- 0056: lr_g=0.002, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 0.994185]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250006][G train loss: 1.096633]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.986207]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.088720]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249985][G eval loss: 0.981575]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249978][G train loss: 1.084182]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.979004]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.081733]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249963][G eval loss: 0.977273]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249947][G train loss: 1.080162]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249936][G eval loss: 0.975632]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249912][G train loss: 1.078689]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249897][G eval loss: 0.973722]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249863][G train loss: 1.076920]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249847][G eval loss: 0.971781]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249802][G train loss: 1.075074]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249786][G eval loss: 0.970234]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249722][G train loss: 1.073587]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249709][G eval loss: 0.969212]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249625][G train loss: 1.072598]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249607][G eval loss: 0.969003]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249494][G train loss: 1.072393]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249469][G eval loss: 0.969326]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249318][G train loss: 1.072699]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249285][G eval loss: 0.970023]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249087][G train loss: 1.073363]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249039][G eval loss: 0.971001]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248784][G train loss: 1.074296]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248717][G eval loss: 0.972173]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248391][G train loss: 1.075414]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248304][G eval loss: 0.973471]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247887][G train loss: 1.076653]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247770][G eval loss: 0.974739]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247240][G train loss: 1.077860]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247140][G eval loss: 0.975905]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246469][G train loss: 1.078967]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246325][G eval loss: 0.976320]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245505][G train loss: 1.079328]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245322][G eval loss: 0.976771]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244343][G train loss: 1.079732]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244071][G eval loss: 0.977508]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.242930][G train loss: 1.080426]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242576][G eval loss: 0.978233]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241248][G train loss: 1.081115]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.240749][G eval loss: 0.979083]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239201][G train loss: 1.081933]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238524][G eval loss: 0.980209]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.236730][G train loss: 1.083035]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.235865][G eval loss: 0.981757]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.233792][G train loss: 1.084563]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.232930][G eval loss: 0.983446]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.230513][G train loss: 1.086245]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.229599][G eval loss: 0.985337]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.226786][G train loss: 1.088130]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.225761][G eval loss: 0.987602]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.222486][G train loss: 1.090383]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.221356][G eval loss: 0.990354]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.217560][G train loss: 1.093123]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.216326][G eval loss: 0.993728]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.211930][G train loss: 1.096471]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.210680][G eval loss: 0.997749]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.205584][G train loss: 1.100474]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.204360][G eval loss: 1.002474]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.198463][G train loss: 1.105170]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.197393][G eval loss: 1.007852]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.190639][G train loss: 1.110479]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.189900][G eval loss: 1.013556]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.182193][G train loss: 1.116130]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.181741][G eval loss: 1.019677]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.173171][G train loss: 1.122138]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.172763][G eval loss: 1.026911]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.163448][G train loss: 1.129196]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.163073][G eval loss: 1.035686]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.153187][G train loss: 1.137616]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.152871][G eval loss: 1.046437]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.142483][G train loss: 1.147979]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.142737][G eval loss: 1.058613]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.131922][G train loss: 1.159602]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.133073][G eval loss: 1.072114]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.121848][G train loss: 1.172349]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.124238][G eval loss: 1.086474]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.112599][G train loss: 1.185701]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.116527][G eval loss: 1.101033]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.104589][G train loss: 1.198639]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.110646][G eval loss: 1.113812]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.098658][G train loss: 1.208658]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.114040][G eval loss: 1.110338]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.099559][G train loss: 1.202644]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.131531][G eval loss: 1.084455]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.162432][G train loss: 1.101870]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.189624][G eval loss: 1.013559]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.241601][G train loss: 1.010875]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.237217][G eval loss: 0.952788]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.294323][G train loss: 0.967127]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.305738][G eval loss: 0.878426]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.321999][G train loss: 0.945715]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.343522][G eval loss: 0.849139]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.354854][G train loss: 0.908270]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.354660][G eval loss: 0.843012]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.373472][G train loss: 0.890495]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.382317][G eval loss: 0.806192]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.396484][G train loss: 0.864715]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.406099][G eval loss: 0.798053]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.401390][G train loss: 0.860587]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.423114][G eval loss: 0.792164]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.002 lr_d=0.0008 -> score=1.215278\n",
      "\n",
      "----- 0056: lr_g=0.002, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250053][G eval loss: 0.990056]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250050][G train loss: 1.092504]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 0.985408]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.087922]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.983849]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249962][G train loss: 1.086455]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.982513]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.085242]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249942][G eval loss: 0.980503]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249916][G train loss: 1.083393]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249845][G eval loss: 0.978537]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249796][G train loss: 1.081594]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249711][G eval loss: 0.976308]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249635][G train loss: 1.079506]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249508][G eval loss: 0.974108]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249386][G train loss: 1.077402]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249192][G eval loss: 0.972320]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249003][G train loss: 1.075673]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248704][G eval loss: 0.971365]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248416][G train loss: 1.074750]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.247979][G eval loss: 0.971499]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.247524][G train loss: 1.074887]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.246908][G eval loss: 0.972845]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246190][G train loss: 1.076215]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.245256][G eval loss: 0.975285]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.244119][G train loss: 1.078621]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.243008][G eval loss: 0.978546]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.241280][G train loss: 1.081831]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.239953][G eval loss: 0.982197]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.237487][G train loss: 1.085425]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.235620][G eval loss: 0.986662]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.232250][G train loss: 1.089825]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.229832][G eval loss: 0.991954]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.225613][G train loss: 1.095047]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.222926][G eval loss: 0.997064]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.217910][G train loss: 1.100090]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.214201][G eval loss: 1.003137]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.208462][G train loss: 1.106102]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.203576][G eval loss: 1.010671]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.197155][G train loss: 1.113579]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.191228][G eval loss: 1.020378]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.183885][G train loss: 1.123227]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.177119][G eval loss: 1.032804]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.168777][G train loss: 1.135586]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.161235][G eval loss: 1.049514]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.151904][G train loss: 1.152220]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.144493][G eval loss: 1.071488]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.134148][G train loss: 1.174134]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.128395][G eval loss: 1.097172]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.116997][G train loss: 1.199776]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.113762][G eval loss: 1.124935]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.101132][G train loss: 1.227508]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.100950][G eval loss: 1.153670]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.086548][G train loss: 1.256231]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.089947][G eval loss: 1.182694]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.073533][G train loss: 1.285262]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.080545][G eval loss: 1.211771]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.062407][G train loss: 1.314325]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.072548][G eval loss: 1.240740]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.053014][G train loss: 1.343270]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.065747][G eval loss: 1.269467]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.045009][G train loss: 1.371976]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.059952][G eval loss: 1.297795]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.038122][G train loss: 1.400307]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.055035][G eval loss: 1.325444]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.032190][G train loss: 1.427988]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.050892][G eval loss: 1.352085]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.027103][G train loss: 1.454663]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.047475][G eval loss: 1.377512]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.022751][G train loss: 1.480112]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.046113][G eval loss: 1.401695]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.019045][G train loss: 1.504294]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.054284][G eval loss: 1.424717]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.015895][G train loss: 1.527300]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.063446][G eval loss: 1.446572]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.013235][G train loss: 1.549137]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.066547][G eval loss: 1.467302]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.011000][G train loss: 1.569851]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.065940][G eval loss: 1.486882]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.009122][G train loss: 1.589422]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.063710][G eval loss: 1.505699]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.007524][G train loss: 1.608236]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.058593][G eval loss: 1.523205]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.006203][G train loss: 1.625737]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.042666][G eval loss: 1.539576]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.005107][G train loss: 1.642105]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.038456][G eval loss: 1.554799]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.004203][G train loss: 1.657331]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.038282][G eval loss: 1.568920]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.003457][G train loss: 1.671457]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.038256][G eval loss: 1.582225]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.002835][G train loss: 1.684771]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.038273][G eval loss: 1.594975]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.002306][G train loss: 1.697529]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.038342][G eval loss: 1.606584]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.001879][G train loss: 1.709147]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.038446][G eval loss: 1.617094]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.001543][G train loss: 1.719665]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.038583][G eval loss: 1.626558]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.001249][G train loss: 1.729134]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.038960][G eval loss: 1.635041]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.001009][G train loss: 1.737616]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.060492][G eval loss: 1.642615]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.000820][G train loss: 1.745191]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.077591][G eval loss: 1.649356]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.000666][G train loss: 1.751932]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.078024][G eval loss: 1.655332]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.000542][G train loss: 1.757913]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.078425][G eval loss: 1.660609]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.000441][G train loss: 1.763190]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.078796][G eval loss: 1.665241]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.000359][G train loss: 1.767806]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.079139][G eval loss: 1.669262]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.000292][G train loss: 1.771800]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.079455][G eval loss: 1.672712]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.000237][G train loss: 1.775215]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.079743][G eval loss: 1.675627]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.000193][G train loss: 1.778077]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.080006][G eval loss: 1.677992]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.000157][G train loss: 1.780369]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.080239][G eval loss: 1.679803]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.000128][G train loss: 1.782094]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.080464][G eval loss: 1.681019]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.000104][G train loss: 1.783214]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.080670][G eval loss: 1.681611]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.000085][G train loss: 1.783681]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.080869][G eval loss: 1.681502]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.000069][G train loss: 1.783430]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.081031][G eval loss: 1.680616]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.000056][G train loss: 1.782367]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.081191][G eval loss: 1.678868]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.000046][G train loss: 1.780400]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.081311][G eval loss: 1.676104]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.000038][G train loss: 1.777394]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.081430][G eval loss: 1.672149]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.009861][G train loss: 1.773132]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.081489][G eval loss: 1.666794]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.000062][G train loss: 1.767393]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.081572][G eval loss: 1.659826]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.000021][G train loss: 1.760010]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.081599][G eval loss: 1.651109]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.000017][G train loss: 1.750917]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.081622][G eval loss: 1.640691]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.000014][G train loss: 1.740198]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.040684][G eval loss: 1.628955]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.000011][G train loss: 1.728184]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.040707][G eval loss: 1.616172]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.000009][G train loss: 1.715234]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.040564][G eval loss: 1.602595]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.000008][G train loss: 1.701586]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.040611][G eval loss: 1.587185]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.030608][G train loss: 1.618892]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.082377][G eval loss: 1.490268]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.079905][G train loss: 1.513074]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.124002][G eval loss: 1.393515]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.119893][G train loss: 1.420758]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.158488][G eval loss: 1.298028]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.149290][G train loss: 1.346935]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.165863][G eval loss: 1.284061]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.189884][G train loss: 1.251862]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.165932][G eval loss: 1.270661]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.209923][G train loss: 1.196799]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.166016][G eval loss: 1.256024]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.239977][G train loss: 1.121412]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.199220][G eval loss: 1.161545]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.269018][G train loss: 1.054674]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.249456][G eval loss: 1.069047]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.279878][G train loss: 1.020737]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.249519][G eval loss: 1.056567]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.300030][G train loss: 0.962972]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.249573][G eval loss: 1.044058]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.316280][G train loss: 0.913927]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.291249][G eval loss: 0.948979]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.320379][G train loss: 0.886647]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.291297][G eval loss: 0.939055]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.359845][G train loss: 0.811081]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.291389][G eval loss: 0.928992]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.388680][G train loss: 0.735171]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.332950][G eval loss: 0.923570]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.414028][G train loss: 0.673775]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.333118][G eval loss: 0.913307]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.429892][G train loss: 0.645599]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.374768][G eval loss: 0.825806]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.439903][G train loss: 0.620046]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.417105][G eval loss: 0.735527]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.449908][G train loss: 0.593711]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.465410][G eval loss: 0.672707]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.449918][G train loss: 0.585641]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.469927][G eval loss: 0.660059]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.449924][G train loss: 0.576000]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.457831][G eval loss: 0.709434]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.456477][G train loss: 0.545719]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.457926][G eval loss: 0.701575]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.459938][G train loss: 0.535687]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.458005][G eval loss: 0.692412]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.460202][G train loss: 0.521078]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.458037][G eval loss: 0.684781]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.469934][G train loss: 0.499492]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.458063][G eval loss: 0.677045]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.462300][G train loss: 0.496651]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.458193][G eval loss: 0.662083]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.460974][G train loss: 0.493681]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.499791][G eval loss: 0.581245]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.469937][G train loss: 0.477770]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.499764][G eval loss: 0.573855]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.479946][G train loss: 0.450213]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.499813][G eval loss: 0.566178]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.479934][G train loss: 0.441657]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.499204][G eval loss: 0.556010]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.459949][G train loss: 0.472281]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.416532][G eval loss: 0.708289]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.459962][G train loss: 0.462154]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.416546][G eval loss: 0.701336]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.445456][G train loss: 0.469961]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.416545][G eval loss: 0.692474]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.439036][G train loss: 0.485297]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.416548][G eval loss: 0.686041]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.448697][G train loss: 0.474336]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.395870][G eval loss: 0.687057]\n",
      "[Epoch 111/200][Batch 1/1][D train loss: 0.426916][G train loss: 0.507767]\n",
      "[Epoch 111/200][Batch 1/1][D eval loss: 0.374915][G eval loss: 0.758329]\n",
      "[Epoch 112/200][Batch 1/1][D train loss: 0.380147][G train loss: 0.595519]\n",
      "[Epoch 112/200][Batch 1/1][D eval loss: 0.374921][G eval loss: 0.754325]\n",
      "[Epoch 113/200][Batch 1/1][D train loss: 0.379975][G train loss: 0.597049]\n",
      "[Epoch 113/200][Batch 1/1][D eval loss: 0.374927][G eval loss: 0.750200]\n",
      "[Epoch 114/200][Batch 1/1][D train loss: 0.380004][G train loss: 0.591319]\n",
      "[Epoch 114/200][Batch 1/1][D eval loss: 0.374933][G eval loss: 0.744892]\n",
      "[Epoch 115/200][Batch 1/1][D train loss: 0.389982][G train loss: 0.569111]\n",
      "[Epoch 115/200][Batch 1/1][D eval loss: 0.416545][G eval loss: 0.655935]\n",
      "[Epoch 116/200][Batch 1/1][D train loss: 0.389980][G train loss: 0.564327]\n",
      "[Epoch 116/200][Batch 1/1][D eval loss: 0.458272][G eval loss: 0.567359]\n",
      "[Epoch 117/200][Batch 1/1][D train loss: 0.389984][G train loss: 0.559224]\n",
      "[Epoch 117/200][Batch 1/1][D eval loss: 0.458273][G eval loss: 0.562103]\n",
      "[Epoch 118/200][Batch 1/1][D train loss: 0.389986][G train loss: 0.554631]\n",
      "[Epoch 118/200][Batch 1/1][D eval loss: 0.458278][G eval loss: 0.558865]\n",
      "[Epoch 119/200][Batch 1/1][D train loss: 0.399983][G train loss: 0.552239]\n",
      "[Epoch 119/200][Batch 1/1][D eval loss: 0.458282][G eval loss: 0.558321]\n",
      "[Epoch 120/200][Batch 1/1][D train loss: 0.429982][G train loss: 0.512393]\n",
      "[Epoch 120/200][Batch 1/1][D eval loss: 0.458286][G eval loss: 0.558654]\n",
      "[Epoch 121/200][Batch 1/1][D train loss: 0.429982][G train loss: 0.513025]\n",
      "[Epoch 121/200][Batch 1/1][D eval loss: 0.458289][G eval loss: 0.558071]\n",
      "[Epoch 122/200][Batch 1/1][D train loss: 0.429983][G train loss: 0.512219]\n",
      "[Epoch 122/200][Batch 1/1][D eval loss: 0.458293][G eval loss: 0.556031]\n",
      "[Epoch 123/200][Batch 1/1][D train loss: 0.439976][G train loss: 0.489455]\n",
      "[Epoch 123/200][Batch 1/1][D eval loss: 0.458296][G eval loss: 0.553427]\n",
      "[Epoch 124/200][Batch 1/1][D train loss: 0.419963][G train loss: 0.525110]\n",
      "[Epoch 124/200][Batch 1/1][D eval loss: 0.458298][G eval loss: 0.551037]\n",
      "[Epoch 125/200][Batch 1/1][D train loss: 0.409957][G train loss: 0.560345]\n",
      "[Epoch 125/200][Batch 1/1][D eval loss: 0.458300][G eval loss: 0.549275]\n",
      "[Epoch 126/200][Batch 1/1][D train loss: 0.389986][G train loss: 0.596885]\n",
      "[Epoch 126/200][Batch 1/1][D eval loss: 0.416498][G eval loss: 0.632828]\n",
      "[Epoch 127/200][Batch 1/1][D train loss: 0.369987][G train loss: 0.655889]\n",
      "[Epoch 127/200][Batch 1/1][D eval loss: 0.374974][G eval loss: 0.718382]\n",
      "[Epoch 128/200][Batch 1/1][D train loss: 0.369937][G train loss: 0.657192]\n",
      "[Epoch 128/200][Batch 1/1][D eval loss: 0.374975][G eval loss: 0.720852]\n",
      "[Epoch 129/200][Batch 1/1][D train loss: 0.359990][G train loss: 0.679400]\n",
      "[Epoch 129/200][Batch 1/1][D eval loss: 0.374976][G eval loss: 0.722491]\n",
      "[Epoch 130/200][Batch 1/1][D train loss: 0.359991][G train loss: 0.681220]\n",
      "[Epoch 130/200][Batch 1/1][D eval loss: 0.374762][G eval loss: 0.722836]\n",
      "[Epoch 131/200][Batch 1/1][D train loss: 0.359991][G train loss: 0.681988]\n",
      "[Epoch 131/200][Batch 1/1][D eval loss: 0.374719][G eval loss: 0.721835]\n",
      "[Epoch 132/200][Batch 1/1][D train loss: 0.359992][G train loss: 0.681234]\n",
      "[Epoch 132/200][Batch 1/1][D eval loss: 0.374686][G eval loss: 0.719534]\n",
      "[Epoch 133/200][Batch 1/1][D train loss: 0.359993][G train loss: 0.679006]\n",
      "[Epoch 133/200][Batch 1/1][D eval loss: 0.416648][G eval loss: 0.632920]\n",
      "[Epoch 134/200][Batch 1/1][D train loss: 0.379993][G train loss: 0.635708]\n",
      "[Epoch 134/200][Batch 1/1][D eval loss: 0.499985][G eval loss: 0.462667]\n",
      "[Epoch 135/200][Batch 1/1][D train loss: 0.399993][G train loss: 0.592016]\n",
      "[Epoch 135/200][Batch 1/1][D eval loss: 0.499987][G eval loss: 0.459387]\n",
      "[Epoch 136/200][Batch 1/1][D train loss: 0.429993][G train loss: 0.528490]\n",
      "[Epoch 136/200][Batch 1/1][D eval loss: 0.541651][G eval loss: 0.456759]\n",
      "[Epoch 137/200][Batch 1/1][D train loss: 0.439994][G train loss: 0.505707]\n",
      "[Epoch 137/200][Batch 1/1][D eval loss: 0.541652][G eval loss: 0.454745]\n",
      "[Epoch 138/200][Batch 1/1][D train loss: 0.449995][G train loss: 0.484341]\n",
      "[Epoch 138/200][Batch 1/1][D eval loss: 0.583319][G eval loss: 0.370081]\n",
      "[Epoch 139/200][Batch 1/1][D train loss: 0.469995][G train loss: 0.444077]\n",
      "[Epoch 139/200][Batch 1/1][D eval loss: 0.583320][G eval loss: 0.369173]\n",
      "[Epoch 140/200][Batch 1/1][D train loss: 0.489996][G train loss: 0.403969]\n",
      "[Epoch 140/200][Batch 1/1][D eval loss: 0.583321][G eval loss: 0.368201]\n",
      "[Epoch 141/200][Batch 1/1][D train loss: 0.509994][G train loss: 0.403606]\n",
      "[Epoch 141/200][Batch 1/1][D eval loss: 0.583322][G eval loss: 0.366710]\n",
      "[Epoch 142/200][Batch 1/1][D train loss: 0.509994][G train loss: 0.402478]\n",
      "[Epoch 142/200][Batch 1/1][D eval loss: 0.583323][G eval loss: 0.364448]\n",
      "[Epoch 143/200][Batch 1/1][D train loss: 0.529994][G train loss: 0.380400]\n",
      "[Epoch 143/200][Batch 1/1][D eval loss: 0.583323][G eval loss: 0.361694]\n",
      "[Epoch 144/200][Batch 1/1][D train loss: 0.519996][G train loss: 0.397551]\n",
      "[Epoch 144/200][Batch 1/1][D eval loss: 0.541658][G eval loss: 0.442137]\n",
      "[Epoch 145/200][Batch 1/1][D train loss: 0.510000][G train loss: 0.414398]\n",
      "[Epoch 145/200][Batch 1/1][D eval loss: 0.541658][G eval loss: 0.439418]\n",
      "[Epoch 146/200][Batch 1/1][D train loss: 0.480005][G train loss: 0.471327]\n",
      "[Epoch 146/200][Batch 1/1][D eval loss: 0.541659][G eval loss: 0.436971]\n",
      "[Epoch 147/200][Batch 1/1][D train loss: 0.489994][G train loss: 0.468486]\n",
      "[Epoch 147/200][Batch 1/1][D eval loss: 0.541659][G eval loss: 0.434781]\n",
      "[Epoch 148/200][Batch 1/1][D train loss: 0.459995][G train loss: 0.525801]\n",
      "[Epoch 148/200][Batch 1/1][D eval loss: 0.541660][G eval loss: 0.432800]\n",
      "[Epoch 149/200][Batch 1/1][D train loss: 0.430028][G train loss: 0.580943]\n",
      "[Epoch 149/200][Batch 1/1][D eval loss: 0.541660][G eval loss: 0.431444]\n",
      "[Epoch 150/200][Batch 1/1][D train loss: 0.420007][G train loss: 0.601325]\n",
      "[Epoch 150/200][Batch 1/1][D eval loss: 0.458328][G eval loss: 0.596826]\n",
      "[Epoch 151/200][Batch 1/1][D train loss: 0.420006][G train loss: 0.599348]\n",
      "[Epoch 151/200][Batch 1/1][D eval loss: 0.458328][G eval loss: 0.595664]\n",
      "[Epoch 152/200][Batch 1/1][D train loss: 0.419996][G train loss: 0.617339]\n",
      "[Epoch 152/200][Batch 1/1][D eval loss: 0.458328][G eval loss: 0.594674]\n",
      "[Epoch 153/200][Batch 1/1][D train loss: 0.389996][G train loss: 0.675381]\n",
      "[Epoch 153/200][Batch 1/1][D eval loss: 0.458329][G eval loss: 0.593868]\n",
      "[Epoch 154/200][Batch 1/1][D train loss: 0.390046][G train loss: 0.673570]\n",
      "[Epoch 154/200][Batch 1/1][D eval loss: 0.458464][G eval loss: 0.593179]\n",
      "[Epoch 155/200][Batch 1/1][D train loss: 0.390095][G train loss: 0.671770]\n",
      "[Epoch 155/200][Batch 1/1][D eval loss: 0.499995][G eval loss: 0.592408]\n",
      "[Epoch 156/200][Batch 1/1][D train loss: 0.409997][G train loss: 0.669795]\n",
      "[Epoch 156/200][Batch 1/1][D eval loss: 0.458329][G eval loss: 0.674681]\n",
      "[Epoch 157/200][Batch 1/1][D train loss: 0.419997][G train loss: 0.647520]\n",
      "[Epoch 157/200][Batch 1/1][D eval loss: 0.458329][G eval loss: 0.673363]\n",
      "[Epoch 158/200][Batch 1/1][D train loss: 0.409997][G train loss: 0.664986]\n",
      "[Epoch 158/200][Batch 1/1][D eval loss: 0.416663][G eval loss: 0.755149]\n",
      "[Epoch 159/200][Batch 1/1][D train loss: 0.409997][G train loss: 0.662275]\n",
      "[Epoch 159/200][Batch 1/1][D eval loss: 0.416663][G eval loss: 0.753453]\n",
      "[Epoch 160/200][Batch 1/1][D train loss: 0.400054][G train loss: 0.679562]\n",
      "[Epoch 160/200][Batch 1/1][D eval loss: 0.416663][G eval loss: 0.751700]\n",
      "[Epoch 161/200][Batch 1/1][D train loss: 0.389997][G train loss: 0.717039]\n",
      "[Epoch 161/200][Batch 1/1][D eval loss: 0.374997][G eval loss: 0.833548]\n",
      "[Epoch 162/200][Batch 1/1][D train loss: 0.349998][G train loss: 0.794906]\n",
      "[Epoch 162/200][Batch 1/1][D eval loss: 0.374997][G eval loss: 0.832644]\n",
      "[Epoch 163/200][Batch 1/1][D train loss: 0.309996][G train loss: 0.873269]\n",
      "[Epoch 163/200][Batch 1/1][D eval loss: 0.291664][G eval loss: 0.998893]\n",
      "[Epoch 164/200][Batch 1/1][D train loss: 0.279998][G train loss: 0.932126]\n",
      "[Epoch 164/200][Batch 1/1][D eval loss: 0.291664][G eval loss: 0.998752]\n",
      "[Epoch 165/200][Batch 1/1][D train loss: 0.239998][G train loss: 1.031126]\n",
      "[Epoch 165/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.081969]\n",
      "[Epoch 166/200][Batch 1/1][D train loss: 0.209998][G train loss: 1.089911]\n",
      "[Epoch 166/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.081740]\n",
      "[Epoch 167/200][Batch 1/1][D train loss: 0.157510][G train loss: 1.188663]\n",
      "[Epoch 167/200][Batch 1/1][D eval loss: 0.208331][G eval loss: 1.164795]\n",
      "[Epoch 168/200][Batch 1/1][D train loss: 0.149998][G train loss: 1.226658]\n",
      "[Epoch 168/200][Batch 1/1][D eval loss: 0.208331][G eval loss: 1.164053]\n",
      "[Epoch 169/200][Batch 1/1][D train loss: 0.149999][G train loss: 1.224440]\n",
      "[Epoch 169/200][Batch 1/1][D eval loss: 0.208331][G eval loss: 1.162974]\n",
      "[Epoch 170/200][Batch 1/1][D train loss: 0.149999][G train loss: 1.221811]\n",
      "[Epoch 170/200][Batch 1/1][D eval loss: 0.208332][G eval loss: 1.161778]\n",
      "[Epoch 171/200][Batch 1/1][D train loss: 0.159999][G train loss: 1.219036]\n",
      "[Epoch 171/200][Batch 1/1][D eval loss: 0.208332][G eval loss: 1.160678]\n",
      "[Epoch 172/200][Batch 1/1][D train loss: 0.169999][G train loss: 1.216495]\n",
      "[Epoch 172/200][Batch 1/1][D eval loss: 0.208332][G eval loss: 1.159962]\n",
      "[Epoch 173/200][Batch 1/1][D train loss: 0.179999][G train loss: 1.214515]\n",
      "[Epoch 173/200][Batch 1/1][D eval loss: 0.208332][G eval loss: 1.159751]\n",
      "[Epoch 174/200][Batch 1/1][D train loss: 0.189999][G train loss: 1.213207]\n",
      "[Epoch 174/200][Batch 1/1][D eval loss: 0.208332][G eval loss: 1.159910]\n",
      "[Epoch 175/200][Batch 1/1][D train loss: 0.189999][G train loss: 1.212380]\n",
      "[Epoch 175/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.160085]\n",
      "[Epoch 176/200][Batch 1/1][D train loss: 0.199999][G train loss: 1.211593]\n",
      "[Epoch 176/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.159853]\n",
      "[Epoch 177/200][Batch 1/1][D train loss: 0.200101][G train loss: 1.210445]\n",
      "[Epoch 177/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.158935]\n",
      "[Epoch 178/200][Batch 1/1][D train loss: 0.219999][G train loss: 1.208708]\n",
      "[Epoch 178/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 1.157296]\n",
      "[Epoch 179/200][Batch 1/1][D train loss: 0.229999][G train loss: 1.206371]\n",
      "[Epoch 179/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 1.155150]\n",
      "[Epoch 180/200][Batch 1/1][D train loss: 0.239999][G train loss: 1.203695]\n",
      "[Epoch 180/200][Batch 1/1][D eval loss: 0.291665][G eval loss: 1.152656]\n",
      "[Epoch 181/200][Batch 1/1][D train loss: 0.239999][G train loss: 1.201116]\n",
      "[Epoch 181/200][Batch 1/1][D eval loss: 0.291665][G eval loss: 1.150281]\n",
      "[Epoch 182/200][Batch 1/1][D train loss: 0.249999][G train loss: 1.198999]\n",
      "[Epoch 182/200][Batch 1/1][D eval loss: 0.291665][G eval loss: 1.148604]\n",
      "[Epoch 183/200][Batch 1/1][D train loss: 0.249999][G train loss: 1.197569]\n",
      "[Epoch 183/200][Batch 1/1][D eval loss: 0.291665][G eval loss: 1.147706]\n",
      "[Epoch 184/200][Batch 1/1][D train loss: 0.269999][G train loss: 1.196851]\n",
      "[Epoch 184/200][Batch 1/1][D eval loss: 0.291970][G eval loss: 1.147220]\n",
      "[Epoch 185/200][Batch 1/1][D train loss: 0.269999][G train loss: 1.196536]\n",
      "[Epoch 185/200][Batch 1/1][D eval loss: 0.333332][G eval loss: 1.146856]\n",
      "[Epoch 186/200][Batch 1/1][D train loss: 0.281044][G train loss: 1.176203]\n",
      "[Epoch 186/200][Batch 1/1][D eval loss: 0.333332][G eval loss: 1.146480]\n",
      "[Epoch 187/200][Batch 1/1][D train loss: 0.319999][G train loss: 1.175572]\n",
      "[Epoch 187/200][Batch 1/1][D eval loss: 0.333332][G eval loss: 1.145998]\n",
      "[Epoch 188/200][Batch 1/1][D train loss: 0.329999][G train loss: 1.154639]\n",
      "[Epoch 188/200][Batch 1/1][D eval loss: 0.333332][G eval loss: 1.145525]\n",
      "[Epoch 189/200][Batch 1/1][D train loss: 0.340044][G train loss: 1.153606]\n",
      "[Epoch 189/200][Batch 1/1][D eval loss: 0.374999][G eval loss: 1.061704]\n",
      "[Epoch 190/200][Batch 1/1][D train loss: 0.360622][G train loss: 1.152596]\n",
      "[Epoch 190/200][Batch 1/1][D eval loss: 0.416665][G eval loss: 1.061234]\n",
      "[Epoch 191/200][Batch 1/1][D train loss: 0.380621][G train loss: 1.131531]\n",
      "[Epoch 191/200][Batch 1/1][D eval loss: 0.416666][G eval loss: 1.060912]\n",
      "[Epoch 192/200][Batch 1/1][D train loss: 0.380187][G train loss: 1.130455]\n",
      "[Epoch 192/200][Batch 1/1][D eval loss: 0.416666][G eval loss: 1.060681]\n",
      "[Epoch 193/200][Batch 1/1][D train loss: 0.380068][G train loss: 1.129317]\n",
      "[Epoch 193/200][Batch 1/1][D eval loss: 0.416666][G eval loss: 1.060402]\n",
      "[Epoch 194/200][Batch 1/1][D train loss: 0.380040][G train loss: 1.128089]\n",
      "[Epoch 194/200][Batch 1/1][D eval loss: 0.374999][G eval loss: 1.143451]\n",
      "[Epoch 195/200][Batch 1/1][D train loss: 0.380006][G train loss: 1.146710]\n",
      "[Epoch 195/200][Batch 1/1][D eval loss: 0.374999][G eval loss: 1.143132]\n",
      "[Epoch 196/200][Batch 1/1][D train loss: 0.380002][G train loss: 1.145239]\n",
      "[Epoch 196/200][Batch 1/1][D eval loss: 0.374999][G eval loss: 1.142869]\n",
      "[Epoch 197/200][Batch 1/1][D train loss: 0.370001][G train loss: 1.163813]\n",
      "[Epoch 197/200][Batch 1/1][D eval loss: 0.374999][G eval loss: 1.142764]\n",
      "[Epoch 198/200][Batch 1/1][D train loss: 0.360000][G train loss: 1.182609]\n",
      "[Epoch 198/200][Batch 1/1][D eval loss: 0.374999][G eval loss: 1.142932]\n",
      "[Epoch 199/200][Batch 1/1][D train loss: 0.359999][G train loss: 1.181726]\n",
      "[Epoch 199/200][Batch 1/1][D eval loss: 0.374999][G eval loss: 1.143471]\n",
      "[Epoch 200/200][Batch 1/1][D train loss: 0.359999][G train loss: 1.181165]\n",
      "[Epoch 200/200][Batch 1/1][D eval loss: 0.374999][G eval loss: 1.144362]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.002 lr_d=0.0015 -> score=1.519361\n",
      "\n",
      "----- 0056: lr_g=0.002, lr_d=0.002 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250109][G eval loss: 0.987100]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250106][G train loss: 1.089548]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 0.984863]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.087376]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.985238]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249952][G train loss: 1.087844]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.983827]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249949][G train loss: 1.086556]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 0.982610]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249850][G train loss: 1.085499]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249723][G eval loss: 0.981131]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249641][G train loss: 1.084188]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249400][G eval loss: 0.979416]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249252][G train loss: 1.082614]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.248837][G eval loss: 0.977982]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248563][G train loss: 1.081274]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.247869][G eval loss: 0.976914]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.247399][G train loss: 1.080267]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.246223][G eval loss: 0.977644]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.245464][G train loss: 1.081027]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.243755][G eval loss: 0.980715]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.242541][G train loss: 1.084101]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.240495][G eval loss: 0.985443]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.238621][G train loss: 1.088805]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.235625][G eval loss: 0.992337]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.232757][G train loss: 1.095659]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.229256][G eval loss: 1.000204]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.225046][G train loss: 1.103467]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.219358][G eval loss: 1.011041]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.213853][G train loss: 1.114239]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.206245][G eval loss: 1.023689]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.199707][G train loss: 1.126819]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.189219][G eval loss: 1.040832]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.181774][G train loss: 1.143889]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.169150][G eval loss: 1.064832]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.160615][G train loss: 1.167811]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.147709][G eval loss: 1.094931]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.138008][G train loss: 1.197834]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.127198][G eval loss: 1.128870]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.115992][G train loss: 1.231704]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.108897][G eval loss: 1.164911]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.095620][G train loss: 1.267679]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.093466][G eval loss: 1.201728]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.076990][G train loss: 1.304447]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.080495][G eval loss: 1.238960]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.061434][G train loss: 1.341648]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.069635][G eval loss: 1.277053]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.048609][G train loss: 1.379727]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.060764][G eval loss: 1.314530]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.038142][G train loss: 1.417203]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.053574][G eval loss: 1.350965]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.029697][G train loss: 1.453646]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.047510][G eval loss: 1.385789]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.022953][G train loss: 1.488482]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.041184][G eval loss: 1.418691]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.017643][G train loss: 1.521390]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.020469][G eval loss: 1.449493]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.013510][G train loss: 1.552195]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.010351][G eval loss: 1.478106]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.010327][G train loss: 1.580812]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.007894][G eval loss: 1.504510]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.007869][G train loss: 1.607220]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.007077][G eval loss: 1.528719]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.005981][G train loss: 1.631435]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.018493][G eval loss: 1.550786]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.004537][G train loss: 1.653511]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.034632][G eval loss: 1.570792]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.003435][G train loss: 1.673523]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.038133][G eval loss: 1.588810]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.002597][G train loss: 1.691548]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.038309][G eval loss: 1.604924]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.001961][G train loss: 1.707670]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.038473][G eval loss: 1.619246]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.001479][G train loss: 1.722000]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.038682][G eval loss: 1.631916]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.001114][G train loss: 1.734675]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.038912][G eval loss: 1.643046]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.000839][G train loss: 1.745805]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.039147][G eval loss: 1.652763]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.000632][G train loss: 1.755521]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.039378][G eval loss: 1.661213]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.000476][G train loss: 1.763967]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.039600][G eval loss: 1.668535]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.000358][G train loss: 1.771283]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.039808][G eval loss: 1.674964]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.000268][G train loss: 1.777707]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.040005][G eval loss: 1.680554]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.000200][G train loss: 1.783286]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.040189][G eval loss: 1.685336]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.000150][G train loss: 1.788052]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.040355][G eval loss: 1.689380]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.000112][G train loss: 1.792075]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.040505][G eval loss: 1.692776]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.000084][G train loss: 1.795444]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.040639][G eval loss: 1.695592]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.000063][G train loss: 1.798228]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.040759][G eval loss: 1.697878]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.000047][G train loss: 1.800483]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.040864][G eval loss: 1.699683]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.000035][G train loss: 1.802254]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.040958][G eval loss: 1.701043]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.000026][G train loss: 1.803570]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.041040][G eval loss: 1.701974]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.000020][G train loss: 1.804440]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.041113][G eval loss: 1.702466]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.000015][G train loss: 1.804853]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.041177][G eval loss: 1.702533]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.000011][G train loss: 1.804820]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.041233][G eval loss: 1.702118]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.000008][G train loss: 1.804292]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.041282][G eval loss: 1.701145]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.009934][G train loss: 1.803195]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.041324][G eval loss: 1.699508]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.009942][G train loss: 1.801412]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.041359][G eval loss: 1.697055]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.009949][G train loss: 1.798788]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.041389][G eval loss: 1.693605]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.009955][G train loss: 1.795151]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.041413][G eval loss: 1.689029]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.009960][G train loss: 1.790354]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.041433][G eval loss: 1.683236]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.009964][G train loss: 1.784301]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.041449][G eval loss: 1.676142]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.009968][G train loss: 1.776945]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.041459][G eval loss: 1.667680]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.009971][G train loss: 1.768205]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.041467][G eval loss: 1.657905]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.009973][G train loss: 1.758116]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.041444][G eval loss: 1.647077]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.009976][G train loss: 1.746945]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.083084][G eval loss: 1.552303]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.030121][G train loss: 1.690531]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.083110][G eval loss: 1.541825]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.079949][G train loss: 1.584539]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.083128][G eval loss: 1.531743]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.139922][G train loss: 1.453722]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.157991][G eval loss: 1.358845]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.209911][G train loss: 1.305772]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.208118][G eval loss: 1.268334]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.269038][G train loss: 1.178329]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.249782][G eval loss: 1.179830]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.289927][G train loss: 1.130779]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.249792][G eval loss: 1.172979]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.319924][G train loss: 1.063039]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.249806][G eval loss: 1.164644]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.329948][G train loss: 1.031722]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.249836][G eval loss: 1.159053]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.360180][G train loss: 0.960994]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.249870][G eval loss: 1.153525]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.380102][G train loss: 0.914865]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.291540][G eval loss: 1.064378]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.390027][G train loss: 0.890030]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.333210][G eval loss: 0.975965]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.399927][G train loss: 0.867922]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.333246][G eval loss: 0.970969]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.419941][G train loss: 0.821806]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.333240][G eval loss: 0.965141]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.439941][G train loss: 0.775362]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.333268][G eval loss: 0.958560]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.459944][G train loss: 0.727913]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.333278][G eval loss: 0.951788]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.459953][G train loss: 0.719834]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.333277][G eval loss: 0.944099]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.459958][G train loss: 0.710930]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.344477][G eval loss: 0.871665]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.469961][G train loss: 0.681067]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.374960][G eval loss: 0.841680]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.479967][G train loss: 0.650100]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.416631][G eval loss: 0.745653]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.479992][G train loss: 0.636137]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.458300][G eval loss: 0.648860]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.499975][G train loss: 0.585406]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.499972][G eval loss: 0.551054]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.499981][G train loss: 0.571345]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.499977][G eval loss: 0.535819]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.509985][G train loss: 0.536560]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.499981][G eval loss: 0.519921]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.509989][G train loss: 0.521481]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.499985][G eval loss: 0.503151]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.509991][G train loss: 0.506116]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.541651][G eval loss: 0.402413]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.509993][G train loss: 0.490565]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.541653][G eval loss: 0.385418]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.509993][G train loss: 0.474882]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.541655][G eval loss: 0.368947]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.509994][G train loss: 0.459096]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.541656][G eval loss: 0.351606]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.509995][G train loss: 0.442700]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.541657][G eval loss: 0.333945]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.519946][G train loss: 0.425497]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.541658][G eval loss: 0.316787]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.519958][G train loss: 0.408743]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.541658][G eval loss: 0.300539]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.519965][G train loss: 0.392745]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.541659][G eval loss: 0.286080]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.519969][G train loss: 0.378169]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.541660][G eval loss: 0.275272]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.519973][G train loss: 0.366420]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.499994][G eval loss: 0.352549]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.519976][G train loss: 0.359084]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.499994][G eval loss: 0.351351]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.519978][G train loss: 0.356237]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.499995][G eval loss: 0.354034]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.519980][G train loss: 0.357263]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.499995][G eval loss: 0.360437]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.509982][G train loss: 0.361617]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.499995][G eval loss: 0.370394]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.509984][G train loss: 0.368339]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.499995][G eval loss: 0.379757]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.509985][G train loss: 0.376133]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.499996][G eval loss: 0.386566]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.509986][G train loss: 0.382770]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.499997][G eval loss: 0.389951]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.509986][G train loss: 0.386721]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.541663][G eval loss: 0.306234]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.509988][G train loss: 0.386497]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.541664][G eval loss: 0.300974]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.509989][G train loss: 0.381571]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.541664][G eval loss: 0.292315]\n",
      "[Epoch 111/200][Batch 1/1][D train loss: 0.509990][G train loss: 0.372796]\n",
      "[Epoch 111/200][Batch 1/1][D eval loss: 0.541664][G eval loss: 0.281757]\n",
      "[Epoch 112/200][Batch 1/1][D train loss: 0.509990][G train loss: 0.361617]\n",
      "[Epoch 112/200][Batch 1/1][D eval loss: 0.541664][G eval loss: 0.270172]\n",
      "[Epoch 113/200][Batch 1/1][D train loss: 0.509990][G train loss: 0.349593]\n",
      "[Epoch 113/200][Batch 1/1][D eval loss: 0.541664][G eval loss: 0.259051]\n",
      "[Epoch 114/200][Batch 1/1][D train loss: 0.509990][G train loss: 0.338670]\n",
      "[Epoch 114/200][Batch 1/1][D eval loss: 0.541664][G eval loss: 0.251091]\n",
      "[Epoch 115/200][Batch 1/1][D train loss: 0.509991][G train loss: 0.331331]\n",
      "[Epoch 115/200][Batch 1/1][D eval loss: 0.541664][G eval loss: 0.247687]\n",
      "[Epoch 116/200][Batch 1/1][D train loss: 0.509991][G train loss: 0.329084]\n",
      "[Epoch 116/200][Batch 1/1][D eval loss: 0.541665][G eval loss: 0.247350]\n",
      "[Epoch 117/200][Batch 1/1][D train loss: 0.509991][G train loss: 0.329648]\n",
      "[Epoch 117/200][Batch 1/1][D eval loss: 0.541665][G eval loss: 0.247471]\n",
      "[Epoch 118/200][Batch 1/1][D train loss: 0.509991][G train loss: 0.330153]\n",
      "[Epoch 118/200][Batch 1/1][D eval loss: 0.541665][G eval loss: 0.246485]\n",
      "[Epoch 119/200][Batch 1/1][D train loss: 0.509990][G train loss: 0.329205]\n",
      "[Epoch 119/200][Batch 1/1][D eval loss: 0.541665][G eval loss: 0.244242]\n",
      "[Epoch 120/200][Batch 1/1][D train loss: 0.509990][G train loss: 0.326559]\n",
      "[Epoch 120/200][Batch 1/1][D eval loss: 0.541665][G eval loss: 0.241063]\n",
      "[Epoch 121/200][Batch 1/1][D train loss: 0.509990][G train loss: 0.322809]\n",
      "[Epoch 121/200][Batch 1/1][D eval loss: 0.541665][G eval loss: 0.237201]\n",
      "[Epoch 122/200][Batch 1/1][D train loss: 0.509989][G train loss: 0.318714]\n",
      "[Epoch 122/200][Batch 1/1][D eval loss: 0.541665][G eval loss: 0.233673]\n",
      "[Epoch 123/200][Batch 1/1][D train loss: 0.509988][G train loss: 0.315028]\n",
      "[Epoch 123/200][Batch 1/1][D eval loss: 0.541665][G eval loss: 0.231586]\n",
      "[Epoch 124/200][Batch 1/1][D train loss: 0.509986][G train loss: 0.312460]\n",
      "[Epoch 124/200][Batch 1/1][D eval loss: 0.541665][G eval loss: 0.231374]\n",
      "[Epoch 125/200][Batch 1/1][D train loss: 0.509984][G train loss: 0.311077]\n",
      "[Epoch 125/200][Batch 1/1][D eval loss: 0.541665][G eval loss: 0.232740]\n",
      "[Epoch 126/200][Batch 1/1][D train loss: 0.500000][G train loss: 0.310925]\n",
      "[Epoch 126/200][Batch 1/1][D eval loss: 0.541665][G eval loss: 0.235063]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.002 lr_d=0.002 -> score=0.776728\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.0003 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.988084]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.090564]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.976230]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249996][G train loss: 1.078957]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.972971]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.076057]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 0.971406]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249987][G train loss: 1.074756]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.970919]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.074355]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.971480]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.074901]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.972493]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.075840]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.973446]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249965][G train loss: 1.076693]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.973954]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.077095]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249961][G eval loss: 0.973772]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249948][G train loss: 1.076817]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 0.972921]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249938][G train loss: 1.075876]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249943][G eval loss: 0.971614]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249926][G train loss: 1.074501]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249933][G eval loss: 0.970317]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249912][G train loss: 1.073158]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249921][G eval loss: 0.969269]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249897][G train loss: 1.072085]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249907][G eval loss: 0.968535]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249880][G train loss: 1.071349]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249890][G eval loss: 0.968022]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249859][G train loss: 1.070842]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249869][G eval loss: 0.967670]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249834][G train loss: 1.070500]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249844][G eval loss: 0.967490]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249804][G train loss: 1.070325]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249815][G eval loss: 0.967478]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249769][G train loss: 1.070314]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249782][G eval loss: 0.967527]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249730][G train loss: 1.070359]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249745][G eval loss: 0.967422]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249687][G train loss: 1.070252]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249706][G eval loss: 0.967008]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249641][G train loss: 1.069830]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249663][G eval loss: 0.966148]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249591][G train loss: 1.068950]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249617][G eval loss: 0.964871]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249540][G train loss: 1.067630]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.249570][G eval loss: 0.963089]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249486][G train loss: 1.065748]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.249515][G eval loss: 0.960537]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249429][G train loss: 1.063036]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.249452][G eval loss: 0.956484]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.249367][G train loss: 1.058689]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.249379][G eval loss: 0.950448]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.249301][G train loss: 1.052237]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.249309][G eval loss: 0.941821]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.249242][G train loss: 1.043097]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.249249][G eval loss: 0.928882]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.249209][G train loss: 1.029551]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.249214][G eval loss: 0.911677]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.249214][G train loss: 1.011547]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.249220][G eval loss: 0.893328]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.249282][G train loss: 0.992003]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.249269][G eval loss: 0.879355]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249408][G train loss: 0.976244]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.249325][G eval loss: 0.873533]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.249536][G train loss: 0.967569]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.249333][G eval loss: 0.868170]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.249585][G train loss: 0.958226]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.249284][G eval loss: 0.854088]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.249565][G train loss: 0.941412]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.249179][G eval loss: 0.831347]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.249475][G train loss: 0.918474]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.249024][G eval loss: 0.808645]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.249334][G train loss: 0.897142]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.248837][G eval loss: 0.789005]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.249162][G train loss: 0.878746]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.248663][G eval loss: 0.766584]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.249046][G train loss: 0.858829]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.248609][G eval loss: 0.737946]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.249098][G train loss: 0.831817]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.248738][G eval loss: 0.707909]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.249262][G train loss: 0.801229]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.248973][G eval loss: 0.679835]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.249423][G train loss: 0.769913]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.249200][G eval loss: 0.653539]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.249554][G train loss: 0.741741]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.249240][G eval loss: 0.624792]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.249565][G train loss: 0.713832]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.249122][G eval loss: 0.594973]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.249465][G train loss: 0.683546]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.248838][G eval loss: 0.561087]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.249249][G train loss: 0.650065]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.248402][G eval loss: 0.527760]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.248888][G train loss: 0.618405]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.247824][G eval loss: 0.508422]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.248387][G train loss: 0.598417]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.247272][G eval loss: 0.506751]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.247834][G train loss: 0.592694]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.246947][G eval loss: 0.516268]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.247454][G train loss: 0.597624]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.246940][G eval loss: 0.525060]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.247398][G train loss: 0.603310]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.247476][G eval loss: 0.522812]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.247718][G train loss: 0.607594]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.248382][G eval loss: 0.515220]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.248297][G train loss: 0.599609]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.249471][G eval loss: 0.501462]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.249151][G train loss: 0.584930]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.250973][G eval loss: 0.490551]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.250174][G train loss: 0.570130]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.252408][G eval loss: 0.488369]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.251058][G train loss: 0.562442]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.253187][G eval loss: 0.491318]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.251625][G train loss: 0.561848]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.253521][G eval loss: 0.494265]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.0003 -> score=0.747786\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.986775]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.089255]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.975964]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.078692]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.973747]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.076833]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.972862]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.076212]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.972447]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.075883]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.972522]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.075942]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.972790]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249939][G train loss: 1.076137]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 0.972991]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076238]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.972881]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.076022]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 0.972301]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249863][G train loss: 1.075346]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249865][G eval loss: 0.971321]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249825][G train loss: 1.074276]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.970161]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249779][G train loss: 1.073047]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249782][G eval loss: 0.969241]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249723][G train loss: 1.072081]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249727][G eval loss: 0.968739]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249655][G train loss: 1.071552]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249658][G eval loss: 0.968655]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249571][G train loss: 1.071466]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249579][G eval loss: 0.968837]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249474][G train loss: 1.071655]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249471][G eval loss: 0.969054]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249347][G train loss: 1.071879]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249344][G eval loss: 0.969378]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249197][G train loss: 1.072205]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249192][G eval loss: 0.969828]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249019][G train loss: 1.072653]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249014][G eval loss: 0.970294]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248810][G train loss: 1.073109]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248805][G eval loss: 0.970551]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248569][G train loss: 1.073356]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248562][G eval loss: 0.970452]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248293][G train loss: 1.073235]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248266][G eval loss: 0.969860]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.247964][G train loss: 1.072597]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.247929][G eval loss: 0.968859]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247594][G train loss: 1.071515]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247562][G eval loss: 0.967220]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247201][G train loss: 1.069715]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247152][G eval loss: 0.964711]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246785][G train loss: 1.066935]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246697][G eval loss: 0.960592]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246347][G train loss: 1.062362]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246193][G eval loss: 0.954414]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245880][G train loss: 1.055551]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245693][G eval loss: 0.945369]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.245474][G train loss: 1.045663]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245287][G eval loss: 0.931847]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.245271][G train loss: 1.030962]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.245104][G eval loss: 0.914033]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.245451][G train loss: 1.011408]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.245203][G eval loss: 0.895361]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.246055][G train loss: 0.990343]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.245440][G eval loss: 0.881844]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.246715][G train loss: 0.974119]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.245645][G eval loss: 0.877419]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.247181][G train loss: 0.966411]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.245656][G eval loss: 0.873742]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.247307][G train loss: 0.958256]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.245266][G eval loss: 0.862030]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.247062][G train loss: 0.942195]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.244450][G eval loss: 0.840141]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.246444][G train loss: 0.918488]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.243213][G eval loss: 0.814473]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.245380][G train loss: 0.894440]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.241843][G eval loss: 0.793898]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.244298][G train loss: 0.874266]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.240947][G eval loss: 0.769380]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.243627][G train loss: 0.853222]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.241040][G eval loss: 0.740603]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.243822][G train loss: 0.827518]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.241701][G eval loss: 0.711399]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.244008][G train loss: 0.798783]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.243544][G eval loss: 0.683486]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.244937][G train loss: 0.768174]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.245486][G eval loss: 0.659814]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.245884][G train loss: 0.739838]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.245892][G eval loss: 0.637654]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.246295][G train loss: 0.714855]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.245523][G eval loss: 0.611536]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.246104][G train loss: 0.687564]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.244737][G eval loss: 0.579775]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.245640][G train loss: 0.656237]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.242981][G eval loss: 0.547579]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.244731][G train loss: 0.622426]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.240408][G eval loss: 0.517886]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.243111][G train loss: 0.590352]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.237019][G eval loss: 0.498071]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.241237][G train loss: 0.570101]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.234386][G eval loss: 0.493093]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.239712][G train loss: 0.562843]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.234311][G eval loss: 0.493576]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.238707][G train loss: 0.563096]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.237833][G eval loss: 0.488874]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.239409][G train loss: 0.564645]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.242105][G eval loss: 0.480953]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.242725][G train loss: 0.558255]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.246890][G eval loss: 0.470073]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.248338][G train loss: 0.540034]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.254335][G eval loss: 0.454821]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.252720][G train loss: 0.525156]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.264269][G eval loss: 0.435458]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.256675][G train loss: 0.509873]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.266481][G eval loss: 0.435348]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.260045][G train loss: 0.504823]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.267556][G eval loss: 0.440662]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.0005 -> score=0.708218\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.984848]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.087328]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.975677]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.078404]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.974773]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.077859]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.974453]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.077803]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.973803]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249953][G train loss: 1.077240]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249947][G eval loss: 0.973300]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249926][G train loss: 1.076720]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249917][G eval loss: 0.972916]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249887][G train loss: 1.076263]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249875][G eval loss: 0.972530]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249835][G train loss: 1.075776]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249818][G eval loss: 0.972123]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249762][G train loss: 1.075264]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249741][G eval loss: 0.971434]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249665][G train loss: 1.074477]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249636][G eval loss: 0.970513]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249535][G train loss: 1.073466]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249500][G eval loss: 0.969577]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249365][G train loss: 1.072461]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249325][G eval loss: 0.968915]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249148][G train loss: 1.071752]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249094][G eval loss: 0.968727]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248867][G train loss: 1.071537]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248790][G eval loss: 0.969056]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248500][G train loss: 1.071860]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248386][G eval loss: 0.969812]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248019][G train loss: 1.072619]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247868][G eval loss: 0.970860]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247404][G train loss: 1.073669]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247288][G eval loss: 0.971835]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246706][G train loss: 1.074647]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246423][G eval loss: 0.972387]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245722][G train loss: 1.075181]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245291][G eval loss: 0.972911]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244482][G train loss: 1.075652]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.243985][G eval loss: 0.973293]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243072][G train loss: 1.075963]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242485][G eval loss: 0.973550]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241470][G train loss: 1.076116]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.240782][G eval loss: 0.973705]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239669][G train loss: 1.076073]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238884][G eval loss: 0.973830]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237706][G train loss: 1.075810]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236863][G eval loss: 0.973796]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235675][G train loss: 1.075098]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234806][G eval loss: 0.972839]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.233712][G train loss: 1.072989]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.232761][G eval loss: 0.969082]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.231886][G train loss: 1.067488]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.231113][G eval loss: 0.961081]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.230819][G train loss: 1.056575]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.230403][G eval loss: 0.947883]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.231742][G train loss: 1.038087]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.230465][G eval loss: 0.929777]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.233492][G train loss: 1.014470]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.231200][G eval loss: 0.909043]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.235480][G train loss: 0.989017]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.235745][G eval loss: 0.885975]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.238279][G train loss: 0.967363]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.240784][G eval loss: 0.875062]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.242252][G train loss: 0.955450]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.242788][G eval loss: 0.877915]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.245024][G train loss: 0.954848]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.243219][G eval loss: 0.881607]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.246681][G train loss: 0.955087]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.241874][G eval loss: 0.881142]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.248794][G train loss: 0.947589]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.233198][G eval loss: 0.883169]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.249183][G train loss: 0.935996]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.229005][G eval loss: 0.870633]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.247419][G train loss: 0.918323]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.225043][G eval loss: 0.853904]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.241932][G train loss: 0.900990]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.220607][G eval loss: 0.837426]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.238162][G train loss: 0.878793]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.215969][G eval loss: 0.821863]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.234581][G train loss: 0.858582]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.212448][G eval loss: 0.808102]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.231260][G train loss: 0.843136]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.212599][G eval loss: 0.787448]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.230720][G train loss: 0.825080]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.220784][G eval loss: 0.755982]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.235822][G train loss: 0.798802]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.231044][G eval loss: 0.723325]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.239106][G train loss: 0.777401]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.243640][G eval loss: 0.697724]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.245269][G train loss: 0.750687]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.247907][G eval loss: 0.690029]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.254809][G train loss: 0.726992]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.257222][G eval loss: 0.678162]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.258913][G train loss: 0.716931]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.258372][G eval loss: 0.674579]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.258473][G train loss: 0.713458]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.258907][G eval loss: 0.668751]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.257740][G train loss: 0.708102]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.259414][G eval loss: 0.657853]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.257837][G train loss: 0.696369]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.259113][G eval loss: 0.642307]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.258631][G train loss: 0.676877]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.259626][G eval loss: 0.617449]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.257860][G train loss: 0.655402]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.267260][G eval loss: 0.570233]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.256481][G train loss: 0.630167]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.270270][G eval loss: 0.539071]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.254977][G train loss: 0.603941]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.271633][G eval loss: 0.513165]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.253623][G train loss: 0.580870]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.273057][G eval loss: 0.496234]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.253284][G train loss: 0.562729]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.274336][G eval loss: 0.488495]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.254345][G train loss: 0.551770]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.275702][G eval loss: 0.489834]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.256656][G train loss: 0.552406]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.278095][G eval loss: 0.487520]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.258848][G train loss: 0.560423]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.283496][G eval loss: 0.481337]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.259869][G train loss: 0.565866]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.288989][G eval loss: 0.480085]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.261594][G train loss: 0.568335]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.289092][G eval loss: 0.481458]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.261544][G train loss: 0.568594]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.288968][G eval loss: 0.477374]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.260974][G train loss: 0.561682]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.288666][G eval loss: 0.470182]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.260039][G train loss: 0.551658]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.288192][G eval loss: 0.461762]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.258756][G train loss: 0.537878]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.287600][G eval loss: 0.450810]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.257200][G train loss: 0.523786]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.286771][G eval loss: 0.442139]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.255546][G train loss: 0.515316]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.285618][G eval loss: 0.438561]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.254105][G train loss: 0.514201]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.284115][G eval loss: 0.442519]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.252982][G train loss: 0.518914]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.281880][G eval loss: 0.451205]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.251658][G train loss: 0.526520]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.279233][G eval loss: 0.459441]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.250601][G train loss: 0.533559]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.276302][G eval loss: 0.464226]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.249966][G train loss: 0.537037]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.273094][G eval loss: 0.467690]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.249482][G train loss: 0.539174]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.270016][G eval loss: 0.473023]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.248930][G train loss: 0.542244]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.267961][G eval loss: 0.481602]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.248952][G train loss: 0.548020]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.266210][G eval loss: 0.496008]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.248894][G train loss: 0.559980]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.265691][G eval loss: 0.511525]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.249539][G train loss: 0.573870]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.265316][G eval loss: 0.521812]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250150][G train loss: 0.583338]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.264377][G eval loss: 0.526412]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250368][G train loss: 0.587082]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.262835][G eval loss: 0.524544]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.250341][G train loss: 0.584237]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.260814][G eval loss: 0.517876]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.249950][G train loss: 0.577385]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.258824][G eval loss: 0.508390]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.249259][G train loss: 0.569745]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.256529][G eval loss: 0.500352]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.248375][G train loss: 0.563678]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.253837][G eval loss: 0.493902]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.247554][G train loss: 0.558582]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.251299][G eval loss: 0.486435]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.246983][G train loss: 0.552496]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.249195][G eval loss: 0.479598]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.246724][G train loss: 0.546013]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.247819][G eval loss: 0.473443]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.246553][G train loss: 0.539878]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.247107][G eval loss: 0.468986]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.246263][G train loss: 0.534760]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.246424][G eval loss: 0.467152]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.245745][G train loss: 0.531318]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.245480][G eval loss: 0.468033]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.244893][G train loss: 0.530594]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.244092][G eval loss: 0.471415]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.0008 -> score=0.715508\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250055][G eval loss: 0.980718]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250052][G train loss: 1.083198]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.974865]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.077593]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.977018]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249964][G train loss: 1.080105]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.977931]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.081281]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249961][G eval loss: 0.976979]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249938][G train loss: 1.080415]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249887][G eval loss: 0.976116]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249846][G train loss: 1.079536]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249788][G eval loss: 0.975345]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249727][G train loss: 1.078691]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249625][G eval loss: 0.975064]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249525][G train loss: 1.078309]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249367][G eval loss: 0.974679]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249212][G train loss: 1.077818]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248970][G eval loss: 0.974108]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248728][G train loss: 1.077149]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248360][G eval loss: 0.973533]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.247995][G train loss: 1.076482]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247454][G eval loss: 0.973343]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246901][G train loss: 1.076216]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246095][G eval loss: 0.973969]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245268][G train loss: 1.076784]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244131][G eval loss: 0.975844]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242913][G train loss: 1.078611]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.241609][G eval loss: 0.978813]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239823][G train loss: 1.081556]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.238252][G eval loss: 0.982869]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235659][G train loss: 1.085608]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.233825][G eval loss: 0.987225]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.230242][G train loss: 1.089953]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.227981][G eval loss: 0.991940]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223303][G train loss: 1.094633]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.220066][G eval loss: 0.998120]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.214515][G train loss: 1.100723]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.210096][G eval loss: 1.005861]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.203861][G train loss: 1.108377]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.198405][G eval loss: 1.014715]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.191605][G train loss: 1.117026]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.185092][G eval loss: 1.025843]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.177487][G train loss: 1.127751]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.170220][G eval loss: 1.039841]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.161901][G train loss: 1.140857]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.154556][G eval loss: 1.056859]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.145716][G train loss: 1.156208]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.140435][G eval loss: 1.073669]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.132027][G train loss: 1.168450]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.141275][G eval loss: 1.065348]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.155200][G train loss: 1.112789]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.170638][G eval loss: 1.009308]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.206895][G train loss: 1.036394]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.222572][G eval loss: 0.935474]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.243935][G train loss: 0.984473]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.241330][G eval loss: 0.916158]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.273120][G train loss: 0.941019]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.263632][G eval loss: 0.867461]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.291009][G train loss: 0.914741]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.294513][G eval loss: 0.840455]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.300276][G train loss: 0.900777]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.304785][G eval loss: 0.850982]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.312701][G train loss: 0.897490]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.327211][G eval loss: 0.861805]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.326757][G train loss: 0.902566]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.353717][G eval loss: 0.873431]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.337629][G train loss: 0.928883]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.365594][G eval loss: 0.902584]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.343719][G train loss: 0.946948]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.368774][G eval loss: 0.918557]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.351772][G train loss: 0.951432]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.371713][G eval loss: 0.912387]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.348448][G train loss: 0.950906]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.373596][G eval loss: 0.891152]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.343583][G train loss: 0.935739]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.345845][G eval loss: 0.898645]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.336776][G train loss: 0.910317]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.331948][G eval loss: 0.883077]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.323961][G train loss: 0.893726]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.323183][G eval loss: 0.858876]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.313912][G train loss: 0.868053]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.316026][G eval loss: 0.837356]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.302855][G train loss: 0.845756]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.308802][G eval loss: 0.824063]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.291983][G train loss: 0.832603]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.299887][G eval loss: 0.820391]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.275630][G train loss: 0.846615]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.288546][G eval loss: 0.832158]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.263336][G train loss: 0.858275]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.279765][G eval loss: 0.853153]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.254988][G train loss: 0.872169]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.274788][G eval loss: 0.867442]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.243939][G train loss: 0.893335]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.271568][G eval loss: 0.880618]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.233735][G train loss: 0.921560]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.269908][G eval loss: 0.890162]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.230351][G train loss: 0.935396]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.269833][G eval loss: 0.896179]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.232648][G train loss: 0.936287]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.273596][G eval loss: 0.895001]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.233805][G train loss: 0.940814]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.275276][G eval loss: 0.903542]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.234069][G train loss: 0.947524]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.277030][G eval loss: 0.912414]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.238085][G train loss: 0.943751]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.277534][G eval loss: 0.926855]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.239912][G train loss: 0.954392]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.278066][G eval loss: 0.940128]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.240609][G train loss: 0.969106]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.278984][G eval loss: 0.952236]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.241300][G train loss: 0.982674]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.280150][G eval loss: 0.961567]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.242107][G train loss: 0.992923]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.281277][G eval loss: 0.964135]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.242952][G train loss: 0.999120]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.282387][G eval loss: 0.963762]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.243928][G train loss: 1.002087]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.283359][G eval loss: 0.962133]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.243440][G train loss: 1.011172]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.284235][G eval loss: 0.958715]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.243632][G train loss: 1.009226]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.284985][G eval loss: 0.953621]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.243657][G train loss: 1.005128]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.285585][G eval loss: 0.946565]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.243082][G train loss: 0.999492]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.286056][G eval loss: 0.939010]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.243379][G train loss: 0.989098]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.286387][G eval loss: 0.929981]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.241843][G train loss: 0.981364]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.286510][G eval loss: 0.919279]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.245142][G train loss: 0.979582]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.286731][G eval loss: 0.910147]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.241067][G train loss: 0.975177]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.286935][G eval loss: 0.900252]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.234108][G train loss: 0.980720]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.287077][G eval loss: 0.894335]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.231369][G train loss: 0.983747]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.287148][G eval loss: 0.884040]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.230069][G train loss: 0.975006]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.287172][G eval loss: 0.872828]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.228732][G train loss: 0.963279]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.286995][G eval loss: 0.853682]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.230015][G train loss: 0.938149]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.287157][G eval loss: 0.840319]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.234948][G train loss: 0.916273]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.287193][G eval loss: 0.826186]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.236829][G train loss: 0.900759]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.287355][G eval loss: 0.812455]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.238509][G train loss: 0.875101]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.287374][G eval loss: 0.796333]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.235940][G train loss: 0.864689]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.287280][G eval loss: 0.777795]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.234432][G train loss: 0.853746]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.287346][G eval loss: 0.755821]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.228140][G train loss: 0.836633]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.287166][G eval loss: 0.736049]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.227633][G train loss: 0.816089]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.287788][G eval loss: 0.711235]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.227225][G train loss: 0.797740]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.287948][G eval loss: 0.692519]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.224579][G train loss: 0.786657]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.288109][G eval loss: 0.674143]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.223818][G train loss: 0.764391]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.288662][G eval loss: 0.652450]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.223524][G train loss: 0.745796]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.289153][G eval loss: 0.626922]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.225763][G train loss: 0.712562]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.289519][G eval loss: 0.613712]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.226480][G train loss: 0.692441]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.302026][G eval loss: 0.543480]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.225587][G train loss: 0.682665]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.301780][G eval loss: 0.535717]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.228367][G train loss: 0.655125]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.302037][G eval loss: 0.528561]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.231727][G train loss: 0.630044]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.304360][G eval loss: 0.519124]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.247580][G train loss: 0.552885]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.305555][G eval loss: 0.507392]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.248328][G train loss: 0.541790]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.306788][G eval loss: 0.494878]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.255426][G train loss: 0.501676]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.306972][G eval loss: 0.484106]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.255977][G train loss: 0.492172]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.306781][G eval loss: 0.477487]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.256285][G train loss: 0.482154]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.307459][G eval loss: 0.471748]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.256467][G train loss: 0.473832]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.306071][G eval loss: 0.466509]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.256684][G train loss: 0.465005]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.274252][G eval loss: 0.463276]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.256602][G train loss: 0.463764]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.273097][G eval loss: 0.461857]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.253121][G train loss: 0.474670]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.285925][G eval loss: 0.390602]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.256156][G train loss: 0.457153]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.285828][G eval loss: 0.389875]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.255828][G train loss: 0.455179]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.283778][G eval loss: 0.392757]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.255379][G train loss: 0.454446]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.285394][G eval loss: 0.389310]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.254953][G train loss: 0.455420]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.287930][G eval loss: 0.386239]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.254410][G train loss: 0.456280]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.288483][G eval loss: 0.389181]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.253872][G train loss: 0.455642]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.290332][G eval loss: 0.390738]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.253343][G train loss: 0.459177]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.287939][G eval loss: 0.390870]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.252840][G train loss: 0.462173]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.278464][G eval loss: 0.388261]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.249582][G train loss: 0.477334]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.267409][G eval loss: 0.392773]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.249291][G train loss: 0.482962]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.270446][G eval loss: 0.398762]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.251315][G train loss: 0.476759]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.273489][G eval loss: 0.406332]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.250988][G train loss: 0.479893]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.275824][G eval loss: 0.410241]\n",
      "[Epoch 111/200][Batch 1/1][D train loss: 0.250769][G train loss: 0.481695]\n",
      "[Epoch 111/200][Batch 1/1][D eval loss: 0.277387][G eval loss: 0.416795]\n",
      "[Epoch 112/200][Batch 1/1][D train loss: 0.250460][G train loss: 0.484483]\n",
      "[Epoch 112/200][Batch 1/1][D eval loss: 0.278266][G eval loss: 0.420876]\n",
      "[Epoch 113/200][Batch 1/1][D train loss: 0.250236][G train loss: 0.487493]\n",
      "[Epoch 113/200][Batch 1/1][D eval loss: 0.278785][G eval loss: 0.420189]\n",
      "[Epoch 114/200][Batch 1/1][D train loss: 0.250087][G train loss: 0.487731]\n",
      "[Epoch 114/200][Batch 1/1][D eval loss: 0.279161][G eval loss: 0.421466]\n",
      "[Epoch 115/200][Batch 1/1][D train loss: 0.250045][G train loss: 0.488509]\n",
      "[Epoch 115/200][Batch 1/1][D eval loss: 0.279159][G eval loss: 0.423257]\n",
      "[Epoch 116/200][Batch 1/1][D train loss: 0.250320][G train loss: 0.491991]\n",
      "[Epoch 116/200][Batch 1/1][D eval loss: 0.278944][G eval loss: 0.425997]\n",
      "[Epoch 117/200][Batch 1/1][D train loss: 0.250128][G train loss: 0.491844]\n",
      "[Epoch 117/200][Batch 1/1][D eval loss: 0.278709][G eval loss: 0.427547]\n",
      "[Epoch 118/200][Batch 1/1][D train loss: 0.249755][G train loss: 0.491487]\n",
      "[Epoch 118/200][Batch 1/1][D eval loss: 0.278486][G eval loss: 0.429000]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.0015 -> score=0.707486\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.002 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250110][G eval loss: 0.977763]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250107][G train loss: 1.080243]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.974332]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.077060]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.978442]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249953][G train loss: 1.081529]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 0.979146]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249952][G train loss: 1.082495]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249924][G eval loss: 0.978303]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249889][G train loss: 1.081740]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249784][G eval loss: 0.977196]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249717][G train loss: 1.080615]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249503][G eval loss: 0.976300]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249375][G train loss: 1.079645]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249036][G eval loss: 0.976558]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248796][G train loss: 1.079801]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.248258][G eval loss: 0.977181]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.247855][G train loss: 1.080317]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.246955][G eval loss: 0.978426]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.246287][G train loss: 1.081461]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.245094][G eval loss: 0.980293]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.243994][G train loss: 1.083228]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.241982][G eval loss: 0.983843]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.240297][G train loss: 1.086688]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.237677][G eval loss: 0.989038]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.235126][G train loss: 1.091822]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.232010][G eval loss: 0.995506]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.228162][G train loss: 1.098231]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.223085][G eval loss: 1.005173]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.217778][G train loss: 1.107860]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.210697][G eval loss: 1.016968]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.204424][G train loss: 1.119614]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.194499][G eval loss: 1.032814]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.187591][G train loss: 1.135341]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.175070][G eval loss: 1.055563]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.167057][G train loss: 1.157919]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.153720][G eval loss: 1.084272]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.144416][G train loss: 1.186359]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.132500][G eval loss: 1.117249]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.121687][G train loss: 1.218982]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.113259][G eval loss: 1.152532]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.100225][G train loss: 1.253751]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.097026][G eval loss: 1.188125]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.081415][G train loss: 1.287823]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.094669][G eval loss: 1.196956]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.072316][G train loss: 1.297617]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.143544][G eval loss: 1.119767]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.196445][G train loss: 1.102978]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.233798][G eval loss: 1.005561]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.246292][G train loss: 1.056542]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.262508][G eval loss: 0.992856]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.279923][G train loss: 1.010989]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.284866][G eval loss: 0.986777]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.308888][G train loss: 0.969085]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.355539][G eval loss: 0.879122]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.332789][G train loss: 0.928873]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.374910][G eval loss: 0.848753]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.342765][G train loss: 0.927430]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.409136][G eval loss: 0.856290]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.360064][G train loss: 0.916695]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.422616][G eval loss: 0.881699]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.369562][G train loss: 0.909706]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.423011][G eval loss: 0.902340]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.002 -> score=1.325351\n",
      "\n",
      "----- 0056: lr_g=0.006, lr_d=0.0003 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.981968]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.084489]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.975723]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.078868]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.971089]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.074567]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 0.970358]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249987][G train loss: 1.073840]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.971784]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.075143]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.973465]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.076666]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.974400]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.077458]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.973953]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249965][G train loss: 1.076887]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.972524]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.075369]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249961][G eval loss: 0.971041]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249949][G train loss: 1.073827]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 0.970089]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249938][G train loss: 1.072845]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249943][G eval loss: 0.969386]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249926][G train loss: 1.072142]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249930][G eval loss: 0.968592]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249910][G train loss: 1.071362]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.967927]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249892][G train loss: 1.070715]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249898][G eval loss: 0.967367]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249872][G train loss: 1.070188]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249879][G eval loss: 0.966599]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249851][G train loss: 1.069470]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 0.965249]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249828][G train loss: 1.068125]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249838][G eval loss: 0.962801]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249804][G train loss: 1.065604]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249817][G eval loss: 0.958183]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249782][G train loss: 1.060869]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249795][G eval loss: 0.948960]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249763][G train loss: 1.051396]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249779][G eval loss: 0.932062]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249755][G train loss: 1.033888]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249782][G eval loss: 0.910593]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249771][G train loss: 1.011331]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249811][G eval loss: 0.897922]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249814][G train loss: 0.997208]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249842][G eval loss: 0.901687]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249857][G train loss: 0.998785]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.898125]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249858][G train loss: 0.993353]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.249768][G eval loss: 0.879998]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249827][G train loss: 0.973150]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.249707][G eval loss: 0.856887]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.249770][G train loss: 0.949194]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.249645][G eval loss: 0.837822]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.249703][G train loss: 0.930565]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.249599][G eval loss: 0.816689]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.249654][G train loss: 0.909899]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.249582][G eval loss: 0.787101]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.249646][G train loss: 0.879902]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.249618][G eval loss: 0.748985]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.249701][G train loss: 0.843071]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.249704][G eval loss: 0.712319]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.249784][G train loss: 0.804505]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.249773][G eval loss: 0.674597]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249826][G train loss: 0.765483]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.249754][G eval loss: 0.630652]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.249805][G train loss: 0.720173]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.249654][G eval loss: 0.575521]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.249687][G train loss: 0.666028]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.249508][G eval loss: 0.527434]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.249461][G train loss: 0.615689]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.249280][G eval loss: 0.517464]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.249107][G train loss: 0.604231]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.249173][G eval loss: 0.548880]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.248865][G train loss: 0.645929]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.249183][G eval loss: 0.596700]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.248790][G train loss: 0.699030]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.249329][G eval loss: 0.634083]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.248847][G train loss: 0.723828]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.249604][G eval loss: 0.627955]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.249027][G train loss: 0.709221]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.250066][G eval loss: 0.593419]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.249315][G train loss: 0.676847]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.250514][G eval loss: 0.575479]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.249654][G train loss: 0.649507]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.250842][G eval loss: 0.573304]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.249942][G train loss: 0.640628]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.251073][G eval loss: 0.571674]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.250112][G train loss: 0.638635]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.251203][G eval loss: 0.562465]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.250156][G train loss: 0.630742]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.251189][G eval loss: 0.546912]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.250086][G train loss: 0.617494]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.250986][G eval loss: 0.531746]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.249882][G train loss: 0.604942]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.250672][G eval loss: 0.521636]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.249568][G train loss: 0.597839]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.250424][G eval loss: 0.518431]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.249356][G train loss: 0.593940]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.250234][G eval loss: 0.515891]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.249157][G train loss: 0.590340]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.250107][G eval loss: 0.510012]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.248960][G train loss: 0.584898]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.250008][G eval loss: 0.500042]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.248811][G train loss: 0.576050]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.249894][G eval loss: 0.486313]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.248647][G train loss: 0.563065]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.249759][G eval loss: 0.471808]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.248454][G train loss: 0.548678]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.249612][G eval loss: 0.460234]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.248236][G train loss: 0.536782]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.249425][G eval loss: 0.455542]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.247959][G train loss: 0.531703]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.249204][G eval loss: 0.455491]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.247620][G train loss: 0.531672]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.248930][G eval loss: 0.456364]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.247223][G train loss: 0.532985]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.248668][G eval loss: 0.454093]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.246807][G train loss: 0.531641]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.248405][G eval loss: 0.449856]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.246406][G train loss: 0.526476]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.248101][G eval loss: 0.443204]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.246027][G train loss: 0.518744]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.247860][G eval loss: 0.433515]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.245687][G train loss: 0.508758]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.247668][G eval loss: 0.424411]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.245412][G train loss: 0.498878]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.247521][G eval loss: 0.418231]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.245170][G train loss: 0.492170]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.247433][G eval loss: 0.415178]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.244967][G train loss: 0.488958]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.247341][G eval loss: 0.413360]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.244771][G train loss: 0.487871]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.247139][G eval loss: 0.411313]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.244489][G train loss: 0.486554]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.246842][G eval loss: 0.409493]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.244093][G train loss: 0.484100]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.246265][G eval loss: 0.408636]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.243610][G train loss: 0.481467]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.245647][G eval loss: 0.407614]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.242982][G train loss: 0.478350]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.245068][G eval loss: 0.406701]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.242230][G train loss: 0.474779]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.244361][G eval loss: 0.407839]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.241439][G train loss: 0.472977]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.243586][G eval loss: 0.409954]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.006 lr_d=0.0003 -> score=0.653540\n",
      "\n",
      "----- 0056: lr_g=0.006, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.980660]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.083180]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.975456]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.078601]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.971864]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.075342]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.971814]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.075296]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.973313]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.076672]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.974509]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.077710]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.974701]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249940][G train loss: 1.077758]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249938][G eval loss: 0.973502]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076436]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.971456]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.074301]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 0.969577]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249865][G train loss: 1.072361]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249866][G eval loss: 0.968497]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249829][G train loss: 1.071252]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.967944]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249782][G train loss: 1.070699]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249777][G eval loss: 0.967536]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249720][G train loss: 1.070304]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249711][G eval loss: 0.967422]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249641][G train loss: 1.070208]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249632][G eval loss: 0.967517]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249548][G train loss: 1.070334]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249541][G eval loss: 0.967339]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249442][G train loss: 1.070200]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249440][G eval loss: 0.966562]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249326][G train loss: 1.069419]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249326][G eval loss: 0.964654]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249200][G train loss: 1.067420]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249204][G eval loss: 0.960490]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249075][G train loss: 1.063098]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249069][G eval loss: 0.951554]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248952][G train loss: 1.053839]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248940][G eval loss: 0.934691]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248867][G train loss: 1.036220]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248901][G eval loss: 0.913240]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248916][G train loss: 1.013468]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249012][G eval loss: 0.900804]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249111][G train loss: 0.999367]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249110][G eval loss: 0.904692]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249231][G train loss: 1.001020]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.248946][G eval loss: 0.900952]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249142][G train loss: 0.995242]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.248488][G eval loss: 0.882598]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.248896][G train loss: 0.974309]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.248012][G eval loss: 0.858508]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.248526][G train loss: 0.948989]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.247573][G eval loss: 0.838108]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.248162][G train loss: 0.929007]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.247278][G eval loss: 0.816444]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.247924][G train loss: 0.907744]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.247154][G eval loss: 0.786735]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.247891][G train loss: 0.877355]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.247397][G eval loss: 0.748519]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.248266][G train loss: 0.840046]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.248070][G eval loss: 0.711532]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.248912][G train loss: 0.800759]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.248689][G eval loss: 0.675359]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249272][G train loss: 0.762817]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.248681][G eval loss: 0.634926]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.249293][G train loss: 0.719836]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.248150][G eval loss: 0.581364]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248780][G train loss: 0.667915]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.247107][G eval loss: 0.530172]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.247631][G train loss: 0.615378]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.245213][G eval loss: 0.511087]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.245343][G train loss: 0.594116]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.243898][G eval loss: 0.535064]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.242972][G train loss: 0.625966]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.244054][G eval loss: 0.578054]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.242212][G train loss: 0.677520]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.245452][G eval loss: 0.617088]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.242844][G train loss: 0.704916]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.248241][G eval loss: 0.614690]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.244810][G train loss: 0.695443]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.252632][G eval loss: 0.582589]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.248001][G train loss: 0.667134]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.256386][G eval loss: 0.564990]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.251460][G train loss: 0.640003]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.258983][G eval loss: 0.566527]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.253893][G train loss: 0.634402]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.260693][G eval loss: 0.564644]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.255172][G train loss: 0.634009]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.261423][G eval loss: 0.554116]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.255643][G train loss: 0.625716]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.261239][G eval loss: 0.538302]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.255424][G train loss: 0.611935]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.259804][G eval loss: 0.523859]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.254530][G train loss: 0.600399]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.257379][G eval loss: 0.516479]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.253272][G train loss: 0.594538]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.256147][G eval loss: 0.513956]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.251923][G train loss: 0.592473]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.255167][G eval loss: 0.514866]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.251343][G train loss: 0.590985]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.254492][G eval loss: 0.514660]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.250978][G train loss: 0.589785]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.254063][G eval loss: 0.509897]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.250745][G train loss: 0.585879]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.253776][G eval loss: 0.501001]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.250685][G train loss: 0.577415]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.253507][G eval loss: 0.489221]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.250692][G train loss: 0.566382]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.253193][G eval loss: 0.481979]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.250647][G train loss: 0.558270]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.252827][G eval loss: 0.481994]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.250517][G train loss: 0.557839]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.252594][G eval loss: 0.485138]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.250374][G train loss: 0.562464]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.252443][G eval loss: 0.487020]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.250150][G train loss: 0.566145]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.252304][G eval loss: 0.484233]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.249931][G train loss: 0.564961]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.252153][G eval loss: 0.478397]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.249737][G train loss: 0.558417]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.252086][G eval loss: 0.470607]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.249556][G train loss: 0.550406]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.252051][G eval loss: 0.463682]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.249408][G train loss: 0.543487]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.252041][G eval loss: 0.455410]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.249252][G train loss: 0.535301]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.251970][G eval loss: 0.445360]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.249016][G train loss: 0.524311]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.251701][G eval loss: 0.437013]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.248667][G train loss: 0.514887]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.251231][G eval loss: 0.435543]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.248220][G train loss: 0.512076]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.250745][G eval loss: 0.435805]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.247681][G train loss: 0.511667]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.250193][G eval loss: 0.433646]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.247054][G train loss: 0.508309]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.249591][G eval loss: 0.428734]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.246418][G train loss: 0.501576]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.248997][G eval loss: 0.424169]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.245869][G train loss: 0.494497]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.248459][G eval loss: 0.422996]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.245302][G train loss: 0.490862]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.247896][G eval loss: 0.423845]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.244702][G train loss: 0.489711]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.247386][G eval loss: 0.423571]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.244060][G train loss: 0.488092]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.246866][G eval loss: 0.421075]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.243385][G train loss: 0.484489]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.246165][G eval loss: 0.420909]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.242692][G train loss: 0.480754]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.245302][G eval loss: 0.422815]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.241801][G train loss: 0.479334]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.244514][G eval loss: 0.424385]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.006 lr_d=0.0005 -> score=0.668898\n",
      "\n",
      "----- 0056: lr_g=0.006, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250011][G eval loss: 0.978732]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250009][G train loss: 1.081252]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.975172]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.078317]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.972897]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.076376]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.973407]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.076889]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 0.974661]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249960][G train loss: 1.078020]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 0.975262]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249934][G train loss: 1.078463]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 0.974779]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249896][G train loss: 1.077835]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 0.972965]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249847][G train loss: 1.075898]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 0.970596]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249782][G train loss: 1.073440]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249764][G eval loss: 0.968600]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249698][G train loss: 1.071384]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249672][G eval loss: 0.967582]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249583][G train loss: 1.070336]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249543][G eval loss: 0.967240]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249424][G train loss: 1.069992]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249357][G eval loss: 0.967147]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249203][G train loss: 1.069911]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249124][G eval loss: 0.967417]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248929][G train loss: 1.070195]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248827][G eval loss: 0.968004]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248580][G train loss: 1.070807]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248445][G eval loss: 0.968540]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248140][G train loss: 1.071373]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247994][G eval loss: 0.968583]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247627][G train loss: 1.071383]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247460][G eval loss: 0.966838]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247051][G train loss: 1.069471]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246819][G eval loss: 0.962497]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246421][G train loss: 1.064770]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.246113][G eval loss: 0.952740]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.245810][G train loss: 1.054285]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.245538][G eval loss: 0.934217]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.245501][G train loss: 1.034234]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.245519][G eval loss: 0.911043]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.245965][G train loss: 1.008754]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.245802][G eval loss: 0.898784]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.246331][G train loss: 0.994564]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.245846][G eval loss: 0.903773]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.246370][G train loss: 0.997249]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.245079][G eval loss: 0.900187]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.245834][G train loss: 0.991457]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.242555][G eval loss: 0.883439]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.244698][G train loss: 0.970124]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.239979][G eval loss: 0.859407]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.242915][G train loss: 0.942050]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.237599][G eval loss: 0.836579]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.241093][G train loss: 0.918964]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.235916][G eval loss: 0.814295]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.239948][G train loss: 0.896656]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.235158][G eval loss: 0.786013]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.239808][G train loss: 0.866688]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.236121][G eval loss: 0.748508]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.242217][G train loss: 0.826583]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.241553][G eval loss: 0.705579]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.246281][G train loss: 0.783325]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.247348][G eval loss: 0.670209]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249011][G train loss: 0.747778]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.250089][G eval loss: 0.638645]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.250470][G train loss: 0.710982]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.251596][G eval loss: 0.592263]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.250208][G train loss: 0.668464]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250620][G eval loss: 0.534656]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.248001][G train loss: 0.617422]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.245574][G eval loss: 0.490004]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.245286][G train loss: 0.569197]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.239058][G eval loss: 0.486831]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.239554][G train loss: 0.561765]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.236822][G eval loss: 0.518432]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.233426][G train loss: 0.605124]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.240787][G eval loss: 0.558382]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.235435][G train loss: 0.645793]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.251247][G eval loss: 0.572291]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.245044][G train loss: 0.658620]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.261141][G eval loss: 0.563652]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.257024][G train loss: 0.643194]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.274421][G eval loss: 0.544237]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.266246][G train loss: 0.622045]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.276644][G eval loss: 0.548919]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.270291][G train loss: 0.617695]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.277119][G eval loss: 0.558407]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.006 lr_d=0.0008 -> score=0.835526\n",
      "\n",
      "----- 0056: lr_g=0.006, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250056][G eval loss: 0.974601]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250054][G train loss: 1.077121]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.974353]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.077498]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.975127]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.078606]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.976872]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.080354]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.977840]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249952][G train loss: 1.081199]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249904][G eval loss: 0.978078]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249866][G train loss: 1.081278]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249799][G eval loss: 0.977194]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249743][G train loss: 1.080250]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249641][G eval loss: 0.975280]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249552][G train loss: 1.078213]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249404][G eval loss: 0.972706]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249266][G train loss: 1.075548]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249033][G eval loss: 0.970615]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248826][G train loss: 1.073394]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248466][G eval loss: 0.969825]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248154][G train loss: 1.072568]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247591][G eval loss: 0.970281]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247108][G train loss: 1.073011]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246328][G eval loss: 0.971658]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245595][G train loss: 1.074380]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244369][G eval loss: 0.974295]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.243262][G train loss: 1.076995]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.241829][G eval loss: 0.977507]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.240204][G train loss: 1.080173]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.238639][G eval loss: 0.980926]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.236297][G train loss: 1.083557]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.234282][G eval loss: 0.984331]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.231122][G train loss: 1.086734]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.228960][G eval loss: 0.986352]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.224937][G train loss: 1.088282]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.222732][G eval loss: 0.984710]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.218750][G train loss: 1.084334]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.218736][G eval loss: 0.970269]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.218928][G train loss: 1.059301]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221193][G eval loss: 0.935009]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.225589][G train loss: 1.015192]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.227863][G eval loss: 0.896719]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.230221][G train loss: 0.979327]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.234159][G eval loss: 0.881483]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.236609][G train loss: 0.959395]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.236188][G eval loss: 0.893177]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.240852][G train loss: 0.963326]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.235454][G eval loss: 0.904314]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.242096][G train loss: 0.968142]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.233517][G eval loss: 0.904141]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.242675][G train loss: 0.962945]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231121][G eval loss: 0.894071]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.240170][G train loss: 0.952715]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.228050][G eval loss: 0.877152]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.235346][G train loss: 0.937121]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.223868][G eval loss: 0.864380]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.230777][G train loss: 0.919841]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.206582][G eval loss: 0.889641]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.223665][G train loss: 0.919182]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.206675][G eval loss: 0.890150]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.006 lr_d=0.0015 -> score=1.096826\n",
      "\n",
      "----- 0056: lr_g=0.006, lr_d=0.002 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250112][G eval loss: 0.971646]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250109][G train loss: 1.074166]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.973825]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.076970]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 0.976565]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.080044]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 0.978083]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249964][G train loss: 1.081565]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249945][G eval loss: 0.979151]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249913][G train loss: 1.082510]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249812][G eval loss: 0.979111]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249749][G train loss: 1.082311]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249529][G eval loss: 0.978044]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249413][G train loss: 1.081100]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249098][G eval loss: 0.976738]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248885][G train loss: 1.079668]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.248373][G eval loss: 0.975217]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.248021][G train loss: 1.078054]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.247162][G eval loss: 0.974891]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.246586][G train loss: 1.077659]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.245450][G eval loss: 0.976392]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.244512][G train loss: 1.079107]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.242503][G eval loss: 0.980490]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.241061][G train loss: 1.083172]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.238312][G eval loss: 0.986347]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.236121][G train loss: 1.089003]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.232975][G eval loss: 0.993176]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.229595][G train loss: 1.095775]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.224741][G eval loss: 1.002877]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.219850][G train loss: 1.105405]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.213248][G eval loss: 1.014095]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.207283][G train loss: 1.116366]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.198106][G eval loss: 1.028154]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.191557][G train loss: 1.129630]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.180468][G eval loss: 1.044431]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.174027][G train loss: 1.143099]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.172144][G eval loss: 1.038643]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.178821][G train loss: 1.106193]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.186997][G eval loss: 0.983968]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.207743][G train loss: 1.034078]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.219653][G eval loss: 0.913056]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.224618][G train loss: 0.985384]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.231964][G eval loss: 0.900726]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.240112][G train loss: 0.955499]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.237398][G eval loss: 0.922918]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.255933][G train loss: 0.956042]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.245637][G eval loss: 0.939674]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.262170][G train loss: 0.974641]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.275017][G eval loss: 0.921181]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.270944][G train loss: 0.976409]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.280237][G eval loss: 0.924111]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.267708][G train loss: 0.973250]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.283105][G eval loss: 0.907108]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.267333][G train loss: 0.951291]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.281149][G eval loss: 0.876691]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.268546][G train loss: 0.912440]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.261274][G eval loss: 0.873570]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.260702][G train loss: 0.895155]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.254190][G eval loss: 0.889888]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.251382][G train loss: 0.901750]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.259257][G eval loss: 0.881056]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.248037][G train loss: 0.907369]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.268244][G eval loss: 0.868722]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.248435][G train loss: 0.900159]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.274913][G eval loss: 0.855965]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.247090][G train loss: 0.892401]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.274190][G eval loss: 0.861148]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.252691][G train loss: 0.874590]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.276530][G eval loss: 0.867381]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.006 lr_d=0.002 -> score=1.143911\n",
      "\n",
      "----- 0056: lr_g=0.008, lr_d=0.0003 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.979550]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.082124]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.974014]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249996][G train loss: 1.077551]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.970131]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.073720]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 0.971132]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249987][G train loss: 1.074486]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.972556]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.075663]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.973257]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.076165]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.972493]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.075276]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.971354]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.074079]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.970332]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.073058]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249959][G eval loss: 0.969314]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249947][G train loss: 1.072069]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 0.968827]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249934][G train loss: 1.071593]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249937][G eval loss: 0.967907]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249921][G train loss: 1.070662]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249926][G eval loss: 0.965860]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249908][G train loss: 1.068511]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249914][G eval loss: 0.962087]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.064497]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249898][G eval loss: 0.955689]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249877][G train loss: 1.057556]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249877][G eval loss: 0.944903]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249856][G train loss: 1.045699]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 0.925071]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249842][G train loss: 1.024745]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249863][G eval loss: 0.888957]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249850][G train loss: 0.987903]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249890][G eval loss: 0.844395]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249886][G train loss: 0.943195]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249926][G eval loss: 0.802790]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249927][G train loss: 0.899526]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249923][G eval loss: 0.745307]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249930][G train loss: 0.842930]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 0.681395]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249890][G train loss: 0.778510]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249827][G eval loss: 0.625317]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249867][G train loss: 0.719906]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249840][G eval loss: 0.580598]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249883][G train loss: 0.669504]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.249892][G eval loss: 0.560265]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249931][G train loss: 0.647833]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.559802]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249987][G train loss: 0.641124]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.250061][G eval loss: 0.552716]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.250036][G train loss: 0.626178]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.250069][G eval loss: 0.523705]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.250036][G train loss: 0.602410]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.250047][G eval loss: 0.501817]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.249962][G train loss: 0.578004]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.250075][G eval loss: 0.502383]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.249900][G train loss: 0.572423]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.250175][G eval loss: 0.490700]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.249984][G train loss: 0.570639]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.250197][G eval loss: 0.484361]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.249987][G train loss: 0.565340]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.250154][G eval loss: 0.474391]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249894][G train loss: 0.549107]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.250078][G eval loss: 0.471242]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.249799][G train loss: 0.538504]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.250084][G eval loss: 0.462433]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.249792][G train loss: 0.528960]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250189][G eval loss: 0.461214]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.249880][G train loss: 0.527969]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.250209][G eval loss: 0.463585]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.249885][G train loss: 0.528787]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.250150][G eval loss: 0.457213]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.249817][G train loss: 0.522166]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.250078][G eval loss: 0.448118]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.249720][G train loss: 0.516037]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.250017][G eval loss: 0.436985]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.249642][G train loss: 0.507541]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.427503]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.249596][G train loss: 0.498667]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.249921][G eval loss: 0.419467]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.249533][G train loss: 0.492291]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.249833][G eval loss: 0.416833]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.249434][G train loss: 0.486846]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.249752][G eval loss: 0.411710]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.249336][G train loss: 0.480598]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.249657][G eval loss: 0.410162]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.249208][G train loss: 0.477350]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.249533][G eval loss: 0.413841]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.249055][G train loss: 0.480199]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.249470][G eval loss: 0.417798]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.248910][G train loss: 0.483772]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.249386][G eval loss: 0.418343]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.248773][G train loss: 0.482798]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.249284][G eval loss: 0.415932]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.248615][G train loss: 0.479306]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.249050][G eval loss: 0.412286]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.248408][G train loss: 0.473269]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.248837][G eval loss: 0.406872]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.248155][G train loss: 0.465282]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.248545][G eval loss: 0.404955]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.247842][G train loss: 0.462908]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.248452][G eval loss: 0.404903]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.247677][G train loss: 0.462906]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.248237][G eval loss: 0.404534]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.247444][G train loss: 0.460931]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.248184][G eval loss: 0.405167]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.247271][G train loss: 0.460413]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.248153][G eval loss: 0.403979]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.247233][G train loss: 0.457858]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.248090][G eval loss: 0.403306]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.247025][G train loss: 0.455366]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.247974][G eval loss: 0.402777]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.246724][G train loss: 0.454075]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.247934][G eval loss: 0.401823]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.246535][G train loss: 0.452569]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.247974][G eval loss: 0.401043]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.246474][G train loss: 0.451180]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.247740][G eval loss: 0.399687]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.246109][G train loss: 0.447748]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.247303][G eval loss: 0.401526]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.245630][G train loss: 0.447128]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.247118][G eval loss: 0.401085]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.245386][G train loss: 0.445886]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.246974][G eval loss: 0.400977]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.245066][G train loss: 0.445596]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.246133][G eval loss: 0.400518]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.244151][G train loss: 0.444930]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.245854][G eval loss: 0.400079]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.243668][G train loss: 0.442943]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.246068][G eval loss: 0.404914]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.243657][G train loss: 0.443063]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.245621][G eval loss: 0.406399]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.242992][G train loss: 0.440943]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.244750][G eval loss: 0.410552]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.242043][G train loss: 0.443356]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.244565][G eval loss: 0.410207]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.241686][G train loss: 0.444241]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.245059][G eval loss: 0.414211]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.241778][G train loss: 0.444081]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.245035][G eval loss: 0.416700]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.241495][G train loss: 0.443885]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.244141][G eval loss: 0.406898]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.240437][G train loss: 0.440710]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.243194][G eval loss: 0.403686]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.239508][G train loss: 0.440971]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.243644][G eval loss: 0.402511]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.239424][G train loss: 0.438033]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.244705][G eval loss: 0.411473]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.240342][G train loss: 0.438872]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.244760][G eval loss: 0.410284]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.240321][G train loss: 0.438880]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.243618][G eval loss: 0.402671]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.238958][G train loss: 0.435578]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.242970][G eval loss: 0.400194]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.238104][G train loss: 0.434548]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.243796][G eval loss: 0.400424]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.238477][G train loss: 0.431836]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.245559][G eval loss: 0.410203]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.008 lr_d=0.0003 -> score=0.655762\n",
      "\n",
      "----- 0056: lr_g=0.008, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250001][G eval loss: 0.978242]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249999][G train loss: 1.080815]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.973742]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.077279]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.970903]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.074491]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 0.972586]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.075940]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.974085]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.077192]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.974303]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.077210]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.972796]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249940][G train loss: 1.075579]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 0.970906]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249921][G train loss: 1.073630]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.969269]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249896][G train loss: 1.071994]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249891][G eval loss: 0.967858]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249862][G train loss: 1.070612]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249856][G eval loss: 0.967243]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249819][G train loss: 1.070008]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249814][G eval loss: 0.966464]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249770][G train loss: 1.069216]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249766][G eval loss: 0.964786]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249714][G train loss: 1.067432]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249708][G eval loss: 0.961554]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249647][G train loss: 1.063952]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249637][G eval loss: 0.955790]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249568][G train loss: 1.057635]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249538][G eval loss: 0.945569]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249467][G train loss: 1.046322]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249444][G eval loss: 0.926222]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249382][G train loss: 1.025813]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249421][G eval loss: 0.890500]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249384][G train loss: 0.989281]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249514][G eval loss: 0.846284]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249514][G train loss: 0.944802]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249656][G eval loss: 0.805388]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249683][G train loss: 0.901611]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249625][G eval loss: 0.748875]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249679][G train loss: 0.845559]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249314][G eval loss: 0.684975]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249463][G train loss: 0.781366]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249111][G eval loss: 0.628540]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249312][G train loss: 0.722855]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249134][G eval loss: 0.583363]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249382][G train loss: 0.671753]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.249371][G eval loss: 0.562592]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249608][G train loss: 0.648759]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.249818][G eval loss: 0.560261]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249935][G train loss: 0.640447]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.250369][G eval loss: 0.552515]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.250257][G train loss: 0.625589]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.250431][G eval loss: 0.524990]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.250311][G train loss: 0.603026]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.250287][G eval loss: 0.501101]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.249905][G train loss: 0.577678]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.250427][G eval loss: 0.500469]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.249433][G train loss: 0.571064]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.251090][G eval loss: 0.488567]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.250002][G train loss: 0.567772]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.251290][G eval loss: 0.481568]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.250103][G train loss: 0.563955]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.251095][G eval loss: 0.472541]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249647][G train loss: 0.548005]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.250682][G eval loss: 0.472434]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.249212][G train loss: 0.538872]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.251001][G eval loss: 0.464535]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.249220][G train loss: 0.530879]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.251761][G eval loss: 0.458609]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.249781][G train loss: 0.527886]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.252128][G eval loss: 0.458435]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.250007][G train loss: 0.528485]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.251665][G eval loss: 0.452363]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.249611][G train loss: 0.522397]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.250927][G eval loss: 0.443194]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.248879][G train loss: 0.514560]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.250004][G eval loss: 0.433688]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.248096][G train loss: 0.506258]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.249330][G eval loss: 0.427885]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.247528][G train loss: 0.497893]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.248743][G eval loss: 0.421215]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.247038][G train loss: 0.490432]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.248001][G eval loss: 0.416180]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.246337][G train loss: 0.484748]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.247396][G eval loss: 0.412554]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.245548][G train loss: 0.480427]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.246881][G eval loss: 0.413488]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.244946][G train loss: 0.481590]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.246666][G eval loss: 0.416539]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.244610][G train loss: 0.483776]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.246538][G eval loss: 0.418109]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.244293][G train loss: 0.482523]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.246093][G eval loss: 0.416160]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.243681][G train loss: 0.477596]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.245003][G eval loss: 0.412513]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.242529][G train loss: 0.471505]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.243474][G eval loss: 0.408217]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.240999][G train loss: 0.466811]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.242486][G eval loss: 0.402197]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.239875][G train loss: 0.462186]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.243649][G eval loss: 0.410561]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.240371][G train loss: 0.464551]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.242472][G eval loss: 0.406278]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.239378][G train loss: 0.461024]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.240647][G eval loss: 0.407413]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.237977][G train loss: 0.462905]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.240595][G eval loss: 0.411880]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.237499][G train loss: 0.465414]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.241950][G eval loss: 0.412686]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.237925][G train loss: 0.465986]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.243379][G eval loss: 0.419840]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.238078][G train loss: 0.471409]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.242914][G eval loss: 0.423382]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.236641][G train loss: 0.472551]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.240245][G eval loss: 0.426927]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.233815][G train loss: 0.474544]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.238358][G eval loss: 0.435724]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.230984][G train loss: 0.481232]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.237214][G eval loss: 0.441256]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.228685][G train loss: 0.486071]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.236740][G eval loss: 0.446734]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.226531][G train loss: 0.492678]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.236284][G eval loss: 0.460325]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.223999][G train loss: 0.504159]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.234510][G eval loss: 0.466917]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.220600][G train loss: 0.511941]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.232523][G eval loss: 0.470074]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.217051][G train loss: 0.518229]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.230990][G eval loss: 0.478686]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.213998][G train loss: 0.525175]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.230274][G eval loss: 0.486542]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.211216][G train loss: 0.529456]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.228496][G eval loss: 0.499685]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.207666][G train loss: 0.538067]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.225139][G eval loss: 0.503751]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.202914][G train loss: 0.542897]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.219488][G eval loss: 0.502760]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.196767][G train loss: 0.545250]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.213377][G eval loss: 0.506953]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.190678][G train loss: 0.549940]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.208909][G eval loss: 0.507662]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.185911][G train loss: 0.547657]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.205218][G eval loss: 0.511249]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.182140][G train loss: 0.544932]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.200702][G eval loss: 0.509805]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.178038][G train loss: 0.541377]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.197821][G eval loss: 0.496741]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.175874][G train loss: 0.530704]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.197530][G eval loss: 0.482415]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.178528][G train loss: 0.514447]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.202212][G eval loss: 0.465437]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.189612][G train loss: 0.489956]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.217977][G eval loss: 0.432381]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.205509][G train loss: 0.462544]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.239354][G eval loss: 0.395802]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.223527][G train loss: 0.434056]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.263017][G eval loss: 0.362694]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.244575][G train loss: 0.405944]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.289282][G eval loss: 0.335222]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.265204][G train loss: 0.385546]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.314824][G eval loss: 0.317620]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.286474][G train loss: 0.369522]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.338243][G eval loss: 0.306938]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.008 lr_d=0.0005 -> score=0.645181\n",
      "\n",
      "----- 0056: lr_g=0.008, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250013][G eval loss: 0.976313]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250010][G train loss: 1.078887]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.973461]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.076997]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.971937]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.075525]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249985][G eval loss: 0.974181]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.077535]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.975433]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249961][G train loss: 1.078539]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 0.975052]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249935][G train loss: 1.077960]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249924][G eval loss: 0.972866]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249899][G train loss: 1.075649]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249886][G eval loss: 0.970361]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249852][G train loss: 1.073084]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249835][G eval loss: 0.968388]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249789][G train loss: 1.071111]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249760][G eval loss: 0.966857]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249697][G train loss: 1.069609]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249655][G eval loss: 0.966309]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249571][G train loss: 1.069071]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249517][G eval loss: 0.965733]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249406][G train loss: 1.068480]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249343][G eval loss: 0.964379]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249203][G train loss: 1.067012]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249130][G eval loss: 0.961531]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248965][G train loss: 1.063896]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248842][G eval loss: 0.956212]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248655][G train loss: 1.057981]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248444][G eval loss: 0.946574]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248241][G train loss: 1.047184]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.248073][G eval loss: 0.927604]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247874][G train loss: 1.026942]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247915][G eval loss: 0.891353]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247799][G train loss: 0.989615]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.248226][G eval loss: 0.846247]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.248251][G train loss: 0.943817]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.248756][G eval loss: 0.805686]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248822][G train loss: 0.900351]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248756][G eval loss: 0.750248]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248770][G train loss: 0.844322]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.247615][G eval loss: 0.685769]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.247921][G train loss: 0.780231]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.246603][G eval loss: 0.630717]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.247142][G train loss: 0.722780]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.246644][G eval loss: 0.583454]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247416][G train loss: 0.669444]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247960][G eval loss: 0.559982]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.248265][G train loss: 0.642194]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.551434]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249935][G train loss: 0.628617]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.252015][G eval loss: 0.539564]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.251412][G train loss: 0.612291]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.251832][G eval loss: 0.512130]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.251659][G train loss: 0.585551]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.251815][G eval loss: 0.485162]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.250500][G train loss: 0.558440]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.252432][G eval loss: 0.476228]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.248852][G train loss: 0.549453]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.254357][G eval loss: 0.464731]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.250407][G train loss: 0.545739]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.254886][G eval loss: 0.460821]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.251225][G train loss: 0.542479]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.254309][G eval loss: 0.451840]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.251065][G train loss: 0.529941]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.253018][G eval loss: 0.450991]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.250430][G train loss: 0.522002]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.253536][G eval loss: 0.444057]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.250762][G train loss: 0.514128]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.254736][G eval loss: 0.439407]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.251524][G train loss: 0.509659]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.254834][G eval loss: 0.444586]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.251976][G train loss: 0.512801]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.254107][G eval loss: 0.444680]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.251759][G train loss: 0.510833]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.253131][G eval loss: 0.438991]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.250966][G train loss: 0.506194]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.252515][G eval loss: 0.432413]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.250330][G train loss: 0.501193]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.251525][G eval loss: 0.426128]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.249590][G train loss: 0.497595]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.250852][G eval loss: 0.425086]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.248747][G train loss: 0.497419]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.249960][G eval loss: 0.426134]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.247543][G train loss: 0.499356]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.249172][G eval loss: 0.428147]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.246571][G train loss: 0.498753]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.248402][G eval loss: 0.433130]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.245717][G train loss: 0.502765]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.247658][G eval loss: 0.439572]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.244970][G train loss: 0.509108]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.246773][G eval loss: 0.444140]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.244108][G train loss: 0.512294]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.245410][G eval loss: 0.445354]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.242780][G train loss: 0.510237]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.243453][G eval loss: 0.442599]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.241107][G train loss: 0.503428]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.241735][G eval loss: 0.437178]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.239527][G train loss: 0.496624]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.241803][G eval loss: 0.427872]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.239138][G train loss: 0.490911]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.242452][G eval loss: 0.421280]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.239196][G train loss: 0.486537]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.243397][G eval loss: 0.415248]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.239475][G train loss: 0.480102]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.246130][G eval loss: 0.405782]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.241138][G train loss: 0.471771]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.251995][G eval loss: 0.394005]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.244966][G train loss: 0.461164]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.254271][G eval loss: 0.390675]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.247651][G train loss: 0.452879]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.252887][G eval loss: 0.377254]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.247402][G train loss: 0.441061]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.251155][G eval loss: 0.374710]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.247437][G train loss: 0.434043]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.251945][G eval loss: 0.372657]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.248682][G train loss: 0.427273]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.251070][G eval loss: 0.376821]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.249582][G train loss: 0.425610]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.248795][G eval loss: 0.382227]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.008 lr_d=0.0008 -> score=0.631023\n",
      "\n",
      "----- 0056: lr_g=0.008, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250058][G eval loss: 0.972181]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250055][G train loss: 1.074755]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.972641]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.076178]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.974157]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.077745]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.977634]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.080988]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.978599]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.081706]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249907][G eval loss: 0.977841]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249873][G train loss: 1.080748]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249812][G eval loss: 0.975263]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249761][G train loss: 1.078045]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249671][G eval loss: 0.972665]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249590][G train loss: 1.075387]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249449][G eval loss: 0.970503]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249327][G train loss: 1.073222]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249076][G eval loss: 0.968880]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248890][G train loss: 1.071625]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248488][G eval loss: 0.968509]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248212][G train loss: 1.071253]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247615][G eval loss: 0.968498]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247201][G train loss: 1.071198]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246418][G eval loss: 0.968240]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245812][G train loss: 1.070755]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244680][G eval loss: 0.967194]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.243818][G train loss: 1.069259]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.242323][G eval loss: 0.963906]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.241104][G train loss: 1.065121]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.239214][G eval loss: 0.955735]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.237548][G train loss: 1.055439]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.236164][G eval loss: 0.936451]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.234219][G train loss: 1.033578]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236484][G eval loss: 0.894997]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.235678][G train loss: 0.986783]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241379][G eval loss: 0.845612]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.241345][G train loss: 0.932359]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.247278][G eval loss: 0.813960]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.247373][G train loss: 0.893895]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.250549][G eval loss: 0.768397]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249136][G train loss: 0.846453]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248642][G eval loss: 0.708922]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.245719][G train loss: 0.786308]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.239642][G eval loss: 0.661193]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.238414][G train loss: 0.743417]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238049][G eval loss: 0.619351]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.236638][G train loss: 0.699597]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.244585][G eval loss: 0.572331]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.241703][G train loss: 0.647466]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.249146][G eval loss: 0.547017]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.251709][G train loss: 0.606180]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.252639][G eval loss: 0.538412]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.261169][G train loss: 0.580037]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.266651][G eval loss: 0.494136]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.262269][G train loss: 0.561532]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.267238][G eval loss: 0.461115]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.262842][G train loss: 0.526374]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.265869][G eval loss: 0.442417]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.260002][G train loss: 0.500193]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.265040][G eval loss: 0.431144]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.254970][G train loss: 0.494665]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.268821][G eval loss: 0.420970]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.256934][G train loss: 0.487703]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.279324][G eval loss: 0.414206]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.259258][G train loss: 0.501861]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.280734][G eval loss: 0.417454]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.257769][G train loss: 0.505564]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.281305][G eval loss: 0.416547]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.256193][G train loss: 0.499719]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.281294][G eval loss: 0.426403]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.254449][G train loss: 0.501942]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.280845][G eval loss: 0.423378]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.252778][G train loss: 0.496810]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.279926][G eval loss: 0.417547]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.251384][G train loss: 0.493902]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.278378][G eval loss: 0.425754]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.250400][G train loss: 0.505019]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.276825][G eval loss: 0.438574]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.250081][G train loss: 0.518279]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.274273][G eval loss: 0.449071]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.249871][G train loss: 0.526954]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.265928][G eval loss: 0.464083]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.249836][G train loss: 0.534506]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.261420][G eval loss: 0.476848]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.250202][G train loss: 0.542235]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.261772][G eval loss: 0.478416]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.251315][G train loss: 0.543616]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.262258][G eval loss: 0.472740]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.251664][G train loss: 0.541441]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.262774][G eval loss: 0.469630]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.252000][G train loss: 0.537785]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.263190][G eval loss: 0.466002]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.252237][G train loss: 0.532165]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.263434][G eval loss: 0.464150]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.252310][G train loss: 0.529300]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.263602][G eval loss: 0.463023]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.252367][G train loss: 0.528293]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.263508][G eval loss: 0.458798]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.252214][G train loss: 0.525225]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.263183][G eval loss: 0.456513]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.251845][G train loss: 0.520572]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.262794][G eval loss: 0.455920]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.251534][G train loss: 0.519052]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.262312][G eval loss: 0.454099]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.251230][G train loss: 0.516242]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.261681][G eval loss: 0.450585]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.250874][G train loss: 0.508961]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.261083][G eval loss: 0.442395]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.250403][G train loss: 0.499150]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.260913][G eval loss: 0.436318]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.250338][G train loss: 0.492561]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.258692][G eval loss: 0.434773]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.250052][G train loss: 0.483921]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.254376][G eval loss: 0.441956]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.249757][G train loss: 0.474233]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.253467][G eval loss: 0.441292]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.249143][G train loss: 0.469080]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.258702][G eval loss: 0.422068]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.249321][G train loss: 0.464050]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.261347][G eval loss: 0.415825]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.249210][G train loss: 0.458256]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.262168][G eval loss: 0.414184]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.249410][G train loss: 0.454530]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.262889][G eval loss: 0.415771]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.249664][G train loss: 0.453187]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.263314][G eval loss: 0.418243]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.008 lr_d=0.0015 -> score=0.681556\n",
      "\n",
      "----- 0056: lr_g=0.008, lr_d=0.002 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250113][G eval loss: 0.969227]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250110][G train loss: 1.071800]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.972119]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.075656]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.975610]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249961][G train loss: 1.079198]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.978854]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.082208]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249946][G eval loss: 0.979904]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249916][G train loss: 1.083010]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249813][G eval loss: 0.978858]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249756][G train loss: 1.081764]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249551][G eval loss: 0.976044]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249445][G train loss: 1.078824]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249152][G eval loss: 0.973985]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248959][G train loss: 1.076702]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.248454][G eval loss: 0.972814]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.248136][G train loss: 1.075521]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.247183][G eval loss: 0.972950]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.246668][G train loss: 1.075673]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.245326][G eval loss: 0.974962]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.244501][G train loss: 1.077651]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.242383][G eval loss: 0.978580]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.241140][G train loss: 1.081168]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.238440][G eval loss: 0.982850]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.236600][G train loss: 1.085139]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.233340][G eval loss: 0.986736]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.230574][G train loss: 1.088424]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.225801][G eval loss: 0.989276]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.221874][G train loss: 1.089579]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.215997][G eval loss: 0.985611]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.211193][G train loss: 1.083864]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.211486][G eval loss: 0.957378]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.212177][G train loss: 1.041012]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.214869][G eval loss: 0.907468]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223766][G train loss: 0.973637]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.227697][G eval loss: 0.857471]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.238061][G train loss: 0.918694]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.255306][G eval loss: 0.838163]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.258849][G train loss: 0.900567]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.267364][G eval loss: 0.836104]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.266465][G train loss: 0.896966]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.257247][G eval loss: 0.845022]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.008 lr_d=0.002 -> score=1.102270\n",
      "\n",
      "----- 0056: lr_g=0.01, lr_d=0.0003 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.981002]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249999][G train loss: 1.083656]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.979221]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249996][G train loss: 1.082762]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.972321]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.075717]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 0.970232]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249987][G train loss: 1.073337]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.970950]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.073830]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.972661]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.075425]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.972346]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.075048]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.971943]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249965][G train loss: 1.074662]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.970470]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249957][G train loss: 1.073252]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249960][G eval loss: 0.967937]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249947][G train loss: 1.070791]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249950][G eval loss: 0.964975]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249935][G train loss: 1.067785]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 0.959699]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249921][G train loss: 1.062158]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249927][G eval loss: 0.950070]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249905][G train loss: 1.051774]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.928253]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249889][G train loss: 1.029276]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249906][G eval loss: 0.889400]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249872][G train loss: 0.990263]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249911][G eval loss: 0.830921]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249865][G train loss: 0.932000]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249944][G eval loss: 0.774276]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249875][G train loss: 0.870732]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 0.712916]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249881][G train loss: 0.803276]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 0.630254]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249868][G train loss: 0.721328]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.580086]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249852][G train loss: 0.670712]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.566241]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249854][G train loss: 0.652899]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.250032][G eval loss: 0.531281]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249897][G train loss: 0.618191]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.250094][G eval loss: 0.486448]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249964][G train loss: 0.567158]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.250166][G eval loss: 0.467182]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.250028][G train loss: 0.546180]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.250155][G eval loss: 0.466960]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.545657]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.250136][G eval loss: 0.468088]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249949][G train loss: 0.545398]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.250141][G eval loss: 0.459027]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.249935][G train loss: 0.532228]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.250174][G eval loss: 0.454873]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.249963][G train loss: 0.524195]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.250214][G eval loss: 0.454956]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.249996][G train loss: 0.524622]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.250231][G eval loss: 0.447872]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.249978][G train loss: 0.517163]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.250210][G eval loss: 0.433356]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.249917][G train loss: 0.500188]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.250193][G eval loss: 0.425405]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.249843][G train loss: 0.489152]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.250232][G eval loss: 0.423378]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249824][G train loss: 0.484060]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.250306][G eval loss: 0.419943]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.249870][G train loss: 0.477265]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.250326][G eval loss: 0.414543]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.249873][G train loss: 0.472289]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250237][G eval loss: 0.407631]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.249740][G train loss: 0.467998]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.250219][G eval loss: 0.403069]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.249693][G train loss: 0.463409]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.250285][G eval loss: 0.398979]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.249721][G train loss: 0.455704]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.250285][G eval loss: 0.400786]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.249659][G train loss: 0.455267]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.250136][G eval loss: 0.391447]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.249450][G train loss: 0.447871]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.387575]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.249262][G train loss: 0.444509]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.249944][G eval loss: 0.391253]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.249200][G train loss: 0.442549]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.249861][G eval loss: 0.393077]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.01 lr_d=0.0003 -> score=0.642938\n",
      "\n",
      "----- 0056: lr_g=0.01, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250001][G eval loss: 0.979693]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250000][G train loss: 1.082347]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.978941]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.082482]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.973085]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.076481]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.971680]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.074785]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.972475]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.075354]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 0.973702]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.076465]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249954][G eval loss: 0.972645]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249939][G train loss: 1.075347]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 0.971496]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249918][G train loss: 1.074215]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 0.969417]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249893][G train loss: 1.072197]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249892][G eval loss: 0.966497]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249860][G train loss: 1.069349]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249858][G eval loss: 0.963425]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249817][G train loss: 1.066233]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249815][G eval loss: 0.958305]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249765][G train loss: 1.060765]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249766][G eval loss: 0.949050]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249702][G train loss: 1.050757]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249710][G eval loss: 0.927783]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249626][G train loss: 1.028808]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249662][G eval loss: 0.889487]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249547][G train loss: 0.990367]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249675][G eval loss: 0.831346]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249504][G train loss: 0.932503]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249803][G eval loss: 0.774430]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249533][G train loss: 0.871095]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249914][G eval loss: 0.713294]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249547][G train loss: 0.803782]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249910][G eval loss: 0.630171]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249477][G train loss: 0.721629]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249897][G eval loss: 0.579284]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249388][G train loss: 0.670266]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 0.564866]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249387][G train loss: 0.651926]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.250186][G eval loss: 0.529708]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249595][G train loss: 0.617250]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.250498][G eval loss: 0.485347]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249926][G train loss: 0.566452]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.250854][G eval loss: 0.466562]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.250211][G train loss: 0.546071]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.250774][G eval loss: 0.467106]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.250020][G train loss: 0.545269]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.250643][G eval loss: 0.468468]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.01 lr_d=0.0005 -> score=0.719111\n",
      "\n",
      "----- 0056: lr_g=0.01, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250014][G eval loss: 0.977763]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250011][G train loss: 1.080417]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.978657]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.082198]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.974118]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.077513]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 0.973274]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.076379]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 0.973823]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.076703]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 0.974467]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249930][G train loss: 1.077229]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.972750]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249892][G train loss: 1.075452]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249882][G eval loss: 0.971005]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249844][G train loss: 1.073723]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249831][G eval loss: 0.968569]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249778][G train loss: 1.071348]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249756][G eval loss: 0.965495]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249684][G train loss: 1.068345]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249649][G eval loss: 0.962453]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249551][G train loss: 1.065258]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249504][G eval loss: 0.957550]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249374][G train loss: 1.060008]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249324][G eval loss: 0.948621]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249149][G train loss: 1.050332]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249113][G eval loss: 0.927712]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248873][G train loss: 1.028769]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248929][G eval loss: 0.889839]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248579][G train loss: 0.990823]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248937][G eval loss: 0.832177]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248384][G train loss: 0.933594]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249418][G eval loss: 0.775750]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.248460][G train loss: 0.873160]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249835][G eval loss: 0.715680]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.248537][G train loss: 0.807188]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249804][G eval loss: 0.632938]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.248244][G train loss: 0.725297]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249811][G eval loss: 0.580808]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.247861][G train loss: 0.673317]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.250238][G eval loss: 0.564173]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.247889][G train loss: 0.653348]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.251235][G eval loss: 0.527443]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248924][G train loss: 0.616066]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.252614][G eval loss: 0.481598]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.250410][G train loss: 0.562273]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.254149][G eval loss: 0.460447]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.251404][G train loss: 0.540333]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.253625][G eval loss: 0.459374]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.250509][G train loss: 0.538763]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.253096][G eval loss: 0.458125]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249299][G train loss: 0.538821]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.253455][G eval loss: 0.447731]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.249198][G train loss: 0.524112]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.254773][G eval loss: 0.442108]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.250191][G train loss: 0.513888]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.255941][G eval loss: 0.436328]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.251168][G train loss: 0.510531]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.255893][G eval loss: 0.428631]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.251304][G train loss: 0.503719]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.254626][G eval loss: 0.415249]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.249964][G train loss: 0.489974]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.253005][G eval loss: 0.407111]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.248329][G train loss: 0.477950]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.251495][G eval loss: 0.406182]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.247295][G train loss: 0.472347]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.251152][G eval loss: 0.401995]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.247813][G train loss: 0.468334]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.251627][G eval loss: 0.405741]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.247976][G train loss: 0.470884]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.249648][G eval loss: 0.403467]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.245038][G train loss: 0.468413]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.248770][G eval loss: 0.400534]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.243477][G train loss: 0.465038]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.249124][G eval loss: 0.396344]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.244780][G train loss: 0.455204]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.249437][G eval loss: 0.397259]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.245344][G train loss: 0.453293]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.247761][G eval loss: 0.393915]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.241429][G train loss: 0.451726]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.247610][G eval loss: 0.393839]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.240532][G train loss: 0.450519]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.248164][G eval loss: 0.401281]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.243379][G train loss: 0.453917]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.248379][G eval loss: 0.400466]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.242229][G train loss: 0.452687]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.246383][G eval loss: 0.409975]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.237021][G train loss: 0.460540]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.247914][G eval loss: 0.406063]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.237268][G train loss: 0.458423]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.253349][G eval loss: 0.398711]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.244078][G train loss: 0.453371]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.256333][G eval loss: 0.403540]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.249511][G train loss: 0.450684]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.255816][G eval loss: 0.394767]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.246702][G train loss: 0.443218]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.257321][G eval loss: 0.393180]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.250733][G train loss: 0.435261]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.261063][G eval loss: 0.387364]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.253688][G train loss: 0.432249]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.260660][G eval loss: 0.388688]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.254592][G train loss: 0.426591]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.261902][G eval loss: 0.388349]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.01 lr_d=0.0008 -> score=0.650251\n",
      "\n",
      "----- 0056: lr_g=0.01, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250059][G eval loss: 0.973631]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250056][G train loss: 1.076285]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.977832]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.081373]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.976319]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.079715]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.976712]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.079816]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.976965]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249953][G train loss: 1.079844]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249908][G eval loss: 0.976990]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249873][G train loss: 1.079752]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249817][G eval loss: 0.974678]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249763][G train loss: 1.077379]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249690][G eval loss: 0.972386]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249608][G train loss: 1.075102]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249486][G eval loss: 0.969535]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249361][G train loss: 1.072307]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249142][G eval loss: 0.966275]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248951][G train loss: 1.069111]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248549][G eval loss: 0.963162]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248188][G train loss: 1.065947]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247480][G eval loss: 0.958141]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246935][G train loss: 1.060579]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246103][G eval loss: 0.949068]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245352][G train loss: 1.050780]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244361][G eval loss: 0.928493]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.243271][G train loss: 1.029640]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.242370][G eval loss: 0.891432]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.240589][G train loss: 0.992931]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.241820][G eval loss: 0.833465]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.238363][G train loss: 0.937186]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245476][G eval loss: 0.772317]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238998][G train loss: 0.875391]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.250233][G eval loss: 0.715538]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.240649][G train loss: 0.814247]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.252969][G eval loss: 0.633843]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.240559][G train loss: 0.733971]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.258760][G eval loss: 0.565266]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.240927][G train loss: 0.672514]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.267549][G eval loss: 0.530716]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.246721][G train loss: 0.635192]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.281467][G eval loss: 0.485507]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.264849][G train loss: 0.577012]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.291868][G eval loss: 0.438695]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.275051][G train loss: 0.527462]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.290366][G eval loss: 0.420122]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.275778][G train loss: 0.501922]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.282751][G eval loss: 0.417439]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.270365][G train loss: 0.498381]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.276229][G eval loss: 0.410942]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.266941][G train loss: 0.487473]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.275617][G eval loss: 0.395327]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.267594][G train loss: 0.470237]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.273222][G eval loss: 0.405727]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.268564][G train loss: 0.473584]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.269495][G eval loss: 0.419797]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.265257][G train loss: 0.488443]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.266270][G eval loss: 0.424814]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.261814][G train loss: 0.493258]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.262746][G eval loss: 0.415349]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.258196][G train loss: 0.484000]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.253267][G eval loss: 0.420857]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.251633][G train loss: 0.485773]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.249774][G eval loss: 0.427372]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.248747][G train loss: 0.488799]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.251677][G eval loss: 0.420708]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.250682][G train loss: 0.478331]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.256757][G eval loss: 0.410918]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.251874][G train loss: 0.475684]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.255883][G eval loss: 0.410589]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.250222][G train loss: 0.478078]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.254701][G eval loss: 0.415773]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.249749][G train loss: 0.483955]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.253488][G eval loss: 0.417133]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.249721][G train loss: 0.481667]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.251387][G eval loss: 0.422489]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.249536][G train loss: 0.484525]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.249355][G eval loss: 0.427366]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.249056][G train loss: 0.485275]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.246000][G eval loss: 0.430714]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.247280][G train loss: 0.488561]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.245520][G eval loss: 0.436333]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.247385][G train loss: 0.488969]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.247976][G eval loss: 0.441879]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.248344][G train loss: 0.494458]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.248175][G eval loss: 0.447451]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.248559][G train loss: 0.496844]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.246099][G eval loss: 0.451172]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.246970][G train loss: 0.496395]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.243892][G eval loss: 0.457122]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.245712][G train loss: 0.500794]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.245387][G eval loss: 0.460126]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.246092][G train loss: 0.507588]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.246946][G eval loss: 0.461815]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.248112][G train loss: 0.507191]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.247024][G eval loss: 0.466623]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.247190][G train loss: 0.508753]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.248400][G eval loss: 0.468083]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.247251][G train loss: 0.511017]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.249099][G eval loss: 0.462911]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.248172][G train loss: 0.508156]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.248083][G eval loss: 0.457571]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.248349][G train loss: 0.501274]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.245414][G eval loss: 0.459883]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.246249][G train loss: 0.500692]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.244473][G eval loss: 0.456506]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.245620][G train loss: 0.500793]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.245958][G eval loss: 0.447865]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.248745][G train loss: 0.497236]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.244956][G eval loss: 0.446943]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.248795][G train loss: 0.494993]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.240223][G eval loss: 0.462227]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.246509][G train loss: 0.494533]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.240952][G eval loss: 0.462527]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.01 lr_d=0.0015 -> score=0.703478\n",
      "\n",
      "----- 0056: lr_g=0.01, lr_d=0.002 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250114][G eval loss: 0.970677]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250111][G train loss: 1.073331]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.977310]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.080851]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.977794]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249965][G train loss: 1.081190]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.978172]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.081277]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.978695]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249907][G train loss: 1.081574]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249797][G eval loss: 0.978816]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249730][G train loss: 1.081577]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249509][G eval loss: 0.976627]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249383][G train loss: 1.079326]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249069][G eval loss: 0.975507]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248839][G train loss: 1.078215]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.248328][G eval loss: 0.974199]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.247933][G train loss: 1.076956]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.247078][G eval loss: 0.972991]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.246433][G train loss: 1.075802]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.245340][G eval loss: 0.972099]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.244312][G train loss: 1.074858]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.242450][G eval loss: 0.970361]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.240889][G train loss: 1.072784]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.238271][G eval loss: 0.965892]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.235923][G train loss: 1.067634]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.232892][G eval loss: 0.949772]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.229404][G train loss: 1.050973]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.225712][G eval loss: 0.917786]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.220433][G train loss: 1.019964]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.223197][G eval loss: 0.856107]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.211695][G train loss: 0.967870]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.236525][G eval loss: 0.770031]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.218261][G train loss: 0.886642]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.260735][G eval loss: 0.717853]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.241112][G train loss: 0.817241]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.283072][G eval loss: 0.650497]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.266965][G train loss: 0.735596]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.292591][G eval loss: 0.561064]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.276302][G train loss: 0.646637]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.296273][G eval loss: 0.508030]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.279805][G train loss: 0.601925]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.295652][G eval loss: 0.486796]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.280421][G train loss: 0.583578]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.293431][G eval loss: 0.461374]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.278913][G train loss: 0.556365]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.290915][G eval loss: 0.429537]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.277757][G train loss: 0.516139]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.287288][G eval loss: 0.404865]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.275578][G train loss: 0.482452]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.284534][G eval loss: 0.392289]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.271692][G train loss: 0.466752]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.280292][G eval loss: 0.395434]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.265294][G train loss: 0.467742]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.277728][G eval loss: 0.404347]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.01 lr_d=0.002 -> score=0.682075\n",
      "\n",
      ">>> Best config for 0056: lr_g=0.008, lr_d=0.0008, score=0.631023\n",
      "\n",
      "========== Grid search for 2330 ==========\n",
      "\n",
      "----- 2330: lr_g=0.002, lr_d=0.0003 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.503008]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.467298]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.492390]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 0.456738]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.485451]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249992][G train loss: 0.449844]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.481578]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249988][G train loss: 0.446019]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.480033]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249983][G train loss: 0.444533]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 0.479608]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249978][G train loss: 0.444177]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.479287]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249973][G train loss: 0.443908]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.478798]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249967][G train loss: 0.443442]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.478195]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249960][G train loss: 0.442829]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249963][G eval loss: 0.477705]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249951][G train loss: 0.442311]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.477423]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249941][G train loss: 0.441990]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249947][G eval loss: 0.477341]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249931][G train loss: 0.441860]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249938][G eval loss: 0.477394]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249918][G train loss: 0.441866]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249927][G eval loss: 0.477506]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.002 lr_d=0.0003 -> score=0.727433\n",
      "\n",
      "----- 2330: lr_g=0.002, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.501696]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249996][G train loss: 0.465986]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.492105]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 0.456453]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.486201]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 0.450595]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.483016]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 0.447457]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.481555]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249966][G train loss: 0.446055]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249965][G eval loss: 0.480643]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249953][G train loss: 0.445212]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249951][G eval loss: 0.479569]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249935][G train loss: 0.444190]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249934][G eval loss: 0.478351]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249913][G train loss: 0.442995]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.477149]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249887][G train loss: 0.441784]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249893][G eval loss: 0.476275]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249858][G train loss: 0.440881]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249866][G eval loss: 0.475879]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249822][G train loss: 0.440445]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 0.475928]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249777][G train loss: 0.440447]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249784][G eval loss: 0.476295]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.002 lr_d=0.0005 -> score=0.726079\n",
      "\n",
      "----- 2330: lr_g=0.002, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250007][G eval loss: 0.499767]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.464057]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.491811]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 0.456159]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.487206]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.451599]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.484589]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249965][G train loss: 0.449030]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249959][G eval loss: 0.482913]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249944][G train loss: 0.447413]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249931][G eval loss: 0.481459]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249909][G train loss: 0.446028]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249893][G eval loss: 0.479781]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249859][G train loss: 0.444402]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249846][G eval loss: 0.478065]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249798][G train loss: 0.442709]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249789][G eval loss: 0.476558]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249721][G train loss: 0.441193]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249709][G eval loss: 0.475519]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249615][G train loss: 0.440126]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249598][G eval loss: 0.475094]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249465][G train loss: 0.439662]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249438][G eval loss: 0.475273]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249248][G train loss: 0.439794]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249224][G eval loss: 0.475885]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248970][G train loss: 0.440361]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248961][G eval loss: 0.476855]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248631][G train loss: 0.441290]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248640][G eval loss: 0.478112]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248216][G train loss: 0.442513]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248243][G eval loss: 0.479581]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247700][G train loss: 0.443954]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247761][G eval loss: 0.481236]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247068][G train loss: 0.445588]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247189][G eval loss: 0.482791]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246309][G train loss: 0.447127]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246464][G eval loss: 0.483521]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245360][G train loss: 0.447849]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245497][G eval loss: 0.484078]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244127][G train loss: 0.448404]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244360][G eval loss: 0.484469]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.242667][G train loss: 0.448802]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243010][G eval loss: 0.485057]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.240929][G train loss: 0.449401]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241349][G eval loss: 0.485931]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.238811][G train loss: 0.450298]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239354][G eval loss: 0.487219]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.002 lr_d=0.0008 -> score=0.726573\n",
      "\n",
      "----- 2330: lr_g=0.002, lr_d=0.0015 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250051][G eval loss: 0.495628]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250049][G train loss: 0.459918]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.491010]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249984][G train loss: 0.455358]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.489474]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249961][G train loss: 0.453867]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 0.488127]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249972][G train loss: 0.452568]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249952][G eval loss: 0.486182]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249926][G train loss: 0.450682]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249873][G eval loss: 0.484378]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249822][G train loss: 0.448947]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249770][G eval loss: 0.482529]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249686][G train loss: 0.447151]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249612][G eval loss: 0.480583]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249475][G train loss: 0.445230]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249356][G eval loss: 0.478902]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249133][G train loss: 0.443540]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248968][G eval loss: 0.477867]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248609][G train loss: 0.442479]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248383][G eval loss: 0.477746]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.247809][G train loss: 0.442319]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247459][G eval loss: 0.478456]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246558][G train loss: 0.442984]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246078][G eval loss: 0.480068]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.002 lr_d=0.0015 -> score=0.726145\n",
      "\n",
      "----- 2330: lr_g=0.002, lr_d=0.002 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.002, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250106][G eval loss: 0.492668]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250104][G train loss: 0.456959]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.490344]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249971][G train loss: 0.454692]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 0.490774]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249919][G train loss: 0.455167]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249945][G eval loss: 0.489595]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249914][G train loss: 0.454036]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249838][G eval loss: 0.487832]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249769][G train loss: 0.452333]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249632][G eval loss: 0.486847]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249490][G train loss: 0.451417]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249273][G eval loss: 0.485413]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249005][G train loss: 0.450038]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.248598][G eval loss: 0.483938]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248104][G train loss: 0.448589]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.247653][G eval loss: 0.483107]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.246801][G train loss: 0.447750]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.245913][G eval loss: 0.483793]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.244437][G train loss: 0.448413]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.243225][G eval loss: 0.485961]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.240705][G train loss: 0.450545]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.239346][G eval loss: 0.489980]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.235144][G train loss: 0.454523]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.234023][G eval loss: 0.495878]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.227514][G train loss: 0.460386]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.226904][G eval loss: 0.503788]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.217645][G train loss: 0.468266]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.217279][G eval loss: 0.511840]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.205523][G train loss: 0.476293]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.204442][G eval loss: 0.521583]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.190828][G train loss: 0.486018]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.188389][G eval loss: 0.534065]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.173988][G train loss: 0.498489]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.167832][G eval loss: 0.552694]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.154646][G train loss: 0.517119]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.145823][G eval loss: 0.577233]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.133530][G train loss: 0.541662]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.124298][G eval loss: 0.606157]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.112337][G train loss: 0.570593]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.105084][G eval loss: 0.639332]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.092984][G train loss: 0.603773]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.086875][G eval loss: 0.675436]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.077141][G train loss: 0.639875]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.065626][G eval loss: 0.714612]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.063494][G train loss: 0.679037]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.050997][G eval loss: 0.747777]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.049943][G train loss: 0.712014]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.042900][G eval loss: 0.778121]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.042463][G train loss: 0.741831]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.040191][G eval loss: 0.790607]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.040499][G train loss: 0.751803]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.236149][G eval loss: 0.504640]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.259753][G train loss: 0.418409]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.436627][G eval loss: 0.268616]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.457529][G train loss: 0.232630]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.452921][G eval loss: 0.271273]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.490105][G train loss: 0.236125]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.458813][G eval loss: 0.274258]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.508916][G train loss: 0.239184]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.463183][G eval loss: 0.274216]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.521116][G train loss: 0.239140]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.466017][G eval loss: 0.271764]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.524170][G train loss: 0.236661]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.467974][G eval loss: 0.268187]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.525230][G train loss: 0.233073]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.469232][G eval loss: 0.264120]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.524151][G train loss: 0.228984]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.470068][G eval loss: 0.259823]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.514696][G train loss: 0.224661]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.470759][G eval loss: 0.255521]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.509428][G train loss: 0.220328]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.470856][G eval loss: 0.251510]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.501133][G train loss: 0.216287]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.470398][G eval loss: 0.248112]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.498082][G train loss: 0.212860]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.469612][G eval loss: 0.245673]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.494222][G train loss: 0.210382]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.468294][G eval loss: 0.244288]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.485850][G train loss: 0.208959]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.466896][G eval loss: 0.243937]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.475203][G train loss: 0.208566]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.499825][G eval loss: 0.244443]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.473164][G train loss: 0.209023]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.497646][G eval loss: 0.245408]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.462638][G train loss: 0.209941]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.495019][G eval loss: 0.246334]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.459879][G train loss: 0.210807]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.039145][G eval loss: 1.100826]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.012338][G train loss: 1.065112]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.039263][G eval loss: 1.105291]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.012346][G train loss: 1.069587]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.039386][G eval loss: 1.109401]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.012354][G train loss: 1.073656]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.039514][G eval loss: 1.113192]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.012363][G train loss: 1.077418]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.039638][G eval loss: 1.116718]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.012375][G train loss: 1.080918]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.039729][G eval loss: 1.120282]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.012382][G train loss: 1.084489]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.039780][G eval loss: 1.124038]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.012385][G train loss: 1.088283]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.039694][G eval loss: 1.128184]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.012395][G train loss: 1.092474]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.039027][G eval loss: 1.132697]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.012428][G train loss: 1.097032]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.026214][G eval loss: 1.137256]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.012348][G train loss: 1.101610]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.003868][G eval loss: 1.141528]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.011808][G train loss: 1.105872]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.003460][G eval loss: 1.145403]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.003642][G train loss: 1.109738]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.003371][G eval loss: 1.148802]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.003402][G train loss: 1.113115]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.003324][G eval loss: 1.151757]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.003337][G train loss: 1.116027]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.003275][G eval loss: 1.154235]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.003280][G train loss: 1.118430]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.003209][G eval loss: 1.156171]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.003215][G train loss: 1.120250]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.003139][G eval loss: 1.157416]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.003152][G train loss: 1.121265]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.003084][G eval loss: 1.156950]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.003110][G train loss: 1.120220]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.003106][G eval loss: 1.151884]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.003170][G train loss: 1.113439]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.011642][G eval loss: 1.082045]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.011728][G train loss: 1.028826]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.074264][G eval loss: 0.999905]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.129245][G train loss: 0.831689]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.110880][G eval loss: 0.932587]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.194535][G train loss: 0.701688]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.147881][G eval loss: 0.861142]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.220734][G train loss: 0.667160]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.184988][G eval loss: 0.788885]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.247879][G train loss: 0.616200]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.184927][G eval loss: 0.793223]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.282897][G train loss: 0.543901]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.184634][G eval loss: 0.796981]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.002 lr_d=0.002 -> score=0.981615\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.0003 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.493740]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.458047]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.481839]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.446254]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.478635]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.443204]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.477677]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249988][G train loss: 0.442363]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.477290]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249984][G train loss: 0.441971]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249985][G eval loss: 0.477632]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249979][G train loss: 0.442231]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.478521]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249975][G train loss: 0.443018]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.479446]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.443842]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.479928]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249962][G train loss: 0.444246]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249964][G eval loss: 0.479709]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249954][G train loss: 0.443990]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.478828]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249944][G train loss: 0.443116]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249947][G eval loss: 0.477592]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249933][G train loss: 0.441924]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249937][G eval loss: 0.476389]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249921][G train loss: 0.440783]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249925][G eval loss: 0.475376]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249906][G train loss: 0.439831]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249911][G eval loss: 0.474606]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249889][G train loss: 0.439101]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 0.474066]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249869][G train loss: 0.438561]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249875][G eval loss: 0.473796]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249845][G train loss: 0.438251]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249853][G eval loss: 0.473880]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249818][G train loss: 0.438273]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249827][G eval loss: 0.474236]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249787][G train loss: 0.438563]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249798][G eval loss: 0.474665]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249752][G train loss: 0.438946]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249766][G eval loss: 0.474985]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249713][G train loss: 0.439248]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249731][G eval loss: 0.475162]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249670][G train loss: 0.439435]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249693][G eval loss: 0.475309]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249622][G train loss: 0.439615]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249650][G eval loss: 0.475498]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249568][G train loss: 0.439840]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.249601][G eval loss: 0.475738]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249507][G train loss: 0.440089]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.249547][G eval loss: 0.475910]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249439][G train loss: 0.440268]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.249489][G eval loss: 0.476002]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.249367][G train loss: 0.440380]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.249427][G eval loss: 0.476060]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.249287][G train loss: 0.440459]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.249355][G eval loss: 0.476136]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.249196][G train loss: 0.440542]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.249268][G eval loss: 0.476263]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.249086][G train loss: 0.440653]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.249169][G eval loss: 0.476445]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.248961][G train loss: 0.440803]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.249061][G eval loss: 0.476664]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.248823][G train loss: 0.440991]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.248947][G eval loss: 0.476857]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.248675][G train loss: 0.441177]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.248827][G eval loss: 0.477004]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.248518][G train loss: 0.441342]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.248697][G eval loss: 0.477137]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248349][G train loss: 0.441489]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.248551][G eval loss: 0.477276]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.248162][G train loss: 0.441611]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.248387][G eval loss: 0.477394]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.247958][G train loss: 0.441701]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.248206][G eval loss: 0.477492]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.247741][G train loss: 0.441746]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.248009][G eval loss: 0.477592]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.247513][G train loss: 0.441714]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.247792][G eval loss: 0.477679]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.247280][G train loss: 0.441516]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.247587][G eval loss: 0.477757]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.247052][G train loss: 0.441032]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.247376][G eval loss: 0.478124]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.246812][G train loss: 0.440310]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.247159][G eval loss: 0.478587]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.0003 -> score=0.725745\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.492428]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.456734]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.481570]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 0.445985]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 0.479411]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.443979]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.479137]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.443824]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.478821]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.443501]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.478656]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249958][G train loss: 0.443256]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249957][G eval loss: 0.478775]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249943][G train loss: 0.443271]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249942][G eval loss: 0.478930]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249924][G train loss: 0.443326]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249923][G eval loss: 0.478793]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249900][G train loss: 0.443111]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 0.478188]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249871][G train loss: 0.442470]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 0.477212]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249836][G train loss: 0.441500]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249837][G eval loss: 0.476158]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249792][G train loss: 0.440490]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249792][G eval loss: 0.475355]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249737][G train loss: 0.439750]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249738][G eval loss: 0.474886]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249670][G train loss: 0.439343]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249672][G eval loss: 0.474737]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249588][G train loss: 0.439233]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249592][G eval loss: 0.474841]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249487][G train loss: 0.439338]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249492][G eval loss: 0.475208]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249363][G train loss: 0.439665]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249376][G eval loss: 0.475873]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249216][G train loss: 0.440269]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249230][G eval loss: 0.476661]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249037][G train loss: 0.440990]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249068][G eval loss: 0.477423]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248834][G train loss: 0.441707]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248882][G eval loss: 0.477996]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248598][G train loss: 0.442263]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248667][G eval loss: 0.478358]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248323][G train loss: 0.442636]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248417][G eval loss: 0.478646]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.247999][G train loss: 0.442956]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248126][G eval loss: 0.479003]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247618][G train loss: 0.443350]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247779][G eval loss: 0.479422]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247164][G train loss: 0.443784]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247374][G eval loss: 0.479907]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246640][G train loss: 0.444281]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246905][G eval loss: 0.480406]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246038][G train loss: 0.444806]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246372][G eval loss: 0.480631]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245363][G train loss: 0.445056]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245772][G eval loss: 0.480848]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244601][G train loss: 0.445291]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245130][G eval loss: 0.481233]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243781][G train loss: 0.445677]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.244402][G eval loss: 0.481869]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.0005 -> score=0.726271\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 0.490498]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250007][G train loss: 0.454804]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.481278]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 0.445692]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 0.480424]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 0.444992]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.480719]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249971][G train loss: 0.445405]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.480171]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249954][G train loss: 0.444852]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249945][G eval loss: 0.479437]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249927][G train loss: 0.444036]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.478919]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249889][G train loss: 0.443416]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 0.478578]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249835][G train loss: 0.442975]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249813][G eval loss: 0.478090]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249762][G train loss: 0.442408]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249735][G eval loss: 0.477506]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249666][G train loss: 0.441790]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249630][G eval loss: 0.476628]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249537][G train loss: 0.440917]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249487][G eval loss: 0.475726]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249360][G train loss: 0.440060]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249297][G eval loss: 0.475144]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249116][G train loss: 0.439541]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249037][G eval loss: 0.474986]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248789][G train loss: 0.439445]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248738][G eval loss: 0.475130]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248411][G train loss: 0.439629]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248379][G eval loss: 0.475665]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247950][G train loss: 0.440167]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247891][G eval loss: 0.476588]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247333][G train loss: 0.441050]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247276][G eval loss: 0.477981]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246555][G train loss: 0.442385]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246648][G eval loss: 0.479481]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245728][G train loss: 0.443822]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245765][G eval loss: 0.480725]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244612][G train loss: 0.445020]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244617][G eval loss: 0.481917]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243186][G train loss: 0.446195]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243252][G eval loss: 0.482861]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241494][G train loss: 0.447148]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241578][G eval loss: 0.483849]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239458][G train loss: 0.448163]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239561][G eval loss: 0.485037]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237029][G train loss: 0.449387]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.237145][G eval loss: 0.486421]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.0008 -> score=0.723566\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.0015 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250053][G eval loss: 0.486357]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250051][G train loss: 0.450664]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.480466]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 0.444881]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.482661]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249966][G train loss: 0.447229]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.484208]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249979][G train loss: 0.448895]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.483420]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249947][G train loss: 0.448101]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249906][G eval loss: 0.482354]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249865][G train loss: 0.446954]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249816][G eval loss: 0.481525]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249749][G train loss: 0.446022]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249664][G eval loss: 0.480777]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249553][G train loss: 0.445174]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249426][G eval loss: 0.479889]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249247][G train loss: 0.444210]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249066][G eval loss: 0.478855]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248785][G train loss: 0.443143]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248515][G eval loss: 0.477918]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248071][G train loss: 0.442213]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247676][G eval loss: 0.477564]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246980][G train loss: 0.441904]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246398][G eval loss: 0.478024]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245311][G train loss: 0.442429]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244469][G eval loss: 0.479712]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242802][G train loss: 0.444183]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.242054][G eval loss: 0.482382]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239542][G train loss: 0.446916]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.238918][G eval loss: 0.485954]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235289][G train loss: 0.450499]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.234947][G eval loss: 0.490688]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.229857][G train loss: 0.455203]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.230249][G eval loss: 0.495887]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223445][G train loss: 0.460349]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.223998][G eval loss: 0.501540]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.215604][G train loss: 0.465926]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.215616][G eval loss: 0.508089]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.206096][G train loss: 0.472414]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.204897][G eval loss: 0.516195]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.194636][G train loss: 0.480487]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.192031][G eval loss: 0.525875]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.181323][G train loss: 0.490150]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.177024][G eval loss: 0.537688]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.166355][G train loss: 0.501946]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.160059][G eval loss: 0.553489]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.149357][G train loss: 0.517656]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.142196][G eval loss: 0.572087]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.132129][G train loss: 0.536034]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.124038][G eval loss: 0.589683]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.117764][G train loss: 0.552973]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.113250][G eval loss: 0.592801]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.111973][G train loss: 0.552079]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.204176][G eval loss: 0.436860]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.229163][G train loss: 0.358379]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.323044][G eval loss: 0.304448]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.325682][G train loss: 0.268915]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.359985][G eval loss: 0.293978]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.359194][G train loss: 0.258347]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.371355][G eval loss: 0.291053]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.370165][G train loss: 0.255437]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.377741][G eval loss: 0.284081]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.376578][G train loss: 0.248454]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.381398][G eval loss: 0.275930]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.380376][G train loss: 0.240798]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.383095][G eval loss: 0.271036]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.382592][G train loss: 0.236329]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.383280][G eval loss: 0.270697]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.386850][G train loss: 0.235935]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.382344][G eval loss: 0.271719]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.387822][G train loss: 0.236632]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.380439][G eval loss: 0.271212]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.386262][G train loss: 0.235611]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.377676][G eval loss: 0.268627]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.383124][G train loss: 0.232350]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.374074][G eval loss: 0.264747]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.379277][G train loss: 0.227692]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.369676][G eval loss: 0.260829]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.372866][G train loss: 0.223044]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.364721][G eval loss: 0.257978]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.364567][G train loss: 0.219538]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.359002][G eval loss: 0.256791]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.358870][G train loss: 0.217770]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.352584][G eval loss: 0.257255]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.352487][G train loss: 0.217449]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.345549][G eval loss: 0.258539]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.345490][G train loss: 0.217601]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.338205][G eval loss: 0.259809]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.338171][G train loss: 0.217493]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.330296][G eval loss: 0.260816]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.330292][G train loss: 0.217131]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.322008][G eval loss: 0.261636]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.322028][G train loss: 0.216871]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.313513][G eval loss: 0.262010]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.313461][G train loss: 0.217475]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.305104][G eval loss: 0.262493]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.299945][G train loss: 0.229435]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.296754][G eval loss: 0.265101]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.294353][G train loss: 0.223942]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.288675][G eval loss: 0.270819]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.287294][G train loss: 0.228320]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.281095][G eval loss: 0.278441]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.278848][G train loss: 0.237264]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.274939][G eval loss: 0.286563]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.274166][G train loss: 0.242573]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.269342][G eval loss: 0.296677]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.0015 -> score=0.566019\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.002 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250108][G eval loss: 0.483399]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250106][G train loss: 0.447705]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.479806]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249976][G train loss: 0.444221]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249946][G eval loss: 0.484010]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249928][G train loss: 0.448579]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249959][G eval loss: 0.485684]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249932][G train loss: 0.450370]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249878][G eval loss: 0.485071]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249818][G train loss: 0.449753]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249709][G eval loss: 0.484582]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249587][G train loss: 0.449182]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249355][G eval loss: 0.483990]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249131][G train loss: 0.448489]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.248852][G eval loss: 0.483929]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248458][G train loss: 0.448331]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.247856][G eval loss: 0.484141]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.247177][G train loss: 0.448469]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.246332][G eval loss: 0.484371]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.245190][G train loss: 0.448671]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.243743][G eval loss: 0.485491]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.241846][G train loss: 0.449801]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.239989][G eval loss: 0.488011]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.236828][G train loss: 0.452369]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.234823][G eval loss: 0.492494]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.002 -> score=0.727317\n",
      "\n",
      "----- 2330: lr_g=0.006, lr_d=0.0003 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.487769]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 0.452071]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.479824]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.444391]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.477480]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249993][G train loss: 0.442214]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.476672]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249989][G train loss: 0.441332]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.477445]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249984][G train loss: 0.441946]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249985][G eval loss: 0.478993]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249980][G train loss: 0.443337]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.479603]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249975][G train loss: 0.443869]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.478961]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.443263]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.477923]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249963][G train loss: 0.442318]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249964][G eval loss: 0.476870]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249954][G train loss: 0.441326]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.475828]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249944][G train loss: 0.440258]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249944][G eval loss: 0.475049]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249931][G train loss: 0.439413]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249932][G eval loss: 0.474697]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249917][G train loss: 0.439020]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.474671]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249901][G train loss: 0.438977]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249905][G eval loss: 0.474543]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249883][G train loss: 0.438859]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249889][G eval loss: 0.474372]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249864][G train loss: 0.438715]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249871][G eval loss: 0.474262]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249841][G train loss: 0.438642]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249850][G eval loss: 0.474113]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249815][G train loss: 0.438501]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249826][G eval loss: 0.473931]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249786][G train loss: 0.438307]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249801][G eval loss: 0.473733]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249754][G train loss: 0.438135]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249771][G eval loss: 0.473767]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249717][G train loss: 0.438187]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249738][G eval loss: 0.473953]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249675][G train loss: 0.438346]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249701][G eval loss: 0.474232]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249628][G train loss: 0.438559]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249662][G eval loss: 0.474489]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249576][G train loss: 0.438730]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.249618][G eval loss: 0.474636]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249517][G train loss: 0.438776]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.249573][G eval loss: 0.474524]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.249456][G train loss: 0.438534]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.249529][G eval loss: 0.473929]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.249388][G train loss: 0.437753]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.249481][G eval loss: 0.472614]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.249312][G train loss: 0.436126]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.249437][G eval loss: 0.469234]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.249231][G train loss: 0.432076]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.249415][G eval loss: 0.459765]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.249162][G train loss: 0.421427]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.249447][G eval loss: 0.446186]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.249134][G train loss: 0.405866]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.249574][G eval loss: 0.436364]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.249193][G train loss: 0.395137]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.249773][G eval loss: 0.435531]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249325][G train loss: 0.395523]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.249945][G eval loss: 0.432420]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.249439][G train loss: 0.394023]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 0.422647]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.249426][G train loss: 0.384413]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.249825][G eval loss: 0.412120]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.249219][G train loss: 0.372998]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.249550][G eval loss: 0.411324]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.248914][G train loss: 0.371386]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.249650][G eval loss: 0.405233]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.248938][G train loss: 0.364054]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.249961][G eval loss: 0.400149]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.249142][G train loss: 0.357257]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.250256][G eval loss: 0.398430]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.249331][G train loss: 0.354617]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.250512][G eval loss: 0.397307]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.249500][G train loss: 0.353690]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.250744][G eval loss: 0.395650]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.249659][G train loss: 0.352897]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.250932][G eval loss: 0.392978]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.249782][G train loss: 0.351216]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.251075][G eval loss: 0.389781]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.249878][G train loss: 0.348984]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.251144][G eval loss: 0.386907]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.249882][G train loss: 0.346849]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.251153][G eval loss: 0.384695]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.249805][G train loss: 0.345099]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.251140][G eval loss: 0.382990]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.249684][G train loss: 0.343734]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.251136][G eval loss: 0.381355]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.249558][G train loss: 0.342302]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.251170][G eval loss: 0.379202]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.249462][G train loss: 0.340499]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.251260][G eval loss: 0.376848]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.249435][G train loss: 0.338517]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.251376][G eval loss: 0.375334]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.249466][G train loss: 0.337133]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.251476][G eval loss: 0.375396]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.249496][G train loss: 0.337184]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.251505][G eval loss: 0.376464]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.249470][G train loss: 0.338064]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.251405][G eval loss: 0.376922]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.249312][G train loss: 0.338205]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.251177][G eval loss: 0.376527]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.249011][G train loss: 0.337383]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.250860][G eval loss: 0.376425]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.248592][G train loss: 0.336667]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.250493][G eval loss: 0.377269]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.248117][G train loss: 0.336784]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.250137][G eval loss: 0.378528]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.006 lr_d=0.0003 -> score=0.628664\n",
      "\n",
      "----- 2330: lr_g=0.006, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.486456]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 0.450758]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.479562]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.444128]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 0.478264]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.442998]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 0.478138]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249978][G train loss: 0.442797]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.478972]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249970][G train loss: 0.443473]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.480007]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249959][G train loss: 0.444351]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.479843]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249944][G train loss: 0.444110]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.478429]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249925][G train loss: 0.442732]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249923][G eval loss: 0.476772]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249902][G train loss: 0.441167]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 0.475340]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249873][G train loss: 0.439796]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249869][G eval loss: 0.474203]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249835][G train loss: 0.438634]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249829][G eval loss: 0.473613]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249787][G train loss: 0.437978]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249780][G eval loss: 0.473672]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249727][G train loss: 0.437996]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249721][G eval loss: 0.474206]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.006 lr_d=0.0005 -> score=0.723928\n",
      "\n",
      "----- 2330: lr_g=0.006, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.484525]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 0.448827]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.479276]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 0.443842]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.479288]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 0.444022]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.479723]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249974][G train loss: 0.444383]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.480315]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249960][G train loss: 0.444815]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 0.480766]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249932][G train loss: 0.445111]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 0.479945]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249895][G train loss: 0.444212]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249879][G eval loss: 0.478011]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249845][G train loss: 0.442314]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249827][G eval loss: 0.475982]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249780][G train loss: 0.440378]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249756][G eval loss: 0.474508]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249691][G train loss: 0.438966]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249653][G eval loss: 0.473451]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249563][G train loss: 0.437884]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249504][G eval loss: 0.473018]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249382][G train loss: 0.437387]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249310][G eval loss: 0.473312]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249139][G train loss: 0.437638]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249056][G eval loss: 0.474171]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248815][G train loss: 0.438479]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248715][G eval loss: 0.475124]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248392][G train loss: 0.439442]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248359][G eval loss: 0.476063]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247936][G train loss: 0.440412]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247913][G eval loss: 0.477107]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247366][G train loss: 0.441496]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247357][G eval loss: 0.478206]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246656][G train loss: 0.442616]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246818][G eval loss: 0.479134]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245927][G train loss: 0.443540]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.246035][G eval loss: 0.479696]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244923][G train loss: 0.444131]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244981][G eval loss: 0.480510]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243604][G train loss: 0.444977]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243711][G eval loss: 0.481497]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242011][G train loss: 0.445954]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.242201][G eval loss: 0.482513]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240130][G train loss: 0.446940]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.240406][G eval loss: 0.483647]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237910][G train loss: 0.448038]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.238285][G eval loss: 0.484812]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.006 lr_d=0.0008 -> score=0.723097\n",
      "\n",
      "----- 2330: lr_g=0.006, lr_d=0.0015 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250055][G eval loss: 0.480384]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250053][G train loss: 0.444686]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.478451]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 0.443018]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.481499]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249970][G train loss: 0.446233]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.483183]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249987][G train loss: 0.447842]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.483548]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249958][G train loss: 0.448049]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.483678]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249881][G train loss: 0.448023]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249826][G eval loss: 0.482486]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249769][G train loss: 0.446754]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249680][G eval loss: 0.480109]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249586][G train loss: 0.444414]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249471][G eval loss: 0.477623]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249317][G train loss: 0.442022]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249152][G eval loss: 0.475688]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248904][G train loss: 0.440150]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248632][G eval loss: 0.474543]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248235][G train loss: 0.438984]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247813][G eval loss: 0.474602]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247187][G train loss: 0.438984]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246543][G eval loss: 0.476145]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245564][G train loss: 0.440488]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244703][G eval loss: 0.478896]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.243190][G train loss: 0.443218]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.242521][G eval loss: 0.482086]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.240219][G train loss: 0.446419]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.239648][G eval loss: 0.485874]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.236312][G train loss: 0.450241]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.235943][G eval loss: 0.490547]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.231269][G train loss: 0.454937]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.231459][G eval loss: 0.495721]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.225194][G train loss: 0.460157]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.225737][G eval loss: 0.500915]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.217928][G train loss: 0.465361]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.218015][G eval loss: 0.506921]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.209088][G train loss: 0.471405]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.207846][G eval loss: 0.514858]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.198196][G train loss: 0.479388]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.195628][G eval loss: 0.524674]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.185314][G train loss: 0.489255]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.181107][G eval loss: 0.536736]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.170632][G train loss: 0.501386]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.164680][G eval loss: 0.552292]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.153895][G train loss: 0.517065]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.148742][G eval loss: 0.567251]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.137801][G train loss: 0.532484]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.141849][G eval loss: 0.560862]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.131873][G train loss: 0.527562]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.273446][G eval loss: 0.348621]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.250499][G train loss: 0.340805]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.331230][G eval loss: 0.309062]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.324701][G train loss: 0.273427]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.346302][G eval loss: 0.294032]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.343434][G train loss: 0.257672]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.352888][G eval loss: 0.279029]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.350186][G train loss: 0.242396]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.356284][G eval loss: 0.281558]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.354166][G train loss: 0.242595]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.357681][G eval loss: 0.283939]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.356139][G train loss: 0.242219]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.357663][G eval loss: 0.277154]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.356650][G train loss: 0.233329]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.356468][G eval loss: 0.263631]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.356625][G train loss: 0.218843]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.354295][G eval loss: 0.250039]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.357835][G train loss: 0.204761]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.351308][G eval loss: 0.238889]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.349895][G train loss: 0.205234]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.347597][G eval loss: 0.229815]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.338957][G train loss: 0.207781]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.343223][G eval loss: 0.226337]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.337014][G train loss: 0.197307]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.338131][G eval loss: 0.223538]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.331309][G train loss: 0.197194]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.332390][G eval loss: 0.222935]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.325668][G train loss: 0.199246]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.326161][G eval loss: 0.224469]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.319715][G train loss: 0.202823]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.319319][G eval loss: 0.227570]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.313276][G train loss: 0.206572]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.312366][G eval loss: 0.231773]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.306712][G train loss: 0.210502]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.305195][G eval loss: 0.237183]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.299885][G train loss: 0.215279]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.297987][G eval loss: 0.245075]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.293548][G train loss: 0.217821]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.290943][G eval loss: 0.255760]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.290794][G train loss: 0.218972]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.284194][G eval loss: 0.268332]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.006 lr_d=0.0015 -> score=0.552526\n",
      "\n",
      "----- 2330: lr_g=0.006, lr_d=0.002 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250110][G eval loss: 0.477425]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250108][G train loss: 0.441728]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 0.477798]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249979][G train loss: 0.442364]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 0.482882]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249936][G train loss: 0.447616]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.484677]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249947][G train loss: 0.449337]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249896][G eval loss: 0.485205]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249845][G train loss: 0.449707]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249712][G eval loss: 0.485858]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249610][G train loss: 0.450204]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249402][G eval loss: 0.484771]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249219][G train loss: 0.449041]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.248937][G eval loss: 0.482078]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248622][G train loss: 0.446385]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.248187][G eval loss: 0.479822]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.247647][G train loss: 0.444228]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.246771][G eval loss: 0.478898]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.245831][G train loss: 0.443378]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.244434][G eval loss: 0.479262]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.242760][G train loss: 0.443732]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.240782][G eval loss: 0.481572]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.237947][G train loss: 0.445991]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.235587][G eval loss: 0.486775]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.006 lr_d=0.002 -> score=0.722362\n",
      "\n",
      "----- 2330: lr_g=0.008, lr_d=0.0003 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.485120]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249999][G train loss: 0.449401]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.488995]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.453405]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.480398]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249993][G train loss: 0.444971]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 0.477391]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249987][G train loss: 0.441766]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.476758]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249982][G train loss: 0.440952]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.478301]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.442427]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.479883]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249972][G train loss: 0.444064]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.479680]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249966][G train loss: 0.444030]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.478930]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249959][G train loss: 0.443422]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249961][G eval loss: 0.478170]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249951][G train loss: 0.442688]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249952][G eval loss: 0.477130]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249940][G train loss: 0.441581]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249942][G eval loss: 0.476055]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249927][G train loss: 0.440411]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249930][G eval loss: 0.475373]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249911][G train loss: 0.439657]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249916][G eval loss: 0.475142]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249894][G train loss: 0.439389]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249901][G eval loss: 0.474965]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249875][G train loss: 0.439232]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249885][G eval loss: 0.474526]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249855][G train loss: 0.438859]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249868][G eval loss: 0.474199]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249833][G train loss: 0.438599]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249849][G eval loss: 0.474218]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249809][G train loss: 0.438699]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249826][G eval loss: 0.474193]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249779][G train loss: 0.438673]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249798][G eval loss: 0.474073]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249743][G train loss: 0.438463]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249766][G eval loss: 0.474173]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249701][G train loss: 0.438432]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249729][G eval loss: 0.474582]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.008 lr_d=0.0003 -> score=0.724312\n",
      "\n",
      "----- 2330: lr_g=0.008, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250001][G eval loss: 0.483807]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249999][G train loss: 0.448088]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.488737]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.453148]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.481191]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.445764]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.478864]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.443238]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.478293]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 0.442487]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249965][G eval loss: 0.479324]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249955][G train loss: 0.443450]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249951][G eval loss: 0.480136]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249938][G train loss: 0.444317]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249935][G eval loss: 0.479169]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249918][G train loss: 0.443519]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249917][G eval loss: 0.477806]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 0.442298]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249894][G eval loss: 0.476663]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249864][G train loss: 0.441181]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249864][G eval loss: 0.475513]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249826][G train loss: 0.439965]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249825][G eval loss: 0.474601]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249776][G train loss: 0.438958]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249776][G eval loss: 0.474301]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249714][G train loss: 0.438586]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249715][G eval loss: 0.474601]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249637][G train loss: 0.438850]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249646][G eval loss: 0.475046]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249547][G train loss: 0.439317]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249567][G eval loss: 0.475271]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249444][G train loss: 0.439609]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249478][G eval loss: 0.475603]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249325][G train loss: 0.440008]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249375][G eval loss: 0.476247]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249185][G train loss: 0.440732]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249257][G eval loss: 0.476761]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249024][G train loss: 0.441245]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249090][G eval loss: 0.477069]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248804][G train loss: 0.441464]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248903][G eval loss: 0.477452]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248552][G train loss: 0.441727]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248686][G eval loss: 0.478056]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248256][G train loss: 0.442274]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248439][G eval loss: 0.478399]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.247916][G train loss: 0.442639]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248166][G eval loss: 0.478400]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247528][G train loss: 0.442698]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247852][G eval loss: 0.478458]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247081][G train loss: 0.442795]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247526][G eval loss: 0.478376]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246610][G train loss: 0.442763]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.247179][G eval loss: 0.478251]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246098][G train loss: 0.442693]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246760][G eval loss: 0.478148]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245478][G train loss: 0.442580]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.246289][G eval loss: 0.477592]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244751][G train loss: 0.441934]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245915][G eval loss: 0.476431]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.244071][G train loss: 0.440619]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.245847][G eval loss: 0.471691]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.243603][G train loss: 0.435965]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.246313][G eval loss: 0.461541]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.243543][G train loss: 0.426106]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.247600][G eval loss: 0.444473]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.244133][G train loss: 0.409452]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.249680][G eval loss: 0.426396]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.245360][G train loss: 0.391680]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.251537][G eval loss: 0.420165]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.246319][G train loss: 0.386176]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.252756][G eval loss: 0.412882]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.246721][G train loss: 0.379779]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.253611][G eval loss: 0.398919]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.246775][G train loss: 0.366829]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.254841][G eval loss: 0.382988]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.246614][G train loss: 0.353243]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.256196][G eval loss: 0.370317]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.246264][G train loss: 0.343973]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.257077][G eval loss: 0.362102]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.246168][G train loss: 0.338192]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.257975][G eval loss: 0.355676]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.247233][G train loss: 0.331586]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.260354][G eval loss: 0.351706]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.249287][G train loss: 0.324751]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.265130][G eval loss: 0.342641]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.251434][G train loss: 0.319273]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.266013][G eval loss: 0.336677]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.252427][G train loss: 0.310886]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.266054][G eval loss: 0.331679]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.253383][G train loss: 0.302663]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.265726][G eval loss: 0.326353]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.253939][G train loss: 0.296594]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.265137][G eval loss: 0.320982]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.254127][G train loss: 0.292751]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.263694][G eval loss: 0.317242]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.254275][G train loss: 0.289240]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.261797][G eval loss: 0.316187]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.254110][G train loss: 0.286843]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.258770][G eval loss: 0.319416]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.254677][G train loss: 0.285044]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.256622][G eval loss: 0.323694]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.008 lr_d=0.0005 -> score=0.580317\n",
      "\n",
      "----- 2330: lr_g=0.008, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250012][G eval loss: 0.481875]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250009][G train loss: 0.446156]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.488458]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 0.452868]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 0.482220]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 0.446793]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.480460]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249972][G train loss: 0.444835]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.479651]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249955][G train loss: 0.443845]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.480111]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249924][G train loss: 0.444238]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249905][G eval loss: 0.480293]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249880][G train loss: 0.444474]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249862][G eval loss: 0.478770]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249825][G train loss: 0.443120]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249808][G eval loss: 0.477063]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249755][G train loss: 0.441555]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249738][G eval loss: 0.475903]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249662][G train loss: 0.440422]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249635][G eval loss: 0.474866]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249530][G train loss: 0.439321]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249494][G eval loss: 0.474158]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249348][G train loss: 0.438517]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249302][G eval loss: 0.474154]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249098][G train loss: 0.438442]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249062][G eval loss: 0.474830]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248778][G train loss: 0.439084]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248758][G eval loss: 0.475755]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248363][G train loss: 0.440031]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248377][G eval loss: 0.476591]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247848][G train loss: 0.440936]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247937][G eval loss: 0.477478]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247248][G train loss: 0.441893]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247467][G eval loss: 0.478703]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246579][G train loss: 0.443193]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246832][G eval loss: 0.479309]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245714][G train loss: 0.443793]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.246020][G eval loss: 0.479638]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244630][G train loss: 0.444038]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.245013][G eval loss: 0.480526]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243281][G train loss: 0.444837]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243828][G eval loss: 0.482089]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241693][G train loss: 0.446340]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.242531][G eval loss: 0.483383]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239893][G train loss: 0.447661]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.240957][G eval loss: 0.484614]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237701][G train loss: 0.448996]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.239095][G eval loss: 0.486064]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235085][G train loss: 0.450542]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.237066][G eval loss: 0.487566]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.232174][G train loss: 0.452090]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.234816][G eval loss: 0.489008]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.228932][G train loss: 0.453591]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.232392][G eval loss: 0.490695]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.225391][G train loss: 0.455330]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.229668][G eval loss: 0.492424]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.008 lr_d=0.0008 -> score=0.722092\n",
      "\n",
      "----- 2330: lr_g=0.008, lr_d=0.0015 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250057][G eval loss: 0.477733]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250055][G train loss: 0.442014]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.487627]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 0.452037]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.484423]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.448996]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.483924]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249984][G train loss: 0.448299]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.482884]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249956][G train loss: 0.447078]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249905][G eval loss: 0.483021]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249869][G train loss: 0.447148]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249808][G eval loss: 0.482836]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249746][G train loss: 0.447017]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249668][G eval loss: 0.480882]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249562][G train loss: 0.445232]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249466][G eval loss: 0.478739]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249286][G train loss: 0.443233]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249162][G eval loss: 0.477177]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248861][G train loss: 0.441699]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248669][G eval loss: 0.476130]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248181][G train loss: 0.440590]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247879][G eval loss: 0.476059]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247102][G train loss: 0.440430]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246674][G eval loss: 0.477361]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245452][G train loss: 0.441672]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244896][G eval loss: 0.480154]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.243008][G train loss: 0.444444]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.242883][G eval loss: 0.483274]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.240057][G train loss: 0.447594]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.240327][G eval loss: 0.487048]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.236195][G train loss: 0.451433]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.237191][G eval loss: 0.491776]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.231393][G train loss: 0.456225]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.233597][G eval loss: 0.497170]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.225699][G train loss: 0.461652]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.229218][G eval loss: 0.502713]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.218804][G train loss: 0.467188]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.223435][G eval loss: 0.509010]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.210363][G train loss: 0.473435]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.215602][G eval loss: 0.517297]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.199768][G train loss: 0.481660]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.205717][G eval loss: 0.527341]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.187343][G train loss: 0.491622]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.193251][G eval loss: 0.539310]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.173633][G train loss: 0.503388]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.179742][G eval loss: 0.553371]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.160137][G train loss: 0.516334]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.174372][G eval loss: 0.552395]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.158782][G train loss: 0.506121]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.230580][G eval loss: 0.464054]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.231259][G train loss: 0.397012]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.301608][G eval loss: 0.363048]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.292151][G train loss: 0.320577]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.370317][G eval loss: 0.301195]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.344455][G train loss: 0.264954]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.386158][G eval loss: 0.296323]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.357072][G train loss: 0.261235]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.394667][G eval loss: 0.301051]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.363341][G train loss: 0.266824]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.399201][G eval loss: 0.299855]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.366506][G train loss: 0.265622]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.400439][G eval loss: 0.291211]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.366955][G train loss: 0.256308]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.397786][G eval loss: 0.280613]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.364901][G train loss: 0.244455]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.386810][G eval loss: 0.272171]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.360818][G train loss: 0.234994]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.370939][G eval loss: 0.269330]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.352652][G train loss: 0.231375]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.363528][G eval loss: 0.268561]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.348105][G train loss: 0.230771]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.354603][G eval loss: 0.270706]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.342238][G train loss: 0.232106]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.349605][G eval loss: 0.271310]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.337270][G train loss: 0.233025]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.344254][G eval loss: 0.272395]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.330354][G train loss: 0.234213]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.336522][G eval loss: 0.273112]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.322548][G train loss: 0.235036]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.325635][G eval loss: 0.272657]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.315703][G train loss: 0.234590]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.312424][G eval loss: 0.274659]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.305638][G train loss: 0.235801]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.264251][G eval loss: 0.330083]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.264826][G train loss: 0.303773]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.287445][G eval loss: 0.292868]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.292034][G train loss: 0.252536]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.281101][G eval loss: 0.299803]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.285654][G train loss: 0.260242]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.275421][G eval loss: 0.302010]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.278530][G train loss: 0.263377]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.270186][G eval loss: 0.303923]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.270932][G train loss: 0.266002]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.265735][G eval loss: 0.309795]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.265788][G train loss: 0.272080]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.261868][G eval loss: 0.320576]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.261944][G train loss: 0.282557]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.258788][G eval loss: 0.331662]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.258846][G train loss: 0.293342]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.256171][G eval loss: 0.342420]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.256218][G train loss: 0.303941]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.254009][G eval loss: 0.352454]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.254045][G train loss: 0.314229]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.252235][G eval loss: 0.364210]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.252268][G train loss: 0.326526]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.250955][G eval loss: 0.376362]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.250980][G train loss: 0.339357]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.250208][G eval loss: 0.388392]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.250228][G train loss: 0.351595]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.401065]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.249987][G train loss: 0.364558]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.250214][G eval loss: 0.413010]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.250231][G train loss: 0.377451]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.250823][G eval loss: 0.421276]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.250837][G train loss: 0.387029]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.251628][G eval loss: 0.428759]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.251635][G train loss: 0.395157]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.252411][G eval loss: 0.433596]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.252415][G train loss: 0.400417]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.253157][G eval loss: 0.436951]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.253159][G train loss: 0.404115]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.253794][G eval loss: 0.439620]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.253794][G train loss: 0.406744]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.254230][G eval loss: 0.440987]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.254223][G train loss: 0.408003]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.254403][G eval loss: 0.440510]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.254395][G train loss: 0.407609]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.254423][G eval loss: 0.439990]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.254415][G train loss: 0.406858]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.254307][G eval loss: 0.439209]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.254299][G train loss: 0.405608]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.254073][G eval loss: 0.436568]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.254065][G train loss: 0.402626]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.253741][G eval loss: 0.432772]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.253732][G train loss: 0.398337]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.253334][G eval loss: 0.428607]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.253326][G train loss: 0.393124]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.252879][G eval loss: 0.424665]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.252872][G train loss: 0.387078]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.252420][G eval loss: 0.419775]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.252415][G train loss: 0.380710]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.251971][G eval loss: 0.411651]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.251965][G train loss: 0.374098]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.251537][G eval loss: 0.402734]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.251527][G train loss: 0.367290]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.251143][G eval loss: 0.394486]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.251137][G train loss: 0.360319]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.250793][G eval loss: 0.386713]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.250784][G train loss: 0.353005]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.250498][G eval loss: 0.380565]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.250488][G train loss: 0.346048]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.250268][G eval loss: 0.374247]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.250262][G train loss: 0.339094]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.250115][G eval loss: 0.367741]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.250109][G train loss: 0.331935]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.250034][G eval loss: 0.362051]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250030][G train loss: 0.325662]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.357495]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.249998][G train loss: 0.320175]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.250012][G eval loss: 0.353734]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.250011][G train loss: 0.315457]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.250065][G eval loss: 0.350709]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.250064][G train loss: 0.310408]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.250153][G eval loss: 0.347724]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.250153][G train loss: 0.304784]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.250270][G eval loss: 0.344688]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.250272][G train loss: 0.300290]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.250391][G eval loss: 0.341096]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.250390][G train loss: 0.296556]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.250505][G eval loss: 0.336059]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.250504][G train loss: 0.292679]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.250618][G eval loss: 0.332577]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.250614][G train loss: 0.290398]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.250733][G eval loss: 0.329707]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.250728][G train loss: 0.287324]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.250839][G eval loss: 0.327388]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.250838][G train loss: 0.284718]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.250933][G eval loss: 0.325747]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.250935][G train loss: 0.283078]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.251007][G eval loss: 0.323181]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.251009][G train loss: 0.280759]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.251058][G eval loss: 0.321139]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.251059][G train loss: 0.279566]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.251074][G eval loss: 0.320839]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.251067][G train loss: 0.279294]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.251064][G eval loss: 0.321125]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.251054][G train loss: 0.278872]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.251031][G eval loss: 0.322280]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.008 lr_d=0.0015 -> score=0.573311\n",
      "\n",
      "----- 2330: lr_g=0.008, lr_d=0.002 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.008, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250111][G eval loss: 0.474775]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250109][G train loss: 0.439056]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.486982]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249981][G train loss: 0.451392]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.485822]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249940][G train loss: 0.450395]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 0.485428]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249949][G train loss: 0.449803]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249892][G eval loss: 0.484569]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249842][G train loss: 0.448764]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249720][G eval loss: 0.485167]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249615][G train loss: 0.449294]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249401][G eval loss: 0.485396]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249197][G train loss: 0.449578]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.248909][G eval loss: 0.484119]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248532][G train loss: 0.448469]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.248041][G eval loss: 0.483138]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.247350][G train loss: 0.447637]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.246588][G eval loss: 0.483134]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.245361][G train loss: 0.447665]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.244360][G eval loss: 0.484176]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.242285][G train loss: 0.448654]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.241039][G eval loss: 0.486876]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.237656][G train loss: 0.451278]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.236671][G eval loss: 0.491567]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.231254][G train loss: 0.455919]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.230926][G eval loss: 0.498594]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.222785][G train loss: 0.462932]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.223629][G eval loss: 0.507132]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.212205][G train loss: 0.471481]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.214254][G eval loss: 0.516987]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.199008][G train loss: 0.481360]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.201635][G eval loss: 0.528540]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.183455][G train loss: 0.492869]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.185738][G eval loss: 0.544366]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.166101][G train loss: 0.508323]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.173529][G eval loss: 0.555996]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.154022][G train loss: 0.517462]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.286800][G eval loss: 0.379058]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.281588][G train loss: 0.322217]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.367577][G eval loss: 0.316532]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.342395][G train loss: 0.280098]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.388510][G eval loss: 0.299850]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.359869][G train loss: 0.264601]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.398468][G eval loss: 0.296540]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.368157][G train loss: 0.262303]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.401551][G eval loss: 0.292888]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.370667][G train loss: 0.258847]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.395543][G eval loss: 0.285343]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.369715][G train loss: 0.250806]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.379938][G eval loss: 0.279793]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.363391][G train loss: 0.244419]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.372415][G eval loss: 0.277717]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.356449][G train loss: 0.241728]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.364524][G eval loss: 0.277679]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.351158][G train loss: 0.241681]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.355133][G eval loss: 0.280572]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.340411][G train loss: 0.244696]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.341361][G eval loss: 0.286932]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.331227][G train loss: 0.250808]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.318819][G eval loss: 0.296403]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.317286][G train loss: 0.259671]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.302815][G eval loss: 0.307674]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.306389][G train loss: 0.269955]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.292207][G eval loss: 0.319708]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.295344][G train loss: 0.280614]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.282346][G eval loss: 0.331371]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.008 lr_d=0.002 -> score=0.613717\n",
      "\n",
      "----- 2330: lr_g=0.01, lr_d=0.0003 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0003\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250001][G eval loss: 0.486087]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249999][G train loss: 0.450327]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.485738]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.450404]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.479403]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249993][G train loss: 0.444021]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 0.475870]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249987][G train loss: 0.440210]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 0.476393]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249982][G train loss: 0.440542]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.478230]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.442406]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.479511]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249973][G train loss: 0.443761]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.479074]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249967][G train loss: 0.443408]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 0.477520]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249958][G train loss: 0.441871]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249959][G eval loss: 0.476215]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249949][G train loss: 0.440622]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249950][G eval loss: 0.475363]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249938][G train loss: 0.439869]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249938][G eval loss: 0.475747]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249924][G train loss: 0.440097]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249929][G eval loss: 0.475387]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249911][G train loss: 0.439768]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.475263]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249898][G train loss: 0.439658]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249906][G eval loss: 0.474896]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249881][G train loss: 0.439209]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249889][G eval loss: 0.474578]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249859][G train loss: 0.438815]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249870][G eval loss: 0.473744]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249836][G train loss: 0.438054]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249850][G eval loss: 0.473444]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249810][G train loss: 0.437882]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249823][G eval loss: 0.473528]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249776][G train loss: 0.437971]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249792][G eval loss: 0.473767]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.249738][G train loss: 0.438146]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.249762][G eval loss: 0.473911]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.249699][G train loss: 0.438265]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.249730][G eval loss: 0.473915]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.249655][G train loss: 0.438263]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249693][G eval loss: 0.473960]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249604][G train loss: 0.438234]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249649][G eval loss: 0.473992]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.01 lr_d=0.0003 -> score=0.723641\n",
      "\n",
      "----- 2330: lr_g=0.01, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250002][G eval loss: 0.484774]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250000][G train loss: 0.449014]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.485480]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.450146]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.480196]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.444813]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.477341]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249976][G train loss: 0.441682]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.477925]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 0.442075]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.479253]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249956][G train loss: 0.443429]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 0.479767]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249940][G train loss: 0.444016]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249936][G eval loss: 0.478566]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 0.442901]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.476399]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249892][G train loss: 0.440750]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249889][G eval loss: 0.474713]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249860][G train loss: 0.439120]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249859][G eval loss: 0.473748]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249821][G train loss: 0.438254]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249817][G eval loss: 0.474286]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249769][G train loss: 0.438637]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249776][G eval loss: 0.474310]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249716][G train loss: 0.438691]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249730][G eval loss: 0.474718]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249654][G train loss: 0.439113]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249665][G eval loss: 0.474984]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249570][G train loss: 0.439297]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249580][G eval loss: 0.475349]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249460][G train loss: 0.439587]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249487][G eval loss: 0.475218]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249336][G train loss: 0.439529]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249394][G eval loss: 0.475562]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249207][G train loss: 0.440000]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249243][G eval loss: 0.476152]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249012][G train loss: 0.440599]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249079][G eval loss: 0.476766]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248797][G train loss: 0.441154]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248915][G eval loss: 0.477196]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248572][G train loss: 0.441566]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248729][G eval loss: 0.477393]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248313][G train loss: 0.441764]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248503][G eval loss: 0.477564]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.247995][G train loss: 0.441883]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248228][G eval loss: 0.477672]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247604][G train loss: 0.441928]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247903][G eval loss: 0.477750]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247145][G train loss: 0.441975]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247564][G eval loss: 0.477372]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246662][G train loss: 0.441676]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.247271][G eval loss: 0.475572]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246187][G train loss: 0.440310]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.247071][G eval loss: 0.470951]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245752][G train loss: 0.436065]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.247134][G eval loss: 0.461707]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.245489][G train loss: 0.427495]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.247931][G eval loss: 0.449120]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.245822][G train loss: 0.416160]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.249140][G eval loss: 0.437085]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.246581][G train loss: 0.404534]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.249695][G eval loss: 0.429690]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.246902][G train loss: 0.396287]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.249656][G eval loss: 0.415822]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.246713][G train loss: 0.381514]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.249231][G eval loss: 0.404094]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.246179][G train loss: 0.369734]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.248832][G eval loss: 0.399675]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.245886][G train loss: 0.364570]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.248801][G eval loss: 0.394229]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.246387][G train loss: 0.356549]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.248467][G eval loss: 0.391550]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.247075][G train loss: 0.350342]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.248314][G eval loss: 0.390580]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.247416][G train loss: 0.347241]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.248664][G eval loss: 0.383453]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.247746][G train loss: 0.340940]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.249463][G eval loss: 0.372905]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.248537][G train loss: 0.330568]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.251544][G eval loss: 0.366108]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.250494][G train loss: 0.322269]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.253842][G eval loss: 0.364921]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.252382][G train loss: 0.320111]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.254437][G eval loss: 0.363423]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.253597][G train loss: 0.315713]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.254090][G eval loss: 0.360735]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.254361][G train loss: 0.308644]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.254241][G eval loss: 0.356798]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.255149][G train loss: 0.302121]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.254854][G eval loss: 0.354381]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.255622][G train loss: 0.298282]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.254960][G eval loss: 0.353496]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.255834][G train loss: 0.296729]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.257454][G eval loss: 0.346758]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.256572][G train loss: 0.296129]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.258646][G eval loss: 0.345718]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.256586][G train loss: 0.296304]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.260566][G eval loss: 0.344568]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.256839][G train loss: 0.296828]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.261699][G eval loss: 0.343199]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.256855][G train loss: 0.296597]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.261064][G eval loss: 0.340851]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.256121][G train loss: 0.295196]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.260096][G eval loss: 0.338716]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.255486][G train loss: 0.294026]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.258883][G eval loss: 0.337496]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.254779][G train loss: 0.293812]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.257570][G eval loss: 0.337232]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.253880][G train loss: 0.294681]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.256575][G eval loss: 0.337864]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.253134][G train loss: 0.296341]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.255690][G eval loss: 0.338959]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.252407][G train loss: 0.298623]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.254745][G eval loss: 0.340928]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.251649][G train loss: 0.300432]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.253916][G eval loss: 0.343448]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.251111][G train loss: 0.302240]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.253145][G eval loss: 0.346026]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.250692][G train loss: 0.304422]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.252320][G eval loss: 0.346928]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.250228][G train loss: 0.305514]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.251640][G eval loss: 0.346764]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.249903][G train loss: 0.306232]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.251152][G eval loss: 0.347846]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.249699][G train loss: 0.308703]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.250715][G eval loss: 0.351024]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.249495][G train loss: 0.311976]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.250436][G eval loss: 0.354127]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.249399][G train loss: 0.313466]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.250264][G eval loss: 0.357871]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.249365][G train loss: 0.314268]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.250101][G eval loss: 0.360820]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.249310][G train loss: 0.316101]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.249957][G eval loss: 0.362741]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.249254][G train loss: 0.318513]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.249858][G eval loss: 0.364683]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.249226][G train loss: 0.319991]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.249759][G eval loss: 0.366822]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.249182][G train loss: 0.320936]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.249643][G eval loss: 0.368013]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.249126][G train loss: 0.322591]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.249536][G eval loss: 0.369785]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.249079][G train loss: 0.324599]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.249527][G eval loss: 0.372036]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.249119][G train loss: 0.325189]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.249529][G eval loss: 0.371767]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.249170][G train loss: 0.325285]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.249551][G eval loss: 0.371995]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.249216][G train loss: 0.326232]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.249542][G eval loss: 0.372331]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.249218][G train loss: 0.326699]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.249523][G eval loss: 0.370979]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.249202][G train loss: 0.326062]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.249506][G eval loss: 0.369011]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.249183][G train loss: 0.325663]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.249480][G eval loss: 0.367661]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.249148][G train loss: 0.326167]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.249461][G eval loss: 0.366399]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.249114][G train loss: 0.326479]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.249428][G eval loss: 0.365613]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.249056][G train loss: 0.325730]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.249393][G eval loss: 0.365087]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.248997][G train loss: 0.325011]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.249359][G eval loss: 0.365108]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.248943][G train loss: 0.324966]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.249303][G eval loss: 0.365435]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.248868][G train loss: 0.324567]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.249256][G eval loss: 0.364506]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.248816][G train loss: 0.323604]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.249202][G eval loss: 0.363726]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.248764][G train loss: 0.323269]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.249098][G eval loss: 0.364798]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.248659][G train loss: 0.323504]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.249041][G eval loss: 0.364312]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.248603][G train loss: 0.322855]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.248895][G eval loss: 0.364160]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.248452][G train loss: 0.321527]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.248762][G eval loss: 0.363342]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.248322][G train loss: 0.320510]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.248591][G eval loss: 0.362819]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.248140][G train loss: 0.319613]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.248350][G eval loss: 0.362056]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.247872][G train loss: 0.318223]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.248113][G eval loss: 0.360109]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.247609][G train loss: 0.316505]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.247795][G eval loss: 0.359047]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.247252][G train loss: 0.315180]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.247465][G eval loss: 0.357583]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.246915][G train loss: 0.314093]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.247145][G eval loss: 0.355820]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.246621][G train loss: 0.312265]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.246751][G eval loss: 0.355219]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.246249][G train loss: 0.309751]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.246656][G eval loss: 0.352847]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.246272][G train loss: 0.307609]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.246213][G eval loss: 0.355102]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.245859][G train loss: 0.306364]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.246734][G eval loss: 0.349112]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.246533][G train loss: 0.305204]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.245884][G eval loss: 0.352652]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.245593][G train loss: 0.302338]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.246015][G eval loss: 0.346748]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.245848][G train loss: 0.298813]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.245980][G eval loss: 0.343530]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.245899][G train loss: 0.297256]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.244984][G eval loss: 0.346673]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.244707][G train loss: 0.296000]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.245310][G eval loss: 0.339084]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.245262][G train loss: 0.292398]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.245148][G eval loss: 0.335771]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.245195][G train loss: 0.290068]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.244133][G eval loss: 0.336348]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.243964][G train loss: 0.288122]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.244400][G eval loss: 0.330654]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.244415][G train loss: 0.284912]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.244783][G eval loss: 0.326914]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.244952][G train loss: 0.283069]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.243842][G eval loss: 0.326001]\n",
      "[Epoch 111/200][Batch 1/1][D train loss: 0.244031][G train loss: 0.279138]\n",
      "[Epoch 111/200][Batch 1/1][D eval loss: 0.243396][G eval loss: 0.324913]\n",
      "[Epoch 112/200][Batch 1/1][D train loss: 0.243804][G train loss: 0.276090]\n",
      "[Epoch 112/200][Batch 1/1][D eval loss: 0.244268][G eval loss: 0.321314]\n",
      "[Epoch 113/200][Batch 1/1][D train loss: 0.245203][G train loss: 0.273004]\n",
      "[Epoch 113/200][Batch 1/1][D eval loss: 0.243751][G eval loss: 0.320056]\n",
      "[Epoch 114/200][Batch 1/1][D train loss: 0.245041][G train loss: 0.269186]\n",
      "[Epoch 114/200][Batch 1/1][D eval loss: 0.242794][G eval loss: 0.321126]\n",
      "[Epoch 115/200][Batch 1/1][D train loss: 0.244473][G train loss: 0.266802]\n",
      "[Epoch 115/200][Batch 1/1][D eval loss: 0.244423][G eval loss: 0.315282]\n",
      "[Epoch 116/200][Batch 1/1][D train loss: 0.246443][G train loss: 0.262870]\n",
      "[Epoch 116/200][Batch 1/1][D eval loss: 0.244533][G eval loss: 0.312210]\n",
      "[Epoch 117/200][Batch 1/1][D train loss: 0.246764][G train loss: 0.259201]\n",
      "[Epoch 117/200][Batch 1/1][D eval loss: 0.243267][G eval loss: 0.312851]\n",
      "[Epoch 118/200][Batch 1/1][D train loss: 0.246111][G train loss: 0.257202]\n",
      "[Epoch 118/200][Batch 1/1][D eval loss: 0.244989][G eval loss: 0.306144]\n",
      "[Epoch 119/200][Batch 1/1][D train loss: 0.247591][G train loss: 0.253542]\n",
      "[Epoch 119/200][Batch 1/1][D eval loss: 0.245352][G eval loss: 0.303645]\n",
      "[Epoch 120/200][Batch 1/1][D train loss: 0.248020][G train loss: 0.251552]\n",
      "[Epoch 120/200][Batch 1/1][D eval loss: 0.243227][G eval loss: 0.304692]\n",
      "[Epoch 121/200][Batch 1/1][D train loss: 0.246860][G train loss: 0.249428]\n",
      "[Epoch 121/200][Batch 1/1][D eval loss: 0.243816][G eval loss: 0.301786]\n",
      "[Epoch 122/200][Batch 1/1][D train loss: 0.247426][G train loss: 0.246796]\n",
      "[Epoch 122/200][Batch 1/1][D eval loss: 0.244994][G eval loss: 0.301206]\n",
      "[Epoch 123/200][Batch 1/1][D train loss: 0.248266][G train loss: 0.246044]\n",
      "[Epoch 123/200][Batch 1/1][D eval loss: 0.243454][G eval loss: 0.301246]\n",
      "[Epoch 124/200][Batch 1/1][D train loss: 0.247676][G train loss: 0.243823]\n",
      "[Epoch 124/200][Batch 1/1][D eval loss: 0.243355][G eval loss: 0.301231]\n",
      "[Epoch 125/200][Batch 1/1][D train loss: 0.248001][G train loss: 0.242571]\n",
      "[Epoch 125/200][Batch 1/1][D eval loss: 0.246526][G eval loss: 0.299577]\n",
      "[Epoch 126/200][Batch 1/1][D train loss: 0.250153][G train loss: 0.240759]\n",
      "[Epoch 126/200][Batch 1/1][D eval loss: 0.246517][G eval loss: 0.298638]\n",
      "[Epoch 127/200][Batch 1/1][D train loss: 0.250679][G train loss: 0.238411]\n",
      "[Epoch 127/200][Batch 1/1][D eval loss: 0.246638][G eval loss: 0.295222]\n",
      "[Epoch 128/200][Batch 1/1][D train loss: 0.250987][G train loss: 0.238318]\n",
      "[Epoch 128/200][Batch 1/1][D eval loss: 0.250698][G eval loss: 0.291096]\n",
      "[Epoch 129/200][Batch 1/1][D train loss: 0.252664][G train loss: 0.237358]\n",
      "[Epoch 129/200][Batch 1/1][D eval loss: 0.252193][G eval loss: 0.293253]\n",
      "[Epoch 130/200][Batch 1/1][D train loss: 0.253219][G train loss: 0.239229]\n",
      "[Epoch 130/200][Batch 1/1][D eval loss: 0.249579][G eval loss: 0.294648]\n",
      "[Epoch 131/200][Batch 1/1][D train loss: 0.252305][G train loss: 0.241615]\n",
      "[Epoch 131/200][Batch 1/1][D eval loss: 0.250624][G eval loss: 0.295186]\n",
      "[Epoch 132/200][Batch 1/1][D train loss: 0.252492][G train loss: 0.243410]\n",
      "[Epoch 132/200][Batch 1/1][D eval loss: 0.252101][G eval loss: 0.298890]\n",
      "[Epoch 133/200][Batch 1/1][D train loss: 0.252777][G train loss: 0.248066]\n",
      "[Epoch 133/200][Batch 1/1][D eval loss: 0.249659][G eval loss: 0.303461]\n",
      "[Epoch 134/200][Batch 1/1][D train loss: 0.251695][G train loss: 0.250299]\n",
      "[Epoch 134/200][Batch 1/1][D eval loss: 0.248327][G eval loss: 0.309326]\n",
      "[Epoch 135/200][Batch 1/1][D train loss: 0.250893][G train loss: 0.254639]\n",
      "[Epoch 135/200][Batch 1/1][D eval loss: 0.250348][G eval loss: 0.310878]\n",
      "[Epoch 136/200][Batch 1/1][D train loss: 0.251455][G train loss: 0.258508]\n",
      "[Epoch 136/200][Batch 1/1][D eval loss: 0.249129][G eval loss: 0.315335]\n",
      "[Epoch 137/200][Batch 1/1][D train loss: 0.250860][G train loss: 0.261518]\n",
      "[Epoch 137/200][Batch 1/1][D eval loss: 0.247800][G eval loss: 0.322187]\n",
      "[Epoch 138/200][Batch 1/1][D train loss: 0.250163][G train loss: 0.266831]\n",
      "[Epoch 138/200][Batch 1/1][D eval loss: 0.249457][G eval loss: 0.324690]\n",
      "[Epoch 139/200][Batch 1/1][D train loss: 0.250852][G train loss: 0.270313]\n",
      "[Epoch 139/200][Batch 1/1][D eval loss: 0.250224][G eval loss: 0.330376]\n",
      "[Epoch 140/200][Batch 1/1][D train loss: 0.251278][G train loss: 0.275169]\n",
      "[Epoch 140/200][Batch 1/1][D eval loss: 0.248348][G eval loss: 0.337776]\n",
      "[Epoch 141/200][Batch 1/1][D train loss: 0.250602][G train loss: 0.280148]\n",
      "[Epoch 141/200][Batch 1/1][D eval loss: 0.249295][G eval loss: 0.342619]\n",
      "[Epoch 142/200][Batch 1/1][D train loss: 0.251263][G train loss: 0.283694]\n",
      "[Epoch 142/200][Batch 1/1][D eval loss: 0.251000][G eval loss: 0.351358]\n",
      "[Epoch 143/200][Batch 1/1][D train loss: 0.252139][G train loss: 0.289430]\n",
      "[Epoch 143/200][Batch 1/1][D eval loss: 0.249796][G eval loss: 0.356923]\n",
      "[Epoch 144/200][Batch 1/1][D train loss: 0.251816][G train loss: 0.292310]\n",
      "[Epoch 144/200][Batch 1/1][D eval loss: 0.247451][G eval loss: 0.365856]\n",
      "[Epoch 145/200][Batch 1/1][D train loss: 0.250764][G train loss: 0.298749]\n",
      "[Epoch 145/200][Batch 1/1][D eval loss: 0.247998][G eval loss: 0.373375]\n",
      "[Epoch 146/200][Batch 1/1][D train loss: 0.251199][G train loss: 0.301395]\n",
      "[Epoch 146/200][Batch 1/1][D eval loss: 0.250282][G eval loss: 0.388181]\n",
      "[Epoch 147/200][Batch 1/1][D train loss: 0.252354][G train loss: 0.309653]\n",
      "[Epoch 147/200][Batch 1/1][D eval loss: 0.249254][G eval loss: 0.396237]\n",
      "[Epoch 148/200][Batch 1/1][D train loss: 0.252185][G train loss: 0.313631]\n",
      "[Epoch 148/200][Batch 1/1][D eval loss: 0.245569][G eval loss: 0.397551]\n",
      "[Epoch 149/200][Batch 1/1][D train loss: 0.250428][G train loss: 0.316527]\n",
      "[Epoch 149/200][Batch 1/1][D eval loss: 0.245216][G eval loss: 0.405726]\n",
      "[Epoch 150/200][Batch 1/1][D train loss: 0.250430][G train loss: 0.321767]\n",
      "[Epoch 150/200][Batch 1/1][D eval loss: 0.247939][G eval loss: 0.418934]\n",
      "[Epoch 151/200][Batch 1/1][D train loss: 0.252469][G train loss: 0.326274]\n",
      "[Epoch 151/200][Batch 1/1][D eval loss: 0.248502][G eval loss: 0.426326]\n",
      "[Epoch 152/200][Batch 1/1][D train loss: 0.252907][G train loss: 0.331271]\n",
      "[Epoch 152/200][Batch 1/1][D eval loss: 0.247161][G eval loss: 0.425556]\n",
      "[Epoch 153/200][Batch 1/1][D train loss: 0.252240][G train loss: 0.334035]\n",
      "[Epoch 153/200][Batch 1/1][D eval loss: 0.247959][G eval loss: 0.429645]\n",
      "[Epoch 154/200][Batch 1/1][D train loss: 0.252554][G train loss: 0.338430]\n",
      "[Epoch 154/200][Batch 1/1][D eval loss: 0.250074][G eval loss: 0.439858]\n",
      "[Epoch 155/200][Batch 1/1][D train loss: 0.253305][G train loss: 0.345040]\n",
      "[Epoch 155/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 0.441635]\n",
      "[Epoch 156/200][Batch 1/1][D train loss: 0.252648][G train loss: 0.350693]\n",
      "[Epoch 156/200][Batch 1/1][D eval loss: 0.248384][G eval loss: 0.439561]\n",
      "[Epoch 157/200][Batch 1/1][D train loss: 0.250929][G train loss: 0.358046]\n",
      "[Epoch 157/200][Batch 1/1][D eval loss: 0.248246][G eval loss: 0.441378]\n",
      "[Epoch 158/200][Batch 1/1][D train loss: 0.250680][G train loss: 0.361964]\n",
      "[Epoch 158/200][Batch 1/1][D eval loss: 0.249016][G eval loss: 0.450070]\n",
      "[Epoch 159/200][Batch 1/1][D train loss: 0.251134][G train loss: 0.365084]\n",
      "[Epoch 159/200][Batch 1/1][D eval loss: 0.248512][G eval loss: 0.455344]\n",
      "[Epoch 160/200][Batch 1/1][D train loss: 0.250656][G train loss: 0.370924]\n",
      "[Epoch 160/200][Batch 1/1][D eval loss: 0.248022][G eval loss: 0.454908]\n",
      "[Epoch 161/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.376095]\n",
      "[Epoch 161/200][Batch 1/1][D eval loss: 0.248439][G eval loss: 0.462491]\n",
      "[Epoch 162/200][Batch 1/1][D train loss: 0.250123][G train loss: 0.377605]\n",
      "[Epoch 162/200][Batch 1/1][D eval loss: 0.248791][G eval loss: 0.467534]\n",
      "[Epoch 163/200][Batch 1/1][D train loss: 0.250113][G train loss: 0.379378]\n",
      "[Epoch 163/200][Batch 1/1][D eval loss: 0.248520][G eval loss: 0.469349]\n",
      "[Epoch 164/200][Batch 1/1][D train loss: 0.249797][G train loss: 0.383504]\n",
      "[Epoch 164/200][Batch 1/1][D eval loss: 0.248483][G eval loss: 0.478577]\n",
      "[Epoch 165/200][Batch 1/1][D train loss: 0.249747][G train loss: 0.385543]\n",
      "[Epoch 165/200][Batch 1/1][D eval loss: 0.248403][G eval loss: 0.493835]\n",
      "[Epoch 166/200][Batch 1/1][D train loss: 0.249564][G train loss: 0.386241]\n",
      "[Epoch 166/200][Batch 1/1][D eval loss: 0.247345][G eval loss: 0.488992]\n",
      "[Epoch 167/200][Batch 1/1][D train loss: 0.248815][G train loss: 0.385451]\n",
      "[Epoch 167/200][Batch 1/1][D eval loss: 0.246152][G eval loss: 0.483461]\n",
      "[Epoch 168/200][Batch 1/1][D train loss: 0.247842][G train loss: 0.388040]\n",
      "[Epoch 168/200][Batch 1/1][D eval loss: 0.245419][G eval loss: 0.489766]\n",
      "[Epoch 169/200][Batch 1/1][D train loss: 0.247198][G train loss: 0.388805]\n",
      "[Epoch 169/200][Batch 1/1][D eval loss: 0.244697][G eval loss: 0.489378]\n",
      "[Epoch 170/200][Batch 1/1][D train loss: 0.246580][G train loss: 0.388903]\n",
      "[Epoch 170/200][Batch 1/1][D eval loss: 0.243865][G eval loss: 0.476760]\n",
      "[Epoch 171/200][Batch 1/1][D train loss: 0.245815][G train loss: 0.386712]\n",
      "[Epoch 171/200][Batch 1/1][D eval loss: 0.243282][G eval loss: 0.473804]\n",
      "[Epoch 172/200][Batch 1/1][D train loss: 0.245364][G train loss: 0.385513]\n",
      "[Epoch 172/200][Batch 1/1][D eval loss: 0.242743][G eval loss: 0.480288]\n",
      "[Epoch 173/200][Batch 1/1][D train loss: 0.244951][G train loss: 0.386328]\n",
      "[Epoch 173/200][Batch 1/1][D eval loss: 0.241737][G eval loss: 0.470778]\n",
      "[Epoch 174/200][Batch 1/1][D train loss: 0.244145][G train loss: 0.385767]\n",
      "[Epoch 174/200][Batch 1/1][D eval loss: 0.240147][G eval loss: 0.464246]\n",
      "[Epoch 175/200][Batch 1/1][D train loss: 0.242858][G train loss: 0.382936]\n",
      "[Epoch 175/200][Batch 1/1][D eval loss: 0.238244][G eval loss: 0.463757]\n",
      "[Epoch 176/200][Batch 1/1][D train loss: 0.241172][G train loss: 0.381651]\n",
      "[Epoch 176/200][Batch 1/1][D eval loss: 0.236619][G eval loss: 0.456040]\n",
      "[Epoch 177/200][Batch 1/1][D train loss: 0.239178][G train loss: 0.382114]\n",
      "[Epoch 177/200][Batch 1/1][D eval loss: 0.234799][G eval loss: 0.451352]\n",
      "[Epoch 178/200][Batch 1/1][D train loss: 0.237773][G train loss: 0.380063]\n",
      "[Epoch 178/200][Batch 1/1][D eval loss: 0.233122][G eval loss: 0.454930]\n",
      "[Epoch 179/200][Batch 1/1][D train loss: 0.237209][G train loss: 0.375435]\n",
      "[Epoch 179/200][Batch 1/1][D eval loss: 0.232339][G eval loss: 0.459716]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.01 lr_d=0.0005 -> score=0.692055\n",
      "\n",
      "----- 2330: lr_g=0.01, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250013][G eval loss: 0.482841]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250010][G train loss: 0.447081]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.485193]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.449859]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 0.481221]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 0.445839]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.478929]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249972][G train loss: 0.443270]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.479276]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249955][G train loss: 0.443426]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249943][G eval loss: 0.480034]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249927][G train loss: 0.444209]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249911][G eval loss: 0.479914]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249886][G train loss: 0.444164]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249867][G eval loss: 0.478156]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249830][G train loss: 0.442491]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249805][G eval loss: 0.475640]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249753][G train loss: 0.439992]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249727][G eval loss: 0.473922]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249653][G train loss: 0.438330]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249624][G eval loss: 0.473050]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249521][G train loss: 0.437558]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249472][G eval loss: 0.473770]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249328][G train loss: 0.438122]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249306][G eval loss: 0.474057]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249106][G train loss: 0.438439]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249098][G eval loss: 0.474833]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248821][G train loss: 0.439232]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248794][G eval loss: 0.475590]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248412][G train loss: 0.439910]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248379][G eval loss: 0.476580]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247869][G train loss: 0.440821]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247909][G eval loss: 0.477073]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247247][G train loss: 0.441383]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247436][G eval loss: 0.478022]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246585][G train loss: 0.442462]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246755][G eval loss: 0.478717]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245689][G train loss: 0.443175]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245874][G eval loss: 0.479455]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244557][G train loss: 0.443855]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244879][G eval loss: 0.480179]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243254][G train loss: 0.444553]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243794][G eval loss: 0.480986]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241791][G train loss: 0.445382]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.242518][G eval loss: 0.482279]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240051][G train loss: 0.446661]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.240951][G eval loss: 0.483783]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237903][G train loss: 0.448080]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.239076][G eval loss: 0.485486]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235329][G train loss: 0.449694]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.237058][G eval loss: 0.487005]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.232506][G train loss: 0.451184]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.234816][G eval loss: 0.487878]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.229362][G train loss: 0.452093]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.232343][G eval loss: 0.487468]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.225891][G train loss: 0.451807]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.229835][G eval loss: 0.484638]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.222339][G train loss: 0.448889]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.229822][G eval loss: 0.473106]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.221891][G train loss: 0.435589]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.238263][G eval loss: 0.444234]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.230715][G train loss: 0.404547]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.250408][G eval loss: 0.413208]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.240155][G train loss: 0.377136]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.260622][G eval loss: 0.398196]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.250360][G train loss: 0.360309]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.266483][G eval loss: 0.383332]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.257400][G train loss: 0.340157]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.270699][G eval loss: 0.352731]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.260980][G train loss: 0.307802]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.277778][G eval loss: 0.320507]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.264132][G train loss: 0.282812]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.289379][G eval loss: 0.300658]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.273197][G train loss: 0.267211]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.299442][G eval loss: 0.288033]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.282812][G train loss: 0.257355]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.304579][G eval loss: 0.287687]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.287252][G train loss: 0.260089]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.305102][G eval loss: 0.282983]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.287479][G train loss: 0.256136]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.302923][G eval loss: 0.278579]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.285256][G train loss: 0.250095]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.300061][G eval loss: 0.279736]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.282394][G train loss: 0.249036]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.297712][G eval loss: 0.283082]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.280246][G train loss: 0.249659]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.295811][G eval loss: 0.283336]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.279240][G train loss: 0.248334]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.293834][G eval loss: 0.282040]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.278574][G train loss: 0.246856]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.291468][G eval loss: 0.282847]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.277534][G train loss: 0.247405]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.288297][G eval loss: 0.286628]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.275759][G train loss: 0.249927]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.284600][G eval loss: 0.288572]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.273136][G train loss: 0.251169]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.280656][G eval loss: 0.292008]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.270229][G train loss: 0.252565]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.277561][G eval loss: 0.296530]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.267506][G train loss: 0.255720]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.274918][G eval loss: 0.299713]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.265035][G train loss: 0.259222]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.272294][G eval loss: 0.298632]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.262745][G train loss: 0.261438]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.269379][G eval loss: 0.301161]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.260584][G train loss: 0.266287]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.266325][G eval loss: 0.308930]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.258518][G train loss: 0.272708]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.263485][G eval loss: 0.310661]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.256767][G train loss: 0.274170]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.261005][G eval loss: 0.312629]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.255390][G train loss: 0.276320]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.258945][G eval loss: 0.316425]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.254200][G train loss: 0.279923]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.257276][G eval loss: 0.317269]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.253149][G train loss: 0.280597]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.255855][G eval loss: 0.318570]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.252246][G train loss: 0.281035]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.254602][G eval loss: 0.321970]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.01 lr_d=0.0008 -> score=0.576572\n",
      "\n",
      "----- 2330: lr_g=0.01, lr_d=0.0015 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250058][G eval loss: 0.478699]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250056][G train loss: 0.442939]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.484354]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 0.449021]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.483398]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249971][G train loss: 0.448016]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.482368]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249986][G train loss: 0.446709]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 0.482489]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249954][G train loss: 0.446638]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249914][G eval loss: 0.482694]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.01 lr_d=0.0015 -> score=0.732608\n",
      "\n",
      "----- 2330: lr_g=0.01, lr_d=0.002 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.01, lr_d=0.002\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250112][G eval loss: 0.475742]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250110][G train loss: 0.439981]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.483720]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249982][G train loss: 0.448386]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249959][G eval loss: 0.484823]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249943][G train loss: 0.449441]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.483884]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249948][G train loss: 0.448225]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249890][G eval loss: 0.484163]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249840][G train loss: 0.448312]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249723][G eval loss: 0.485115]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249619][G train loss: 0.449290]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249415][G eval loss: 0.484747]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249223][G train loss: 0.448997]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.248943][G eval loss: 0.483206]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.248586][G train loss: 0.447547]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.248046][G eval loss: 0.481210]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.247416][G train loss: 0.445570]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.246741][G eval loss: 0.480235]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.245668][G train loss: 0.444654]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.244545][G eval loss: 0.481053]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.242714][G train loss: 0.445576]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.241142][G eval loss: 0.484716]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.238027][G train loss: 0.449103]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.237017][G eval loss: 0.489755]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.231958][G train loss: 0.454165]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.231969][G eval loss: 0.496984]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.224275][G train loss: 0.461426]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.225385][G eval loss: 0.504902]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.214546][G train loss: 0.469282]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.216005][G eval loss: 0.514691]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.201704][G train loss: 0.478991]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.203360][G eval loss: 0.526782]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.185975][G train loss: 0.491089]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.187149][G eval loss: 0.543776]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.167628][G train loss: 0.508158]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.168520][G eval loss: 0.567519]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.148021][G train loss: 0.532036]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.150029][G eval loss: 0.592903]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.130796][G train loss: 0.557422]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.205575][G eval loss: 0.478078]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.192874][G train loss: 0.435237]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.364114][G eval loss: 0.296709]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.350514][G train loss: 0.261130]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.384236][G eval loss: 0.282825]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.369408][G train loss: 0.247862]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.392487][G eval loss: 0.288264]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.377786][G train loss: 0.253666]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.396674][G eval loss: 0.280995]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.381963][G train loss: 0.246426]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.396436][G eval loss: 0.265202]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.380409][G train loss: 0.230194]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.389831][G eval loss: 0.261063]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.379850][G train loss: 0.225643]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.375025][G eval loss: 0.261441]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.373979][G train loss: 0.225671]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.366404][G eval loss: 0.254819]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.351157][G train loss: 0.248677]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.360484][G eval loss: 0.255067]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.372409][G train loss: 0.219053]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.353998][G eval loss: 0.256810]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.368395][G train loss: 0.221114]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.346926][G eval loss: 0.260019]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.352859][G train loss: 0.224253]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.339388][G eval loss: 0.260841]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.339844][G train loss: 0.224629]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.330884][G eval loss: 0.260243]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.330947][G train loss: 0.223812]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.321533][G eval loss: 0.256683]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.321546][G train loss: 0.220464]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.311703][G eval loss: 0.244261]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.311712][G train loss: 0.209039]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.301694][G eval loss: 0.245210]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.301701][G train loss: 0.204193]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.291727][G eval loss: 0.277854]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.291733][G train loss: 0.225100]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.282347][G eval loss: 0.299997]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.282352][G train loss: 0.244652]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.273976][G eval loss: 0.308244]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.273981][G train loss: 0.254847]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.267171][G eval loss: 0.308976]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.267175][G train loss: 0.259844]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.261471][G eval loss: 0.310598]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.261474][G train loss: 0.266591]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.257066][G eval loss: 0.318655]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.257068][G train loss: 0.279237]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.253956][G eval loss: 0.333496]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.253955][G train loss: 0.297291]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.251890][G eval loss: 0.351403]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.251889][G train loss: 0.317076]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.250641][G eval loss: 0.368401]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.250640][G train loss: 0.334746]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.250060][G eval loss: 0.384451]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.250060][G train loss: 0.350388]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.250081][G eval loss: 0.400842]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.250081][G train loss: 0.365703]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.250527][G eval loss: 0.416170]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.250527][G train loss: 0.379469]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.251202][G eval loss: 0.429014]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.251202][G train loss: 0.390531]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.251971][G eval loss: 0.439066]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.251971][G train loss: 0.399240]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.252712][G eval loss: 0.447084]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.252712][G train loss: 0.406660]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.253221][G eval loss: 0.450375]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.253221][G train loss: 0.410270]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.253519][G eval loss: 0.448973]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.253519][G train loss: 0.410051]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.253605][G eval loss: 0.445549]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.253605][G train loss: 0.408215]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.253486][G eval loss: 0.442000]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.253486][G train loss: 0.406115]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.253180][G eval loss: 0.437200]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.253180][G train loss: 0.402162]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.252712][G eval loss: 0.431139]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.252712][G train loss: 0.396467]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.252146][G eval loss: 0.424979]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.252146][G train loss: 0.390516]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.251550][G eval loss: 0.416367]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.251550][G train loss: 0.382247]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.250985][G eval loss: 0.405161]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.250985][G train loss: 0.371147]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.250512][G eval loss: 0.394632]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.250512][G train loss: 0.360217]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.250178][G eval loss: 0.384378]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.250178][G train loss: 0.349442]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.250015][G eval loss: 0.373469]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.250015][G train loss: 0.338972]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.250029][G eval loss: 0.364294]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.250030][G train loss: 0.331157]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.250201][G eval loss: 0.355893]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.250202][G train loss: 0.323135]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.250481][G eval loss: 0.348698]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.250482][G train loss: 0.314616]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.250807][G eval loss: 0.342848]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.250807][G train loss: 0.307701]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.251082][G eval loss: 0.335836]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.251082][G train loss: 0.301494]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.251286][G eval loss: 0.329383]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.251286][G train loss: 0.296691]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.251392][G eval loss: 0.328564]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.251392][G train loss: 0.296171]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.251397][G eval loss: 0.329610]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.251397][G train loss: 0.296190]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.251311][G eval loss: 0.331306]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.251312][G train loss: 0.296586]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.251148][G eval loss: 0.335414]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.251148][G train loss: 0.299670]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.250919][G eval loss: 0.337093]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.250919][G train loss: 0.301293]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.250637][G eval loss: 0.339379]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.250637][G train loss: 0.303987]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.250360][G eval loss: 0.344288]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.250360][G train loss: 0.309176]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.250135][G eval loss: 0.349316]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.250135][G train loss: 0.314028]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.250011][G eval loss: 0.357237]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250011][G train loss: 0.321087]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.250031][G eval loss: 0.367403]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250031][G train loss: 0.330192]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.250209][G eval loss: 0.374708]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.250209][G train loss: 0.337215]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.250495][G eval loss: 0.380235]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.250495][G train loss: 0.342895]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.250791][G eval loss: 0.385744]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.250791][G train loss: 0.347832]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.251012][G eval loss: 0.388984]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.251012][G train loss: 0.349301]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.251114][G eval loss: 0.391474]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.251114][G train loss: 0.349176]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.251105][G eval loss: 0.393748]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.251105][G train loss: 0.349331]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.250987][G eval loss: 0.391563]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.250987][G train loss: 0.346443]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.250794][G eval loss: 0.386869]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.250794][G train loss: 0.342003]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.250563][G eval loss: 0.381799]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.250563][G train loss: 0.337307]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.250337][G eval loss: 0.374610]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.250337][G train loss: 0.330238]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.250151][G eval loss: 0.366661]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.250151][G train loss: 0.322034]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.250037][G eval loss: 0.360168]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.250038][G train loss: 0.315378]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.250001][G eval loss: 0.354415]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.250001][G train loss: 0.310334]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.250016][G eval loss: 0.347564]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.250016][G train loss: 0.304910]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.250073][G eval loss: 0.341398]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.250073][G train loss: 0.300495]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.250159][G eval loss: 0.336431]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.250159][G train loss: 0.296583]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.250259][G eval loss: 0.332240]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.250259][G train loss: 0.292119]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.250358][G eval loss: 0.329544]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.250358][G train loss: 0.288346]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.250446][G eval loss: 0.328383]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.250446][G train loss: 0.286475]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.250513][G eval loss: 0.327269]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.250513][G train loss: 0.285407]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.250554][G eval loss: 0.325860]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.250554][G train loss: 0.284550]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.250567][G eval loss: 0.325466]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.250567][G train loss: 0.284738]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.250555][G eval loss: 0.325815]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.250555][G train loss: 0.285230]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.250519][G eval loss: 0.326202]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.250519][G train loss: 0.285113]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.250465][G eval loss: 0.327459]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.250465][G train loss: 0.285227]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.250402][G eval loss: 0.329956]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.250402][G train loss: 0.286348]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.250332][G eval loss: 0.332698]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.250332][G train loss: 0.288115]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.250259][G eval loss: 0.335121]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.250259][G train loss: 0.290206]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.250188][G eval loss: 0.337728]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.250188][G train loss: 0.292765]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.250124][G eval loss: 0.340391]\n",
      "[Epoch 111/200][Batch 1/1][D train loss: 0.250124][G train loss: 0.295101]\n",
      "[Epoch 111/200][Batch 1/1][D eval loss: 0.250070][G eval loss: 0.343145]\n",
      "[Epoch 112/200][Batch 1/1][D train loss: 0.250070][G train loss: 0.296785]\n",
      "[Epoch 112/200][Batch 1/1][D eval loss: 0.250031][G eval loss: 0.346570]\n",
      "[Epoch 113/200][Batch 1/1][D train loss: 0.250031][G train loss: 0.298584]\n",
      "[Epoch 113/200][Batch 1/1][D eval loss: 0.250007][G eval loss: 0.350868]\n",
      "[Epoch 114/200][Batch 1/1][D train loss: 0.250007][G train loss: 0.301386]\n",
      "[Epoch 114/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.354792]\n",
      "[Epoch 115/200][Batch 1/1][D train loss: 0.250000][G train loss: 0.304639]\n",
      "[Epoch 115/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.357230]\n",
      "[Epoch 116/200][Batch 1/1][D train loss: 0.250010][G train loss: 0.307384]\n",
      "[Epoch 116/200][Batch 1/1][D eval loss: 0.250035][G eval loss: 0.358650]\n",
      "[Epoch 117/200][Batch 1/1][D train loss: 0.250035][G train loss: 0.309736]\n",
      "[Epoch 117/200][Batch 1/1][D eval loss: 0.250073][G eval loss: 0.359956]\n",
      "[Epoch 118/200][Batch 1/1][D train loss: 0.250073][G train loss: 0.311996]\n",
      "[Epoch 118/200][Batch 1/1][D eval loss: 0.250120][G eval loss: 0.361443]\n",
      "[Epoch 119/200][Batch 1/1][D train loss: 0.250120][G train loss: 0.313999]\n",
      "[Epoch 119/200][Batch 1/1][D eval loss: 0.250174][G eval loss: 0.362950]\n",
      "[Epoch 120/200][Batch 1/1][D train loss: 0.250174][G train loss: 0.315606]\n",
      "[Epoch 120/200][Batch 1/1][D eval loss: 0.250229][G eval loss: 0.364781]\n",
      "[Epoch 121/200][Batch 1/1][D train loss: 0.250229][G train loss: 0.317174]\n",
      "[Epoch 121/200][Batch 1/1][D eval loss: 0.250282][G eval loss: 0.366602]\n",
      "[Epoch 122/200][Batch 1/1][D train loss: 0.250282][G train loss: 0.318707]\n",
      "[Epoch 122/200][Batch 1/1][D eval loss: 0.250328][G eval loss: 0.367419]\n",
      "[Epoch 123/200][Batch 1/1][D train loss: 0.250328][G train loss: 0.319587]\n",
      "[Epoch 123/200][Batch 1/1][D eval loss: 0.250365][G eval loss: 0.366755]\n",
      "[Epoch 124/200][Batch 1/1][D train loss: 0.250365][G train loss: 0.319743]\n",
      "[Epoch 124/200][Batch 1/1][D eval loss: 0.250389][G eval loss: 0.365195]\n",
      "[Epoch 125/200][Batch 1/1][D train loss: 0.250389][G train loss: 0.319760]\n",
      "[Epoch 125/200][Batch 1/1][D eval loss: 0.250398][G eval loss: 0.364025]\n",
      "[Epoch 126/200][Batch 1/1][D train loss: 0.250398][G train loss: 0.319892]\n",
      "[Epoch 126/200][Batch 1/1][D eval loss: 0.250393][G eval loss: 0.363122]\n",
      "[Epoch 127/200][Batch 1/1][D train loss: 0.250392][G train loss: 0.319563]\n",
      "[Epoch 127/200][Batch 1/1][D eval loss: 0.250371][G eval loss: 0.362337]\n",
      "[Epoch 128/200][Batch 1/1][D train loss: 0.250371][G train loss: 0.318624]\n",
      "[Epoch 128/200][Batch 1/1][D eval loss: 0.250336][G eval loss: 0.361699]\n",
      "[Epoch 129/200][Batch 1/1][D train loss: 0.250336][G train loss: 0.317454]\n",
      "[Epoch 129/200][Batch 1/1][D eval loss: 0.250290][G eval loss: 0.360703]\n",
      "[Epoch 130/200][Batch 1/1][D train loss: 0.250289][G train loss: 0.316117]\n",
      "[Epoch 130/200][Batch 1/1][D eval loss: 0.250236][G eval loss: 0.358537]\n",
      "[Epoch 131/200][Batch 1/1][D train loss: 0.250235][G train loss: 0.314177]\n",
      "[Epoch 131/200][Batch 1/1][D eval loss: 0.250178][G eval loss: 0.355346]\n",
      "[Epoch 132/200][Batch 1/1][D train loss: 0.250177][G train loss: 0.311677]\n",
      "[Epoch 132/200][Batch 1/1][D eval loss: 0.250123][G eval loss: 0.352147]\n",
      "[Epoch 133/200][Batch 1/1][D train loss: 0.250122][G train loss: 0.309200]\n",
      "[Epoch 133/200][Batch 1/1][D eval loss: 0.250073][G eval loss: 0.349466]\n",
      "[Epoch 134/200][Batch 1/1][D train loss: 0.250072][G train loss: 0.306718]\n",
      "[Epoch 134/200][Batch 1/1][D eval loss: 0.250035][G eval loss: 0.347036]\n",
      "[Epoch 135/200][Batch 1/1][D train loss: 0.250034][G train loss: 0.303790]\n",
      "[Epoch 135/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.344923]\n",
      "[Epoch 136/200][Batch 1/1][D train loss: 0.250009][G train loss: 0.300564]\n",
      "[Epoch 136/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.343203]\n",
      "[Epoch 137/200][Batch 1/1][D train loss: 0.249999][G train loss: 0.297700]\n",
      "[Epoch 137/200][Batch 1/1][D eval loss: 0.250005][G eval loss: 0.341699]\n",
      "[Epoch 138/200][Batch 1/1][D train loss: 0.250004][G train loss: 0.295062]\n",
      "[Epoch 138/200][Batch 1/1][D eval loss: 0.250022][G eval loss: 0.339225]\n",
      "[Epoch 139/200][Batch 1/1][D train loss: 0.250022][G train loss: 0.292277]\n",
      "[Epoch 139/200][Batch 1/1][D eval loss: 0.250047][G eval loss: 0.336486]\n",
      "[Epoch 140/200][Batch 1/1][D train loss: 0.250046][G train loss: 0.289863]\n",
      "[Epoch 140/200][Batch 1/1][D eval loss: 0.250074][G eval loss: 0.334425]\n",
      "[Epoch 141/200][Batch 1/1][D train loss: 0.250074][G train loss: 0.288258]\n",
      "[Epoch 141/200][Batch 1/1][D eval loss: 0.250101][G eval loss: 0.333112]\n",
      "[Epoch 142/200][Batch 1/1][D train loss: 0.250100][G train loss: 0.286774]\n",
      "[Epoch 142/200][Batch 1/1][D eval loss: 0.250125][G eval loss: 0.332457]\n",
      "[Epoch 143/200][Batch 1/1][D train loss: 0.250124][G train loss: 0.285259]\n",
      "[Epoch 143/200][Batch 1/1][D eval loss: 0.250144][G eval loss: 0.332645]\n",
      "[Epoch 144/200][Batch 1/1][D train loss: 0.250144][G train loss: 0.284121]\n",
      "[Epoch 144/200][Batch 1/1][D eval loss: 0.250159][G eval loss: 0.332993]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.01 lr_d=0.002 -> score=0.583152\n",
      "\n",
      ">>> Best config for 2330: lr_g=0.006, lr_d=0.0015, score=0.552526\n",
      "\n",
      "========== Grid search finished ==========\n",
      "0050: best lr_g=0.006, lr_d=0.002, score=0.687410\n",
      "0056: best lr_g=0.008, lr_d=0.0008, score=0.631023\n",
      "2330: best lr_g=0.006, lr_d=0.0015, score=0.552526\n"
     ]
    }
   ],
   "source": [
    "from LOB_GAN_training_raw import train_lob_gan\n",
    "stock_list = [\"0050\", \"0056\", \"2330\"]\n",
    "\n",
    "lr_g_candidates = [0.002,0.004,0.006,0.008,0.01]\n",
    "lr_d_candidates = [0.0003,0.0005,0.0008,0.0015,0.002]\n",
    "\n",
    "batch_size = 50\n",
    "seed = 307\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "best_config = {}\n",
    "best_score = {}\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"\\n========== Grid search for {stock} ==========\")\n",
    "    best_config[stock] = None\n",
    "    best_score[stock] = float(\"inf\")\n",
    "\n",
    "    for lr_g in lr_g_candidates:\n",
    "        for lr_d in lr_d_candidates:\n",
    "            print(f\"\\n----- {stock}: lr_g={lr_g}, lr_d={lr_d} -----\")\n",
    "            res = train_lob_gan(\n",
    "                stock=stock,\n",
    "                lr_g=lr_g,\n",
    "                lr_d=lr_d,\n",
    "                batch_size=batch_size,\n",
    "                seed=seed,\n",
    "            )\n",
    "\n",
    "            if res is None:\n",
    "                print(f\"[WARN] {stock} lr_g={lr_g} lr_d={lr_d}\")\n",
    "                continue\n",
    "\n",
    "            eval_g = res[\"eval_g_loss\"]\n",
    "            eval_d = res[\"eval_d_loss\"]\n",
    "            if len(eval_g) == 0 or len(eval_d) == 0:\n",
    "                print(f\"[WARN] {stock} lr_g={lr_g} lr_d={lr_d}\")\n",
    "                continue\n",
    "\n",
    "            score = eval_g[-1] + eval_d[-1]\n",
    "            print(f\"[INFO] {stock} lr_g={lr_g} lr_d={lr_d} -> score={score:.6f}\")\n",
    "\n",
    "            all_results[(stock, lr_g, lr_d)] = {\n",
    "                \"res\": res,\n",
    "                \"score\": score,\n",
    "            }\n",
    "\n",
    "            if score < best_score[stock]:\n",
    "                best_score[stock] = score\n",
    "                best_config[stock] = (lr_g, lr_d)\n",
    "\n",
    "    print(f\"\\n>>> Best config for {stock}: \"\n",
    "          f\"lr_g={best_config[stock][0]}, lr_d={best_config[stock][1]}, \"\n",
    "          f\"score={best_score[stock]:.6f}\")\n",
    "\n",
    "print(\"\\n========== Grid search finished ==========\")\n",
    "for stock in stock_list:\n",
    "    if best_config[stock] is not None:\n",
    "        print(f\"{stock}: best lr_g={best_config[stock][0]}, \"\n",
    "              f\"lr_d={best_config[stock][1]}, score={best_score[stock]:.6f}\")\n",
    "    else:\n",
    "        print(f\"{stock}: no valid config found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ddb1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'0050': lr_d     0.0003    0.0005    0.0008    0.0015    0.0020\n",
       " lr_g                                                   \n",
       " 0.002  0.966982  0.914681  1.267156  1.379080  0.861360\n",
       " 0.004  1.001999  0.994673  0.700129  0.907621  1.175688\n",
       " 0.006  1.007936  1.007377  0.991706  0.934845  0.687410\n",
       " 0.008  1.017952  1.019384  1.012849  0.852859  0.915344\n",
       " 0.010  1.011101  1.013047  1.011540  0.991145  1.214332,\n",
       " '0056': lr_d     0.0003    0.0005    0.0008    0.0015    0.0020\n",
       " lr_g                                                   \n",
       " 0.002  0.707686  0.936457  1.215278  1.519361  0.776728\n",
       " 0.004  0.747786  0.708218  0.715508  0.707486  1.325351\n",
       " 0.006  0.653540  0.668898  0.835526  1.096826  1.143911\n",
       " 0.008  0.655762  0.645181  0.631023  0.681556  1.102270\n",
       " 0.010  0.642938  0.719111  0.650251  0.703478  0.682075,\n",
       " '2330': lr_d     0.0003    0.0005    0.0008    0.0015    0.0020\n",
       " lr_g                                                   \n",
       " 0.002  0.727433  0.726079  0.726573  0.726145  0.981615\n",
       " 0.004  0.725745  0.726271  0.723566  0.566019  0.727317\n",
       " 0.006  0.628664  0.723928  0.723097  0.552526  0.722362\n",
       " 0.008  0.724312  0.580317  0.722092  0.573311  0.613717\n",
       " 0.010  0.723641  0.692055  0.576572  0.732608  0.583152}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "with open(\"result.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "pattern = r\"\\[INFO\\]\\s*(\\d+)\\s+lr_g=([0-9.]+)\\s+lr_d=([0-9.]+)\\s*->\\s*score=([0-9.]+)\"\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "df = pd.DataFrame(matches, columns=[\"stock\", \"lr_g\", \"lr_d\", \"score\"])\n",
    "df[\"lr_g\"] = df[\"lr_g\"].astype(float)\n",
    "df[\"lr_d\"] = df[\"lr_d\"].astype(float)\n",
    "df[\"score\"] = df[\"score\"].astype(float)\n",
    "\n",
    "\n",
    "stocks = [\"0050\", \"0056\", \"2330\"]\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "tables = {}\n",
    "\n",
    "for i, s in enumerate(stocks, 1):\n",
    "    sub = df[df[\"stock\"] == s]\n",
    "    pivot = sub.pivot(index=\"lr_g\", columns=\"lr_d\", values=\"score\")\n",
    "    tables[s] = pivot\n",
    "    \n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"viridis\")\n",
    "    plt.title(f\"{s} Score Heatmap\")\n",
    "    plt.xlabel(\"lr_d\")\n",
    "    plt.ylabel(\"lr_g\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "tables  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c86f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50415595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training ADJUSTED GAN for 0050 =====\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.297082]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.118483]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.270163]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.108594]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.260239]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 1.108031]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.259625]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 1.107663]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.259283]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 1.106205]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249957][G train loss: 1.257858]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249945][G eval loss: 1.105429]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249931][G train loss: 1.257097]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 1.105595]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249897][G train loss: 1.257266]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249881][G eval loss: 1.105764]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249853][G train loss: 1.257418]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 1.105285]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249790][G train loss: 1.256906]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249755][G eval loss: 1.104132]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249703][G train loss: 1.255709]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249655][G eval loss: 1.102789]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249586][G train loss: 1.254317]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249519][G eval loss: 1.101652]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249425][G train loss: 1.253123]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249342][G eval loss: 1.100688]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249220][G train loss: 1.252100]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249104][G eval loss: 1.100070]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248947][G train loss: 1.251417]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248789][G eval loss: 1.099955]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248589][G train loss: 1.251228]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248377][G eval loss: 1.100421]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248125][G train loss: 1.251626]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247867][G eval loss: 1.100872]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247550][G train loss: 1.252012]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247307][G eval loss: 1.100510]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246920][G train loss: 1.251514]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246465][G eval loss: 1.098973]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245998][G train loss: 1.249774]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245433][G eval loss: 1.096361]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244867][G train loss: 1.246884]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244225][G eval loss: 1.092349]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243558][G train loss: 1.242497]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242773][G eval loss: 1.086392]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242006][G train loss: 1.236056]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241052][G eval loss: 1.077646]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240170][G train loss: 1.226677]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239045][G eval loss: 1.065128]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238014][G train loss: 1.213254]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236814][G eval loss: 1.047958]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235562][G train loss: 1.194925]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234458][G eval loss: 1.025588]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.232879][G train loss: 1.171072]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231960][G eval loss: 0.996933]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.230013][G train loss: 1.140806]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.229413][G eval loss: 0.961122]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.226988][G train loss: 1.103639]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.227096][G eval loss: 0.918504]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.224001][G train loss: 1.059771]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.225587][G eval loss: 0.871323]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.221428][G train loss: 1.010458]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.226078][G eval loss: 0.821702]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.219976][G train loss: 0.957120]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.229495][G eval loss: 0.771464]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.220964][G train loss: 0.899839]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.235891][G eval loss: 0.727185]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.225621][G train loss: 0.842304]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.244287][G eval loss: 0.696898]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.234947][G train loss: 0.791124]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.256125][G eval loss: 0.679493]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248008][G train loss: 0.753258]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.269728][G eval loss: 0.672261]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.261403][G train loss: 0.730609]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.282324][G eval loss: 0.669270]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.273122][G train loss: 0.715017]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.292606][G eval loss: 0.660643]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.282797][G train loss: 0.697548]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.300592][G eval loss: 0.637728]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.290857][G train loss: 0.674207]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.306993][G eval loss: 0.604636]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.297357][G train loss: 0.643970]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.311947][G eval loss: 0.567230]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.302066][G train loss: 0.612784]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.315593][G eval loss: 0.531245]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.305331][G train loss: 0.587449]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.317739][G eval loss: 0.506741]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.307374][G train loss: 0.575154]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.318056][G eval loss: 0.495672]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.308173][G train loss: 0.572496]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.316369][G eval loss: 0.490602]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.307499][G train loss: 0.574058]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.313640][G eval loss: 0.488337]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.305870][G train loss: 0.573497]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.310144][G eval loss: 0.487382]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.303171][G train loss: 0.569973]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.306285][G eval loss: 0.486348]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.299788][G train loss: 0.562555]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.302282][G eval loss: 0.482679]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.296549][G train loss: 0.554732]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.297764][G eval loss: 0.481197]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.293635][G train loss: 0.549131]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.292555][G eval loss: 0.483055]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.290931][G train loss: 0.547108]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.287708][G eval loss: 0.483770]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.288009][G train loss: 0.545790]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.283412][G eval loss: 0.480921]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.284894][G train loss: 0.545219]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.279523][G eval loss: 0.479344]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.281514][G train loss: 0.545060]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.275876][G eval loss: 0.482349]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.277995][G train loss: 0.546494]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.273022][G eval loss: 0.486523]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve_adjusted.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1_adjusted.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1_adjusted.pth\n",
      "Done training ADJUSTED LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "===== Training ADJUSTED GAN for 0056 =====\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.115726]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.981110]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.083683]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.975169]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.078262]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.971721]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.075157]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.971442]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.074874]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.973303]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.076593]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.975201]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.078314]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.974955]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249941][G train loss: 1.077922]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 0.973092]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249920][G train loss: 1.075968]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.970842]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249896][G train loss: 1.073674]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249896][G eval loss: 0.969009]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249866][G train loss: 1.071847]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249865][G eval loss: 0.967904]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249827][G train loss: 1.070757]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249824][G eval loss: 0.967371]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249777][G train loss: 1.070202]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249771][G eval loss: 0.967650]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249713][G train loss: 1.070452]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249708][G eval loss: 0.967735]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249637][G train loss: 1.070535]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249633][G eval loss: 0.967628]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249548][G train loss: 1.070414]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249548][G eval loss: 0.967049]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249447][G train loss: 1.069774]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249454][G eval loss: 0.965682]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249334][G train loss: 1.068256]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249335][G eval loss: 0.963174]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249191][G train loss: 1.065523]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249195][G eval loss: 0.959323]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249021][G train loss: 1.061409]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249039][G eval loss: 0.953069]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248826][G train loss: 1.054853]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248864][G eval loss: 0.942309]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248603][G train loss: 1.043792]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248665][G eval loss: 0.924487]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248347][G train loss: 1.025701]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248449][G eval loss: 0.898480]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248068][G train loss: 0.999306]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248232][G eval loss: 0.862520]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247769][G train loss: 0.962614]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.248043][G eval loss: 0.814798]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247461][G train loss: 0.913970]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247913][G eval loss: 0.757776]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.247171][G train loss: 0.856289]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.247927][G eval loss: 0.695337]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.247014][G train loss: 0.792650]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.248227][G eval loss: 0.634249]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.247069][G train loss: 0.727884]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.248680][G eval loss: 0.593514]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.247286][G train loss: 0.678918]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.248966][G eval loss: 0.566917]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.247241][G train loss: 0.647101]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.249388][G eval loss: 0.567214]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.247254][G train loss: 0.643039]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.250405][G eval loss: 0.583749]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve_adjusted.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1_adjusted.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1_adjusted.pth\n",
      "Done training ADJUSTED LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "===== Training ADJUSTED GAN for 2330 =====\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.483047]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250029][G eval loss: 0.487091]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250027][G train loss: 0.451488]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.480732]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 0.445105]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.481630]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 0.446171]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.482074]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249951][G train loss: 0.446774]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249937][G eval loss: 0.481850]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249911][G train loss: 0.446582]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249880][G eval loss: 0.481423]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249837][G train loss: 0.446085]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249786][G eval loss: 0.480876]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249717][G train loss: 0.445414]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249639][G eval loss: 0.479943]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249533][G train loss: 0.444349]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249412][G eval loss: 0.478640]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249251][G train loss: 0.442943]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249114][G eval loss: 0.477040]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248870][G train loss: 0.441309]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248667][G eval loss: 0.475671]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248288][G train loss: 0.439971]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248019][G eval loss: 0.475138]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247463][G train loss: 0.439507]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247203][G eval loss: 0.475680]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246409][G train loss: 0.440118]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246175][G eval loss: 0.477194]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245053][G train loss: 0.441689]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244656][G eval loss: 0.478804]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243103][G train loss: 0.443308]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242547][G eval loss: 0.481147]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.240436][G train loss: 0.445610]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.239972][G eval loss: 0.483916]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.237150][G train loss: 0.448309]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236854][G eval loss: 0.486808]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.233199][G train loss: 0.451134]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232927][G eval loss: 0.489856]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.228380][G train loss: 0.454123]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227903][G eval loss: 0.493456]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.222427][G train loss: 0.457706]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221836][G eval loss: 0.496956]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.215438][G train loss: 0.461226]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.214310][G eval loss: 0.501308]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.207060][G train loss: 0.465615]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.205278][G eval loss: 0.506674]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.197270][G train loss: 0.471019]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.194673][G eval loss: 0.513109]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.186036][G train loss: 0.477491]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.183075][G eval loss: 0.519711]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.173645][G train loss: 0.484091]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.169927][G eval loss: 0.527421]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.160346][G train loss: 0.491730]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.156479][G eval loss: 0.535092]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.148102][G train loss: 0.499277]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.146365][G eval loss: 0.537620]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.140044][G train loss: 0.501249]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.174167][G eval loss: 0.470898]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.176105][G train loss: 0.420644]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.278184][G eval loss: 0.339345]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.280184][G train loss: 0.298060]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.333179][G eval loss: 0.313111]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.331183][G train loss: 0.276286]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.354454][G eval loss: 0.305463]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.351848][G train loss: 0.269125]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.364382][G eval loss: 0.291800]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.361718][G train loss: 0.255929]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.369828][G eval loss: 0.279181]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.367180][G train loss: 0.243518]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.372797][G eval loss: 0.272534]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.370493][G train loss: 0.236498]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.374096][G eval loss: 0.273423]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.372152][G train loss: 0.235872]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.374259][G eval loss: 0.274195]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.372738][G train loss: 0.234361]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.373494][G eval loss: 0.269871]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.374195][G train loss: 0.228242]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.372077][G eval loss: 0.260535]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.374891][G train loss: 0.217592]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.369578][G eval loss: 0.246985]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.361107][G train loss: 0.224838]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.336999][G eval loss: 0.275679]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.357734][G train loss: 0.212998]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.294883][G eval loss: 0.312974]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.349201][G train loss: 0.209239]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.285423][G eval loss: 0.330685]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.339945][G train loss: 0.209276]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.282549][G eval loss: 0.329925]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.337845][G train loss: 0.203990]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.282534][G eval loss: 0.319067]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.335530][G train loss: 0.200990]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.297440][G eval loss: 0.286931]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.334702][G train loss: 0.197156]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.311118][G eval loss: 0.265120]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.330575][G train loss: 0.196825]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.312768][G eval loss: 0.261228]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.325478][G train loss: 0.196490]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.307963][G eval loss: 0.263618]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.320113][G train loss: 0.195567]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.302558][G eval loss: 0.265475]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.320053][G train loss: 0.183999]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.296819][G eval loss: 0.268387]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.319047][G train loss: 0.175191]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.312717][G eval loss: 0.223109]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.312479][G train loss: 0.177992]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.306072][G eval loss: 0.228377]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.305841][G train loss: 0.181488]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.299461][G eval loss: 0.234192]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.299256][G train loss: 0.186041]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.293216][G eval loss: 0.239636]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.293052][G train loss: 0.191647]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.287125][G eval loss: 0.246160]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.286846][G train loss: 0.199007]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.281203][G eval loss: 0.254292]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.279922][G train loss: 0.209214]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.275821][G eval loss: 0.263167]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.273196][G train loss: 0.222341]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.271173][G eval loss: 0.273213]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.270946][G train loss: 0.227728]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.266868][G eval loss: 0.283512]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.267477][G train loss: 0.237015]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.262969][G eval loss: 0.294178]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.263701][G train loss: 0.247041]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.259498][G eval loss: 0.304332]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.260296][G train loss: 0.257235]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.256393][G eval loss: 0.314480]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.257332][G train loss: 0.267579]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.253794][G eval loss: 0.324775]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.254859][G train loss: 0.278338]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.251770][G eval loss: 0.335173]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.252924][G train loss: 0.289779]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.250191][G eval loss: 0.346521]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.251463][G train loss: 0.302270]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.249202][G eval loss: 0.358158]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.250534][G train loss: 0.315020]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.248638][G eval loss: 0.369986]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.250031][G train loss: 0.327598]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.248504][G eval loss: 0.381236]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.249872][G train loss: 0.339190]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.248631][G eval loss: 0.392251]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.349718]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.249020][G eval loss: 0.403021]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.250332][G train loss: 0.359424]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.249584][G eval loss: 0.413446]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.250874][G train loss: 0.368663]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.250261][G eval loss: 0.423370]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.251523][G train loss: 0.377227]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.251047][G eval loss: 0.432221]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.252188][G train loss: 0.385339]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.251679][G eval loss: 0.439089]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.252619][G train loss: 0.391280]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.252276][G eval loss: 0.444025]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.254450][G train loss: 0.395637]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.252831][G eval loss: 0.447792]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.254576][G train loss: 0.398927]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.253398][G eval loss: 0.450738]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.254047][G train loss: 0.401993]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.253809][G eval loss: 0.452614]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.254591][G train loss: 0.404321]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.254116][G eval loss: 0.453657]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.254658][G train loss: 0.406598]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.254281][G eval loss: 0.454368]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.254500][G train loss: 0.408377]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.254327][G eval loss: 0.454439]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.254560][G train loss: 0.409201]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.254363][G eval loss: 0.454834]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.254984][G train loss: 0.409072]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.254386][G eval loss: 0.455230]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.254649][G train loss: 0.408319]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.254307][G eval loss: 0.455454]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.254822][G train loss: 0.407495]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.254102][G eval loss: 0.453235]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.254510][G train loss: 0.405969]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.253786][G eval loss: 0.450262]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.254149][G train loss: 0.403212]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.253422][G eval loss: 0.446333]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.253757][G train loss: 0.399522]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.253010][G eval loss: 0.441754]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.253347][G train loss: 0.395328]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.252535][G eval loss: 0.436799]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.252923][G train loss: 0.390669]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.252049][G eval loss: 0.431578]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.252481][G train loss: 0.385711]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.251561][G eval loss: 0.426304]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.252045][G train loss: 0.380426]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.251045][G eval loss: 0.421286]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.251624][G train loss: 0.375078]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.250507][G eval loss: 0.416770]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.251245][G train loss: 0.369810]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.250032][G eval loss: 0.412074]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.250899][G train loss: 0.364380]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.249635][G eval loss: 0.408105]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.250625][G train loss: 0.359509]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.249328][G eval loss: 0.404509]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.250417][G train loss: 0.355192]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.249035][G eval loss: 0.400915]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.250243][G train loss: 0.350953]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.248758][G eval loss: 0.397346]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.250090][G train loss: 0.346849]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.248248][G eval loss: 0.394586]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.249963][G train loss: 0.342870]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.246835][G eval loss: 0.394471]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.249863][G train loss: 0.338888]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.237714][G eval loss: 0.430067]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.249792][G train loss: 0.334729]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.235778][G eval loss: 0.431439]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve_adjusted.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1_adjusted.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1_adjusted.pth\n",
      "Done training ADJUSTED LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n"
     ]
    }
   ],
   "source": [
    "from LOB_GAN_training_adjusted import train_lob_gan\n",
    "\n",
    "stock_list = [\"0050\", \"0056\", \"2330\"]\n",
    "\n",
    "lr_config = {\n",
    "    \"0050\": {\"lr_g\": 0.004, \"lr_d\": 0.0008},\n",
    "    \"0056\": {\"lr_g\": 0.006, \"lr_d\": 0.0005},\n",
    "    \"2330\": {\"lr_g\": 0.0045, \"lr_d\": 0.0012},\n",
    "}\n",
    "\n",
    "\n",
    "batch_config = {\n",
    "    \"0050\": 50,\n",
    "    \"0056\": 50,\n",
    "    \"2330\": 50,\n",
    "}\n",
    "\n",
    "results_adjusted = {}\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"===== Training ADJUSTED GAN for {stock} =====\")\n",
    "    cfg = lr_config[stock]\n",
    "    bs = batch_config[stock]\n",
    "    res = train_lob_gan(\n",
    "        stock=stock,\n",
    "        lr_g=cfg[\"lr_g\"],\n",
    "        lr_d=cfg[\"lr_d\"],\n",
    "        batch_size=bs,\n",
    "        seed=307,\n",
    "    )\n",
    "    results_adjusted[stock] = res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
