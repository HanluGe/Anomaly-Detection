{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230eab1a",
   "metadata": {},
   "source": [
    "## HW4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c01e0e",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd34ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training GAN for 0050 =====\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.117348]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.269022]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.107848]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.259430]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.107934]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.259470]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 1.107752]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.259352]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.106480]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.258148]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 1.105823]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249934][G train loss: 1.257526]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 1.105642]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249900][G train loss: 1.257354]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 1.105422]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249856][G train loss: 1.257124]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249829][G eval loss: 1.104922]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249791][G train loss: 1.256597]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249755][G eval loss: 1.103952]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249704][G train loss: 1.255591]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249656][G eval loss: 1.102809]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249587][G train loss: 1.254401]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249521][G eval loss: 1.101852]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249428][G train loss: 1.253398]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249352][G eval loss: 1.101090]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249232][G train loss: 1.252594]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249129][G eval loss: 1.100708]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248976][G train loss: 1.252176]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248830][G eval loss: 1.100744]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248635][G train loss: 1.252164]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248435][G eval loss: 1.101221]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248189][G train loss: 1.252581]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247928][G eval loss: 1.102093]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247619][G train loss: 1.253393]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247369][G eval loss: 1.102612]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246991][G train loss: 1.253839]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246538][G eval loss: 1.101883]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246085][G train loss: 1.253011]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245511][G eval loss: 1.100232]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244968][G train loss: 1.251205]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244334][G eval loss: 1.097981]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243709][G train loss: 1.248718]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242917][G eval loss: 1.095047]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242209][G train loss: 1.245433]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241217][G eval loss: 1.090575]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240423][G train loss: 1.240480]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239243][G eval loss: 1.083274]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238351][G train loss: 1.232568]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236957][G eval loss: 1.072087]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235960][G train loss: 1.220541]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234424][G eval loss: 1.055990]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.233300][G train loss: 1.203394]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231631][G eval loss: 1.034966]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.230363][G train loss: 1.180851]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.228569][G eval loss: 1.008484]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.227142][G train loss: 1.152276]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.225200][G eval loss: 0.974675]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.223656][G train loss: 1.116490]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.221947][G eval loss: 0.932603]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.220142][G train loss: 1.073285]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.219502][G eval loss: 0.883689]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.217126][G train loss: 1.024095]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.218658][G eval loss: 0.832171]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.215622][G train loss: 0.971668]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.221429][G eval loss: 0.781474]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.217015][G train loss: 0.918553]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.228506][G eval loss: 0.735036]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.222167][G train loss: 0.866600]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.238255][G eval loss: 0.696277]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.230798][G train loss: 0.817407]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250014][G eval loss: 0.665469]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.242000][G train loss: 0.774323]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.263483][G eval loss: 0.639508]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.254340][G train loss: 0.738828]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.277342][G eval loss: 0.618302]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.267042][G train loss: 0.709377]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.290981][G eval loss: 0.597570]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.279433][G train loss: 0.682120]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.302760][G eval loss: 0.575034]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.290966][G train loss: 0.655376]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.312883][G eval loss: 0.549432]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.301308][G train loss: 0.628705]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.321247][G eval loss: 0.525353]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.309518][G train loss: 0.603509]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.327861][G eval loss: 0.505094]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.315519][G train loss: 0.582509]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.332504][G eval loss: 0.488767]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.319466][G train loss: 0.565839]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.334148][G eval loss: 0.477095]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.321850][G train loss: 0.553709]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.334225][G eval loss: 0.468145]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.323206][G train loss: 0.544595]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.333293][G eval loss: 0.461388]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.323749][G train loss: 0.537106]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.331393][G eval loss: 0.457375]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.322852][G train loss: 0.532372]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.328569][G eval loss: 0.455950]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.320276][G train loss: 0.530057]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.325559][G eval loss: 0.455366]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.317726][G train loss: 0.527674]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.322156][G eval loss: 0.454873]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.315422][G train loss: 0.524987]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.318218][G eval loss: 0.453617]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.312949][G train loss: 0.521860]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.314182][G eval loss: 0.450924]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.309992][G train loss: 0.517351]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.309999][G eval loss: 0.446332]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.307085][G train loss: 0.510987]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.304943][G eval loss: 0.443888]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.303634][G train loss: 0.506479]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.299970][G eval loss: 0.441739]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299643][G train loss: 0.501919]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.295039][G eval loss: 0.441201]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.295083][G train loss: 0.499082]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.290655][G eval loss: 0.443324]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.290636][G train loss: 0.499080]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.286974][G eval loss: 0.443465]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.286837][G train loss: 0.498879]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.283545][G eval loss: 0.444566]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.283169][G train loss: 0.500298]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.279999][G eval loss: 0.445908]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.279103][G train loss: 0.502009]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.276858][G eval loss: 0.445695]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.275081][G train loss: 0.502841]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.274115][G eval loss: 0.446005]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.271605][G train loss: 0.504608]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.271596][G eval loss: 0.446998]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.269004][G train loss: 0.506768]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.269076][G eval loss: 0.448190]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.266708][G train loss: 0.508424]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.266521][G eval loss: 0.449291]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.264536][G train loss: 0.509336]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.263590][G eval loss: 0.449091]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.262477][G train loss: 0.509300]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.260577][G eval loss: 0.447475]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.260595][G train loss: 0.508215]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.257835][G eval loss: 0.446247]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.258982][G train loss: 0.507135]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.256090][G eval loss: 0.446100]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.257574][G train loss: 0.507109]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.254893][G eval loss: 0.445716]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.256320][G train loss: 0.507197]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.253912][G eval loss: 0.445059]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.255211][G train loss: 0.506519]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.253114][G eval loss: 0.445002]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.254254][G train loss: 0.505375]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.252473][G eval loss: 0.446369]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.253438][G train loss: 0.504807]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.251914][G eval loss: 0.448215]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "===== Training GAN for 0056 =====\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.980660]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.083180]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.975456]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.078601]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.971864]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.075342]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.971814]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.075296]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.973313]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.076672]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.974509]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.077710]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.974701]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249940][G train loss: 1.077758]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249938][G eval loss: 0.973502]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076436]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.971456]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.074301]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 0.969577]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249865][G train loss: 1.072361]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249866][G eval loss: 0.968497]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249829][G train loss: 1.071252]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.967944]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249782][G train loss: 1.070699]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249777][G eval loss: 0.967536]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249720][G train loss: 1.070304]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249711][G eval loss: 0.967422]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249641][G train loss: 1.070208]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249632][G eval loss: 0.967517]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249548][G train loss: 1.070334]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249541][G eval loss: 0.967339]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249442][G train loss: 1.070200]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249440][G eval loss: 0.966562]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249326][G train loss: 1.069419]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249326][G eval loss: 0.964654]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249200][G train loss: 1.067420]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249204][G eval loss: 0.960490]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249075][G train loss: 1.063098]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249069][G eval loss: 0.951554]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248952][G train loss: 1.053839]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248940][G eval loss: 0.934691]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248867][G train loss: 1.036220]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248901][G eval loss: 0.913240]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248916][G train loss: 1.013468]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249012][G eval loss: 0.900804]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249111][G train loss: 0.999367]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249110][G eval loss: 0.904692]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249231][G train loss: 1.001020]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.248946][G eval loss: 0.900952]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249142][G train loss: 0.995242]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.248488][G eval loss: 0.882598]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.248896][G train loss: 0.974309]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.248012][G eval loss: 0.858508]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.248526][G train loss: 0.948989]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.247573][G eval loss: 0.838108]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.248162][G train loss: 0.929007]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.247278][G eval loss: 0.816444]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.247924][G train loss: 0.907744]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.247154][G eval loss: 0.786735]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.247891][G train loss: 0.877355]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.247397][G eval loss: 0.748519]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.248266][G train loss: 0.840046]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.248070][G eval loss: 0.711532]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.248912][G train loss: 0.800759]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.248689][G eval loss: 0.675359]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249272][G train loss: 0.762817]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.248681][G eval loss: 0.634926]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.249293][G train loss: 0.719836]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.248150][G eval loss: 0.581364]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248780][G train loss: 0.667915]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.247107][G eval loss: 0.530172]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.247631][G train loss: 0.615378]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.245213][G eval loss: 0.511087]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.245343][G train loss: 0.594116]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.243898][G eval loss: 0.535064]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.242972][G train loss: 0.625966]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.244054][G eval loss: 0.578054]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.242212][G train loss: 0.677520]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.245452][G eval loss: 0.617088]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.242844][G train loss: 0.704916]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.248241][G eval loss: 0.614690]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.244810][G train loss: 0.695443]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.252632][G eval loss: 0.582589]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.248001][G train loss: 0.667134]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.256386][G eval loss: 0.564990]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.251460][G train loss: 0.640003]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.258983][G eval loss: 0.566527]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.253893][G train loss: 0.634402]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.260693][G eval loss: 0.564644]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.255172][G train loss: 0.634009]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.261423][G eval loss: 0.554116]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.255643][G train loss: 0.625716]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.261239][G eval loss: 0.538302]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.255424][G train loss: 0.611935]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.259804][G eval loss: 0.523859]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.254530][G train loss: 0.600399]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.257379][G eval loss: 0.516479]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.253272][G train loss: 0.594538]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.256147][G eval loss: 0.513956]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.251923][G train loss: 0.592473]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.255167][G eval loss: 0.514866]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.251343][G train loss: 0.590985]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.254492][G eval loss: 0.514660]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.250978][G train loss: 0.589785]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.254063][G eval loss: 0.509897]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.250745][G train loss: 0.585879]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.253776][G eval loss: 0.501001]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.250685][G train loss: 0.577415]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.253507][G eval loss: 0.489221]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.250692][G train loss: 0.566382]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.253193][G eval loss: 0.481979]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.250647][G train loss: 0.558270]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.252827][G eval loss: 0.481994]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.250517][G train loss: 0.557839]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.252594][G eval loss: 0.485138]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.250374][G train loss: 0.562464]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.252443][G eval loss: 0.487020]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.250150][G train loss: 0.566145]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.252304][G eval loss: 0.484233]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.249931][G train loss: 0.564961]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.252153][G eval loss: 0.478397]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.249737][G train loss: 0.558417]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.252086][G eval loss: 0.470607]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.249556][G train loss: 0.550406]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.252051][G eval loss: 0.463682]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.249408][G train loss: 0.543487]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.252041][G eval loss: 0.455410]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.249252][G train loss: 0.535301]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.251970][G eval loss: 0.445360]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.249016][G train loss: 0.524311]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.251701][G eval loss: 0.437013]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.248667][G train loss: 0.514887]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.251231][G eval loss: 0.435543]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.248220][G train loss: 0.512076]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.250745][G eval loss: 0.435805]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.247681][G train loss: 0.511667]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.250193][G eval loss: 0.433646]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.247054][G train loss: 0.508309]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.249591][G eval loss: 0.428734]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.246418][G train loss: 0.501576]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.248997][G eval loss: 0.424169]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.245869][G train loss: 0.494497]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.248459][G eval loss: 0.422996]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.245302][G train loss: 0.490862]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.247896][G eval loss: 0.423845]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.244702][G train loss: 0.489711]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.247386][G eval loss: 0.423571]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.244060][G train loss: 0.488092]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.246866][G eval loss: 0.421075]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.243385][G train loss: 0.484489]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.246165][G eval loss: 0.420909]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.242692][G train loss: 0.480754]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.245302][G eval loss: 0.422815]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.241801][G train loss: 0.479334]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.244514][G eval loss: 0.424385]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "===== Training GAN for 2330 =====\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 0.486292]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250027][G train loss: 0.450600]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.479923]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 0.444362]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.481482]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.446120]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 0.482475]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249953][G train loss: 0.447205]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249937][G eval loss: 0.481927]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249912][G train loss: 0.446598]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249876][G eval loss: 0.481018]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249835][G train loss: 0.445568]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249785][G eval loss: 0.480284]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249723][G train loss: 0.444704]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249646][G eval loss: 0.480220]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249550][G train loss: 0.444537]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249458][G eval loss: 0.479573]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249315][G train loss: 0.443844]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249193][G eval loss: 0.478328]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248978][G train loss: 0.442610]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248807][G eval loss: 0.477089]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248471][G train loss: 0.441428]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248228][G eval loss: 0.476347]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247733][G train loss: 0.440754]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247507][G eval loss: 0.476152]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246800][G train loss: 0.440618]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246524][G eval loss: 0.475808]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245552][G train loss: 0.440313]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.245153][G eval loss: 0.476429]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243831][G train loss: 0.440921]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.243316][G eval loss: 0.478305]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241534][G train loss: 0.442749]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240931][G eval loss: 0.480987]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238556][G train loss: 0.445365]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.238057][G eval loss: 0.483880]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.234955][G train loss: 0.448192]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.234699][G eval loss: 0.486707]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230704][G train loss: 0.450998]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.230272][G eval loss: 0.490388]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.225316][G train loss: 0.454681]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.224907][G eval loss: 0.494166]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.218991][G train loss: 0.458498]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.218360][G eval loss: 0.498574]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.211545][G train loss: 0.462974]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.210325][G eval loss: 0.503805]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.202705][G train loss: 0.468228]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.200473][G eval loss: 0.510305]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.192220][G train loss: 0.474723]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.189452][G eval loss: 0.517455]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.180458][G train loss: 0.481873]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.177219][G eval loss: 0.525273]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.167460][G train loss: 0.489663]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.163555][G eval loss: 0.534419]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.154040][G train loss: 0.498758]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.148863][G eval loss: 0.545109]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.141038][G train loss: 0.509380]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.135718][G eval loss: 0.554413]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.130157][G train loss: 0.518363]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.130735][G eval loss: 0.549266]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.127296][G train loss: 0.511469]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.224962][G eval loss: 0.385034]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.233680][G train loss: 0.332835]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.323702][G eval loss: 0.307164]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.323383][G train loss: 0.270017]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.353376][G eval loss: 0.303166]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.351374][G train loss: 0.266807]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.365669][G eval loss: 0.294487]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.363509][G train loss: 0.258097]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.372865][G eval loss: 0.283133]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.370912][G train loss: 0.246987]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.377203][G eval loss: 0.273598]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.375619][G train loss: 0.237019]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.379643][G eval loss: 0.266727]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.378488][G train loss: 0.228800]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.380693][G eval loss: 0.262613]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.381451][G train loss: 0.222563]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.380809][G eval loss: 0.259467]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.383567][G train loss: 0.217323]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.379986][G eval loss: 0.253299]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.381912][G train loss: 0.211141]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.378717][G eval loss: 0.252664]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.382479][G train loss: 0.207974]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.376705][G eval loss: 0.249009]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.380605][G train loss: 0.202747]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.374023][G eval loss: 0.242573]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.377737][G train loss: 0.195526]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.370691][G eval loss: 0.234509]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.373895][G train loss: 0.187445]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.366952][G eval loss: 0.226386]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.368625][G train loss: 0.179756]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.362875][G eval loss: 0.218714]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.362781][G train loss: 0.173599]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.358369][G eval loss: 0.212533]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.358320][G train loss: 0.169914]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.353433][G eval loss: 0.208596]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.353138][G train loss: 0.168225]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.347832][G eval loss: 0.207256]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.346710][G train loss: 0.169180]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.341810][G eval loss: 0.208164]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.339302][G train loss: 0.172413]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.335417][G eval loss: 0.210681]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.331833][G train loss: 0.177041]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.328797][G eval loss: 0.214336]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.324647][G train loss: 0.180788]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.321925][G eval loss: 0.217915]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.317882][G train loss: 0.183787]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.314893][G eval loss: 0.221908]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.311574][G train loss: 0.186409]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.307778][G eval loss: 0.226386]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.305399][G train loss: 0.189210]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.300681][G eval loss: 0.232166]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299433][G train loss: 0.193134]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.293688][G eval loss: 0.239575]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.293331][G train loss: 0.198927]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.287007][G eval loss: 0.249333]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.286893][G train loss: 0.207356]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.280681][G eval loss: 0.261383]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n"
     ]
    }
   ],
   "source": [
    "from LOB_GAN_training_raw import train_lob_gan\n",
    "\n",
    "stock_list = [\"0050\", \"0056\", \"2330\"]\n",
    "\n",
    "lr_config = {\n",
    "    \"0050\": {\"lr_g\": 0.004, \"lr_d\": 0.0008},\n",
    "    \"0056\": {\"lr_g\": 0.006, \"lr_d\": 0.0005},\n",
    "    \"2330\": {\"lr_g\": 0.0045, \"lr_d\": 0.0012},\n",
    "}\n",
    "\n",
    "batch_config = {\n",
    "    \"0050\": 50,\n",
    "    \"0056\": 50,\n",
    "    \"2330\": 50,   \n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"===== Training GAN for {stock} =====\")\n",
    "    cfg = lr_config[stock]\n",
    "    bs = batch_config[stock]\n",
    "    res = train_lob_gan(\n",
    "        stock=stock,\n",
    "        lr_g=cfg[\"lr_g\"],\n",
    "        lr_d=cfg[\"lr_d\"],\n",
    "        batch_size=bs,\n",
    "        seed=307,\n",
    "    )\n",
    "    results[stock] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d028d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Grid search for 0050 ==========\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.119275]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.270949]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.108129]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.259711]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.106901]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249986][G train loss: 1.258437]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.106161]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.257762]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.105135]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.256803]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 1.105068]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249959][G train loss: 1.256771]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 1.105553]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249944][G train loss: 1.257264]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 1.105930]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249926][G train loss: 1.257632]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 1.105726]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249904][G train loss: 1.257400]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249899][G eval loss: 1.104848]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249876][G train loss: 1.256486]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249869][G eval loss: 1.103633]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249840][G train loss: 1.255223]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 1.102427]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249797][G train loss: 1.253971]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249786][G eval loss: 1.101383]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249743][G train loss: 1.252883]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249729][G eval loss: 1.100673]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249678][G train loss: 1.252138]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249659][G eval loss: 1.100297]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249597][G train loss: 1.251712]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249577][G eval loss: 1.100231]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249501][G train loss: 1.251582]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249477][G eval loss: 1.100348]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249387][G train loss: 1.251635]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249357][G eval loss: 1.100292]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249250][G train loss: 1.251506]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249217][G eval loss: 1.099502]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249089][G train loss: 1.250608]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249054][G eval loss: 1.097937]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248903][G train loss: 1.248861]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248861][G eval loss: 1.095587]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248682][G train loss: 1.246260]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248626][G eval loss: 1.092116]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248418][G train loss: 1.242439]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248327][G eval loss: 1.086703]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248088][G train loss: 1.236572]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.247977][G eval loss: 1.078333]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247706][G train loss: 1.227635]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247595][G eval loss: 1.065985]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247284][G train loss: 1.214485]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247200][G eval loss: 1.048447]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246843][G train loss: 1.195944]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246778][G eval loss: 1.025737]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246371][G train loss: 1.171796]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246377][G eval loss: 0.997435]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245899][G train loss: 1.141487]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.246000][G eval loss: 0.961977]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.245434][G train loss: 1.104153]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245643][G eval loss: 0.919482]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.244990][G train loss: 1.060108]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.245313][G eval loss: 0.872464]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.244558][G train loss: 1.011675]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.245055][G eval loss: 0.825606]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.244149][G train loss: 0.962630]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.244898][G eval loss: 0.785120]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.243767][G train loss: 0.917053]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.244772][G eval loss: 0.754281]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.243367][G train loss: 0.876966]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.244608][G eval loss: 0.736057]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.242894][G train loss: 0.845592]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.244392][G eval loss: 0.732642]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.242360][G train loss: 0.826491]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.244304][G eval loss: 0.740996]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.241911][G train loss: 0.817197]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.244569][G eval loss: 0.750104]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.0005 -> score=0.994673\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.117348]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.269022]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.107848]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.259430]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.107934]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.259470]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 1.107752]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.259352]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.106480]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.258148]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 1.105823]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249934][G train loss: 1.257526]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 1.105642]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249900][G train loss: 1.257354]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 1.105422]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249856][G train loss: 1.257124]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249829][G eval loss: 1.104922]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249791][G train loss: 1.256597]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249755][G eval loss: 1.103952]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249704][G train loss: 1.255591]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249656][G eval loss: 1.102809]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249587][G train loss: 1.254401]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249521][G eval loss: 1.101852]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249428][G train loss: 1.253398]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249352][G eval loss: 1.101090]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249232][G train loss: 1.252594]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249129][G eval loss: 1.100708]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248976][G train loss: 1.252176]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248830][G eval loss: 1.100744]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248635][G train loss: 1.252164]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248435][G eval loss: 1.101221]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248189][G train loss: 1.252581]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247928][G eval loss: 1.102093]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247619][G train loss: 1.253393]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247369][G eval loss: 1.102612]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246991][G train loss: 1.253839]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246538][G eval loss: 1.101883]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246085][G train loss: 1.253011]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245511][G eval loss: 1.100232]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244968][G train loss: 1.251205]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244334][G eval loss: 1.097981]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243709][G train loss: 1.248718]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242917][G eval loss: 1.095047]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242209][G train loss: 1.245433]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241217][G eval loss: 1.090575]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240423][G train loss: 1.240480]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239243][G eval loss: 1.083274]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238351][G train loss: 1.232568]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236957][G eval loss: 1.072087]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235960][G train loss: 1.220541]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234424][G eval loss: 1.055990]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.233300][G train loss: 1.203394]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231631][G eval loss: 1.034966]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.230363][G train loss: 1.180851]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.228569][G eval loss: 1.008484]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.227142][G train loss: 1.152276]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.225200][G eval loss: 0.974675]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.223656][G train loss: 1.116490]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.221947][G eval loss: 0.932603]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.220142][G train loss: 1.073285]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.219502][G eval loss: 0.883689]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.217126][G train loss: 1.024095]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.218658][G eval loss: 0.832171]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.215622][G train loss: 0.971668]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.221429][G eval loss: 0.781474]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.217015][G train loss: 0.918553]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.228506][G eval loss: 0.735036]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.222167][G train loss: 0.866600]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.238255][G eval loss: 0.696277]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.230798][G train loss: 0.817407]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250014][G eval loss: 0.665469]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.242000][G train loss: 0.774323]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.263483][G eval loss: 0.639508]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.254340][G train loss: 0.738828]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.277342][G eval loss: 0.618302]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.267042][G train loss: 0.709377]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.290981][G eval loss: 0.597570]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.279433][G train loss: 0.682120]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.302760][G eval loss: 0.575034]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.290966][G train loss: 0.655376]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.312883][G eval loss: 0.549432]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.301308][G train loss: 0.628705]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.321247][G eval loss: 0.525353]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.309518][G train loss: 0.603509]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.327861][G eval loss: 0.505094]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.315519][G train loss: 0.582509]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.332504][G eval loss: 0.488767]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.319466][G train loss: 0.565839]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.334148][G eval loss: 0.477095]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.321850][G train loss: 0.553709]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.334225][G eval loss: 0.468145]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.323206][G train loss: 0.544595]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.333293][G eval loss: 0.461388]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.323749][G train loss: 0.537106]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.331393][G eval loss: 0.457375]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.322852][G train loss: 0.532372]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.328569][G eval loss: 0.455950]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.320276][G train loss: 0.530057]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.325559][G eval loss: 0.455366]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.317726][G train loss: 0.527674]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.322156][G eval loss: 0.454873]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.315422][G train loss: 0.524987]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.318218][G eval loss: 0.453617]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.312949][G train loss: 0.521860]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.314182][G eval loss: 0.450924]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.309992][G train loss: 0.517351]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.309999][G eval loss: 0.446332]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.307085][G train loss: 0.510987]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.304943][G eval loss: 0.443888]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.303634][G train loss: 0.506479]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.299970][G eval loss: 0.441739]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299643][G train loss: 0.501919]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.295039][G eval loss: 0.441201]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.295083][G train loss: 0.499082]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.290655][G eval loss: 0.443324]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.290636][G train loss: 0.499080]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.286974][G eval loss: 0.443465]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.286837][G train loss: 0.498879]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.283545][G eval loss: 0.444566]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.283169][G train loss: 0.500298]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.279999][G eval loss: 0.445908]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.279103][G train loss: 0.502009]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.276858][G eval loss: 0.445695]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.275081][G train loss: 0.502841]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.274115][G eval loss: 0.446005]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.271605][G train loss: 0.504608]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.271596][G eval loss: 0.446998]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.269004][G train loss: 0.506768]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.269076][G eval loss: 0.448190]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.266708][G train loss: 0.508424]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.266521][G eval loss: 0.449291]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.264536][G train loss: 0.509336]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.263590][G eval loss: 0.449091]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.262477][G train loss: 0.509300]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.260577][G eval loss: 0.447475]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.260595][G train loss: 0.508215]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.257835][G eval loss: 0.446247]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.258982][G train loss: 0.507135]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.256090][G eval loss: 0.446100]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.257574][G train loss: 0.507109]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.254893][G eval loss: 0.445716]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.256320][G train loss: 0.507197]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.253912][G eval loss: 0.445059]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.255211][G train loss: 0.506519]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.253114][G eval loss: 0.445002]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.254254][G train loss: 0.505375]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.252473][G eval loss: 0.446369]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.253438][G train loss: 0.504807]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.251914][G eval loss: 0.448215]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.0008 -> score=0.700129\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.001 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 1.116171]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 1.267844]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.107673]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.259255]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.108644]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.260181]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 1.108809]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249965][G train loss: 1.260410]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249952][G eval loss: 1.107412]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249939][G train loss: 1.259080]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 1.106431]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249897][G train loss: 1.258134]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249867][G eval loss: 1.105880]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249837][G train loss: 1.257592]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249794][G eval loss: 1.105497]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249749][G train loss: 1.257199]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249687][G eval loss: 1.105077]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249622][G train loss: 1.256753]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249530][G eval loss: 1.104199]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249438][G train loss: 1.255840]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249321][G eval loss: 1.103112]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249194][G train loss: 1.254706]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249023][G eval loss: 1.102191]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248851][G train loss: 1.253739]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248601][G eval loss: 1.101598]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248376][G train loss: 1.253104]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248028][G eval loss: 1.101521]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247733][G train loss: 1.252994]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247238][G eval loss: 1.101930]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246859][G train loss: 1.253360]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246351][G eval loss: 1.102577]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245876][G train loss: 1.253946]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245017][G eval loss: 1.103332]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244436][G train loss: 1.254655]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243256][G eval loss: 1.104121]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242574][G train loss: 1.255380]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241084][G eval loss: 1.104424]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.240303][G train loss: 1.255584]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238426][G eval loss: 1.104204]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.237534][G train loss: 1.255196]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235225][G eval loss: 1.103539]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.234215][G train loss: 1.254282]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.231486][G eval loss: 1.101820]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.230359][G train loss: 1.252208]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.227055][G eval loss: 1.098445]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.225835][G train loss: 1.248333]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.221749][G eval loss: 1.092525]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.220516][G train loss: 1.241765]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.215699][G eval loss: 1.082622]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.214417][G train loss: 1.230926]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.208709][G eval loss: 1.068415]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.207431][G train loss: 1.215564]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.201098][G eval loss: 1.049602]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.199810][G train loss: 1.195059]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.192841][G eval loss: 1.025746]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.191597][G train loss: 1.168901]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.184222][G eval loss: 0.994314]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.183145][G train loss: 1.134928]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.176124][G eval loss: 0.953846]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.175771][G train loss: 1.091273]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.172868][G eval loss: 0.899435]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.174521][G train loss: 1.032238]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.190882][G eval loss: 0.809265]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.190961][G train loss: 0.944590]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.235776][G eval loss: 0.695268]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.233583][G train loss: 0.831161]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.276027][G eval loss: 0.621473]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.273203][G train loss: 0.752678]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.303321][G eval loss: 0.581332]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.300691][G train loss: 0.704992]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.319442][G eval loss: 0.558190]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.317377][G train loss: 0.673251]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.329859][G eval loss: 0.543727]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.327954][G train loss: 0.650012]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.337413][G eval loss: 0.535044]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.335438][G train loss: 0.631360]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.343368][G eval loss: 0.525369]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.340358][G train loss: 0.613188]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.348508][G eval loss: 0.513134]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.343520][G train loss: 0.594278]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.353581][G eval loss: 0.498469]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.345339][G train loss: 0.574358]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.361860][G eval loss: 0.481383]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.346035][G train loss: 0.553724]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.364554][G eval loss: 0.462986]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.345854][G train loss: 0.532096]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.365285][G eval loss: 0.444435]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.345459][G train loss: 0.512527]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.364652][G eval loss: 0.427719]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.345134][G train loss: 0.495720]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.363032][G eval loss: 0.411602]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.343420][G train loss: 0.481685]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.360639][G eval loss: 0.400016]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.341208][G train loss: 0.472159]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.357705][G eval loss: 0.394061]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.339123][G train loss: 0.467725]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.354546][G eval loss: 0.395455]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.335431][G train loss: 0.469892]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.351153][G eval loss: 0.400807]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.329888][G train loss: 0.474030]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.347674][G eval loss: 0.403550]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.323496][G train loss: 0.474855]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.343843][G eval loss: 0.403587]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.318321][G train loss: 0.472042]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.339656][G eval loss: 0.403018]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.313590][G train loss: 0.467441]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.335190][G eval loss: 0.407081]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.308850][G train loss: 0.466702]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.330535][G eval loss: 0.410541]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.304108][G train loss: 0.466949]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.325730][G eval loss: 0.409296]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299352][G train loss: 0.464822]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.320753][G eval loss: 0.406817]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.294562][G train loss: 0.462928]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.315598][G eval loss: 0.408766]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.289750][G train loss: 0.465406]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.310232][G eval loss: 0.414239]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.284977][G train loss: 0.471132]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.304687][G eval loss: 0.418085]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.280343][G train loss: 0.475688]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.299075][G eval loss: 0.420728]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.276022][G train loss: 0.479792]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.293564][G eval loss: 0.425662]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.272106][G train loss: 0.486750]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.287961][G eval loss: 0.432691]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.268258][G train loss: 0.496458]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.282567][G eval loss: 0.438829]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.263938][G train loss: 0.507395]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.277875][G eval loss: 0.445092]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.259136][G train loss: 0.520184]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.274304][G eval loss: 0.454134]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.258991][G train loss: 0.527500]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.271368][G eval loss: 0.462104]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.257277][G train loss: 0.536719]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.268918][G eval loss: 0.468243]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.255743][G train loss: 0.543748]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.267013][G eval loss: 0.474976]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.253852][G train loss: 0.551611]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.265515][G eval loss: 0.483342]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.253689][G train loss: 0.558543]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.264464][G eval loss: 0.489744]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.253196][G train loss: 0.563946]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.263894][G eval loss: 0.494340]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.253053][G train loss: 0.567689]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.263718][G eval loss: 0.500600]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.252848][G train loss: 0.573890]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.263697][G eval loss: 0.511079]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.252455][G train loss: 0.583619]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.263792][G eval loss: 0.522250]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.252330][G train loss: 0.593366]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.263811][G eval loss: 0.529987]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.252358][G train loss: 0.599884]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.263920][G eval loss: 0.533785]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.252497][G train loss: 0.602797]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.264104][G eval loss: 0.536412]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.252733][G train loss: 0.604475]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.264546][G eval loss: 0.540638]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.253046][G train loss: 0.606525]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.264909][G eval loss: 0.546347]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.253404][G train loss: 0.608279]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.265047][G eval loss: 0.551624]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.253917][G train loss: 0.608882]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.264916][G eval loss: 0.552944]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.253353][G train loss: 0.611236]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.264630][G eval loss: 0.550315]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.253247][G train loss: 0.610969]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.264200][G eval loss: 0.548874]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.253027][G train loss: 0.611219]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.263665][G eval loss: 0.548193]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.252841][G train loss: 0.611721]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.263036][G eval loss: 0.547959]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.253635][G train loss: 0.609080]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.262375][G eval loss: 0.547273]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.253527][G train loss: 0.604421]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.261654][G eval loss: 0.548683]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.253561][G train loss: 0.597897]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.261036][G eval loss: 0.558938]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.001 -> score=0.819974\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 1.114991]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250029][G train loss: 1.266665]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.107474]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.259055]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 1.109319]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.260855]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 1.109840]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.261440]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249935][G eval loss: 1.108373]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249916][G train loss: 1.260041]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249878][G eval loss: 1.107101]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249849][G train loss: 1.258805]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249797][G eval loss: 1.106237]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249754][G train loss: 1.257949]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249675][G eval loss: 1.106217]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249609][G train loss: 1.257920]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249496][G eval loss: 1.105857]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249393][G train loss: 1.257533]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249225][G eval loss: 1.104958]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249070][G train loss: 1.256600]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248774][G eval loss: 1.103803]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248571][G train loss: 1.255399]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248152][G eval loss: 1.102811]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247893][G train loss: 1.254362]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247297][G eval loss: 1.102483]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246960][G train loss: 1.253994]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246269][G eval loss: 1.102867]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245815][G train loss: 1.254346]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244762][G eval loss: 1.103499]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.244171][G train loss: 1.254942]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242699][G eval loss: 1.104732]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241964][G train loss: 1.256123]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240008][G eval loss: 1.106510]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.239119][G train loss: 1.257855]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236636][G eval loss: 1.108501]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.235566][G train loss: 1.259768]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232527][G eval loss: 1.109794]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.231288][G train loss: 1.260957]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227304][G eval loss: 1.111045]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.225966][G train loss: 1.262037]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221008][G eval loss: 1.111966]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.219701][G train loss: 1.262708]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.213564][G eval loss: 1.112175]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.212284][G train loss: 1.262552]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.204801][G eval loss: 1.111251]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.203625][G train loss: 1.261080]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.194655][G eval loss: 1.108435]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.193556][G train loss: 1.257527]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.183386][G eval loss: 1.102129]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.182324][G train loss: 1.250216]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.171197][G eval loss: 1.091333]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.170229][G train loss: 1.238105]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.158302][G eval loss: 1.077215]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.157603][G train loss: 1.221949]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.145405][G eval loss: 1.059211]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.145171][G train loss: 1.201029]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.133993][G eval loss: 1.033157]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.135117][G train loss: 1.169824]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.129182][G eval loss: 0.989625]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.145666][G train loss: 1.098715]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.170433][G eval loss: 0.871871]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.178854][G train loss: 1.000695]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.275845][G eval loss: 0.698090]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.276035][G train loss: 0.826793]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.332638][G eval loss: 0.624575]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.332821][G train loss: 0.754581]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.351869][G eval loss: 0.580622]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.352589][G train loss: 0.711017]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.361958][G eval loss: 0.551581]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.363663][G train loss: 0.679890]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.368363][G eval loss: 0.543397]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.370762][G train loss: 0.663407]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.372761][G eval loss: 0.536414]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.375353][G train loss: 0.646931]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.376165][G eval loss: 0.522107]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.378108][G train loss: 0.624148]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.379864][G eval loss: 0.506770]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.379442][G train loss: 0.599793]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.386073][G eval loss: 0.498858]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.379634][G train loss: 0.581807]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.401793][G eval loss: 0.495126]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.378883][G train loss: 0.571123]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.408968][G eval loss: 0.490859]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.0012 -> score=0.899828\n",
      "\n",
      "----- 0050: lr_g=0.0045, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.117444]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.269119]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.107194]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.258755]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.106875]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.258419]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.105370]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.256998]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.104736]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.256428]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.105475]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249960][G train loss: 1.257193]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 1.106403]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249946][G train loss: 1.258119]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 1.106492]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249927][G train loss: 1.258180]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 1.105583]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249904][G train loss: 1.257228]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249898][G eval loss: 1.104096]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249876][G train loss: 1.255684]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249868][G eval loss: 1.102530]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249840][G train loss: 1.254061]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249831][G eval loss: 1.101248]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249797][G train loss: 1.252745]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249786][G eval loss: 1.100382]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249744][G train loss: 1.251860]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249728][G eval loss: 1.099935]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249678][G train loss: 1.251374]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249656][G eval loss: 1.099761]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249595][G train loss: 1.251131]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249573][G eval loss: 1.099675]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249500][G train loss: 1.250961]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249472][G eval loss: 1.099144]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249386][G train loss: 1.250318]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249350][G eval loss: 1.097839]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249250][G train loss: 1.248860]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249203][G eval loss: 1.095504]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249087][G train loss: 1.246308]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249038][G eval loss: 1.091721]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248904][G train loss: 1.242253]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248847][G eval loss: 1.085855]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248694][G train loss: 1.236011]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248623][G eval loss: 1.076830]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248453][G train loss: 1.226404]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248352][G eval loss: 1.063087]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248167][G train loss: 1.211805]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248051][G eval loss: 1.042005]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247857][G train loss: 1.189413]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247768][G eval loss: 1.012387]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247555][G train loss: 1.158051]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247537][G eval loss: 0.976059]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.247294][G train loss: 1.119506]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.247359][G eval loss: 0.936189]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.247077][G train loss: 1.076809]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.247189][G eval loss: 0.893906]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.246883][G train loss: 1.031512]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.246935][G eval loss: 0.847055]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.246625][G train loss: 0.983417]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.246526][G eval loss: 0.798746]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.246232][G train loss: 0.935360]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.246011][G eval loss: 0.760630]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.245714][G train loss: 0.894670]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.245538][G eval loss: 0.738977]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.245212][G train loss: 0.863261]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.245241][G eval loss: 0.729012]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.244816][G train loss: 0.838432]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.245150][G eval loss: 0.727104]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.244596][G train loss: 0.821048]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.245407][G eval loss: 0.734019]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.244556][G train loss: 0.812566]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.245851][G eval loss: 0.740931]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.0045 lr_d=0.0005 -> score=0.986782\n",
      "\n",
      "----- 0050: lr_g=0.0045, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.115516]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.267191]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.106914]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.258475]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.107912]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.259456]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 1.106964]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.258592]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 1.106084]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249959][G train loss: 1.257776]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249951][G eval loss: 1.106227]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249937][G train loss: 1.257946]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249924][G eval loss: 1.106486]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249904][G train loss: 1.258202]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249886][G eval loss: 1.105974]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249860][G train loss: 1.257663]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 1.104767]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249795][G train loss: 1.256412]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249759][G eval loss: 1.103184]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249710][G train loss: 1.254774]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249660][G eval loss: 1.101691]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249594][G train loss: 1.253224]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249526][G eval loss: 1.100639]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249437][G train loss: 1.252138]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249359][G eval loss: 1.100052]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249244][G train loss: 1.251532]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249136][G eval loss: 1.099941]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248988][G train loss: 1.251382]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248835][G eval loss: 1.100204]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248647][G train loss: 1.251576]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248442][G eval loss: 1.100695]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248208][G train loss: 1.251982]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247938][G eval loss: 1.100956]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247649][G train loss: 1.252130]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247404][G eval loss: 1.100196]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247050][G train loss: 1.251216]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246572][G eval loss: 1.097913]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246158][G train loss: 1.248701]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245538][G eval loss: 1.094048]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.245077][G train loss: 1.244516]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244393][G eval loss: 1.088236]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243893][G train loss: 1.238267]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243074][G eval loss: 1.079585]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242542][G train loss: 1.228960]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241571][G eval loss: 1.066462]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.241017][G train loss: 1.214856]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239922][G eval loss: 1.045863]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.239373][G train loss: 1.192702]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.238224][G eval loss: 1.016835]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.237726][G train loss: 1.161562]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.236817][G eval loss: 0.981044]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.236333][G train loss: 1.123176]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.236020][G eval loss: 0.941715]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.235403][G train loss: 1.080687]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.235486][G eval loss: 0.900940]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.234738][G train loss: 1.035981]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.234355][G eval loss: 0.856417]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.233758][G train loss: 0.989010]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.232151][G eval loss: 0.809478]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.232006][G train loss: 0.941748]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.229289][G eval loss: 0.771200]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.229703][G train loss: 0.901068]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.227394][G eval loss: 0.748136]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.228254][G train loss: 0.869109]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.228794][G eval loss: 0.730310]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.229226][G train loss: 0.838918]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.236430][G eval loss: 0.708710]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.234751][G train loss: 0.806492]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.251842][G eval loss: 0.682342]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.246284][G train loss: 0.774143]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.266815][G eval loss: 0.665387]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.260893][G train loss: 0.747148]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.283041][G eval loss: 0.646423]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.275249][G train loss: 0.725059]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.299445][G eval loss: 0.627484]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.288284][G train loss: 0.701870]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.312559][G eval loss: 0.603835]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.299106][G train loss: 0.675137]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.322289][G eval loss: 0.577856]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.308413][G train loss: 0.647571]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.328900][G eval loss: 0.558579]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.315462][G train loss: 0.625050]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.333209][G eval loss: 0.541907]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.320356][G train loss: 0.606742]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.335669][G eval loss: 0.523094]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.323273][G train loss: 0.588543]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.335746][G eval loss: 0.505206]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.324276][G train loss: 0.571471]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.333339][G eval loss: 0.492498]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.323429][G train loss: 0.559722]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.328883][G eval loss: 0.485286]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.321490][G train loss: 0.553636]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.323344][G eval loss: 0.481829]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.318565][G train loss: 0.548597]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.317148][G eval loss: 0.480754]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.314932][G train loss: 0.545461]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.310966][G eval loss: 0.483053]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.311305][G train loss: 0.546876]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.305528][G eval loss: 0.482450]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.307909][G train loss: 0.548692]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.301002][G eval loss: 0.477718]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.304487][G train loss: 0.548116]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.297142][G eval loss: 0.475355]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.300805][G train loss: 0.547992]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.292818][G eval loss: 0.479753]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.296801][G train loss: 0.551006]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.287969][G eval loss: 0.484312]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.292523][G train loss: 0.553573]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.282864][G eval loss: 0.483803]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.288284][G train loss: 0.551722]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.278229][G eval loss: 0.481414]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.284061][G train loss: 0.548293]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.274115][G eval loss: 0.482024]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.279734][G train loss: 0.546876]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.270895][G eval loss: 0.483535]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.275785][G train loss: 0.545669]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.268454][G eval loss: 0.482757]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.272305][G train loss: 0.542282]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.266521][G eval loss: 0.482023]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.269352][G train loss: 0.539448]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.264670][G eval loss: 0.484578]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.266712][G train loss: 0.540004]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.262740][G eval loss: 0.487815]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.0045 lr_d=0.0008 -> score=0.750555\n",
      "\n",
      "----- 0050: lr_g=0.0045, lr_d=0.001 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 1.114339]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 1.266014]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.106739]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.258300]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.108624]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.260169]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 1.108023]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.259651]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 1.107015]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249942][G train loss: 1.258707]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 1.106836]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249901][G train loss: 1.258555]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249871][G eval loss: 1.106726]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249841][G train loss: 1.258443]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249796][G eval loss: 1.106056]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249752][G train loss: 1.257745]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249688][G eval loss: 1.104856]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249625][G train loss: 1.256503]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249534][G eval loss: 1.103307]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249446][G train loss: 1.254899]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249324][G eval loss: 1.101838]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249201][G train loss: 1.253371]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249027][G eval loss: 1.100836]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248861][G train loss: 1.252335]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248611][G eval loss: 1.100373]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248394][G train loss: 1.251854]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248037][G eval loss: 1.100603]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247753][G train loss: 1.252047]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247240][G eval loss: 1.101279]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246876][G train loss: 1.252653]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246334][G eval loss: 1.102006]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245876][G train loss: 1.253296]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.244987][G eval loss: 1.102241]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244434][G train loss: 1.253414]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243229][G eval loss: 1.101914]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242593][G train loss: 1.252908]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241071][G eval loss: 1.100770]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.240355][G train loss: 1.251511]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238434][G eval loss: 1.098446]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.237648][G train loss: 1.248837]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235316][G eval loss: 1.094220]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.234463][G train loss: 1.244154]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.231754][G eval loss: 1.086769]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.230858][G train loss: 1.235977]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.227597][G eval loss: 1.074670]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.226794][G train loss: 1.222565]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.223067][G eval loss: 1.054620]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.222590][G train loss: 1.200374]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.218759][G eval loss: 1.025442]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.219106][G train loss: 1.167706]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.216815][G eval loss: 0.986028]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.218055][G train loss: 1.124478]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.220748][G eval loss: 0.938215]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.219515][G train loss: 1.077828]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.225789][G eval loss: 0.896267]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.222440][G train loss: 1.033075]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.228538][G eval loss: 0.857618]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.225538][G train loss: 0.986667]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.230248][G eval loss: 0.811410]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.228595][G train loss: 0.934081]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.232580][G eval loss: 0.759829]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.233130][G train loss: 0.881213]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.237847][G eval loss: 0.716472]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.243670][G train loss: 0.829552]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.250780][G eval loss: 0.682304]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.261565][G train loss: 0.783619]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.272821][G eval loss: 0.649906]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.283544][G train loss: 0.747838]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.300226][G eval loss: 0.624431]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.304312][G train loss: 0.725136]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.324161][G eval loss: 0.608116]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.321454][G train loss: 0.708364]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.348213][G eval loss: 0.587074]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.333539][G train loss: 0.694657]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.358448][G eval loss: 0.576897]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.341403][G train loss: 0.678524]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.363060][G eval loss: 0.565488]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.345773][G train loss: 0.659645]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.363596][G eval loss: 0.551551]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.349129][G train loss: 0.637952]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.362341][G eval loss: 0.532475]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.353162][G train loss: 0.610428]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.360553][G eval loss: 0.509343]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.354771][G train loss: 0.583216]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.358471][G eval loss: 0.488089]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.354377][G train loss: 0.559230]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.355846][G eval loss: 0.472143]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.353179][G train loss: 0.539966]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.352676][G eval loss: 0.455302]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.351953][G train loss: 0.521178]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.349238][G eval loss: 0.442895]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.350369][G train loss: 0.507088]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.345463][G eval loss: 0.433172]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.347804][G train loss: 0.498020]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.341247][G eval loss: 0.421044]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.343531][G train loss: 0.488265]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.336303][G eval loss: 0.415053]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.336926][G train loss: 0.484942]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.330247][G eval loss: 0.412560]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.329339][G train loss: 0.483052]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.321953][G eval loss: 0.410075]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.322042][G train loss: 0.479699]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.315065][G eval loss: 0.411251]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.315713][G train loss: 0.478523]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.309729][G eval loss: 0.411172]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.310187][G train loss: 0.475363]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.304863][G eval loss: 0.416534]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.304627][G train loss: 0.476133]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.300230][G eval loss: 0.423008]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.0045 lr_d=0.001 -> score=0.723238\n",
      "\n",
      "----- 0050: lr_g=0.0045, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 1.113159]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250029][G train loss: 1.264834]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.106540]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 1.258101]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 1.109300]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.260844]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.109055]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.260683]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 1.107972]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249922][G train loss: 1.259664]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249885][G eval loss: 1.107498]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249857][G train loss: 1.259217]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249803][G eval loss: 1.107069]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249762][G train loss: 1.258785]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249681][G eval loss: 1.106734]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249618][G train loss: 1.258424]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249503][G eval loss: 1.105660]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249405][G train loss: 1.257307]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249238][G eval loss: 1.104158]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249090][G train loss: 1.255751]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248794][G eval loss: 1.102656]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248597][G train loss: 1.254191]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248167][G eval loss: 1.101612]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247915][G train loss: 1.253112]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247329][G eval loss: 1.101462]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246998][G train loss: 1.252945]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246314][G eval loss: 1.102115]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245869][G train loss: 1.253564]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244795][G eval loss: 1.102998]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.244219][G train loss: 1.254378]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242744][G eval loss: 1.104198]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.242034][G train loss: 1.255492]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240056][G eval loss: 1.105386]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.239214][G train loss: 1.256549]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236739][G eval loss: 1.106141]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.235733][G train loss: 1.257131]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232650][G eval loss: 1.105970]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.231519][G train loss: 1.256659]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227534][G eval loss: 1.104986]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.226363][G train loss: 1.255259]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221406][G eval loss: 1.102216]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.220343][G train loss: 1.251957]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.214264][G eval loss: 1.096602]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.213338][G train loss: 1.245522]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.205990][G eval loss: 1.086618]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.205418][G train loss: 1.234067]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.197008][G eval loss: 1.068902]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.197504][G train loss: 1.212841]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.190815][G eval loss: 1.037048]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.194815][G train loss: 1.173037]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.200970][G eval loss: 0.975107]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.202103][G train loss: 1.116343]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.217707][G eval loss: 0.921305]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.212868][G train loss: 1.065211]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.225425][G eval loss: 0.901669]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.225486][G train loss: 1.027665]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.230504][G eval loss: 0.886340]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.240818][G train loss: 0.986229]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.235124][G eval loss: 0.862675]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.258377][G train loss: 0.935541]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.240465][G eval loss: 0.822354]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.272512][G train loss: 0.877805]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.248377][G eval loss: 0.764736]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.283686][G train loss: 0.820498]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.259398][G eval loss: 0.704733]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.292123][G train loss: 0.780067]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.272234][G eval loss: 0.682542]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.299727][G train loss: 0.762213]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.290629][G eval loss: 0.668861]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.312635][G train loss: 0.750054]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.306344][G eval loss: 0.659491]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.321784][G train loss: 0.740092]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.327760][G eval loss: 0.623245]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.328569][G train loss: 0.724446]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.355326][G eval loss: 0.578122]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.335872][G train loss: 0.707777]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.361363][G eval loss: 0.564614]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.341793][G train loss: 0.692847]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.363940][G eval loss: 0.553259]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.348570][G train loss: 0.677893]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.364190][G eval loss: 0.538464]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.351983][G train loss: 0.656742]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.362882][G eval loss: 0.522019]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.353056][G train loss: 0.631682]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.360614][G eval loss: 0.506975]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.350721][G train loss: 0.608074]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.357303][G eval loss: 0.498862]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.345199][G train loss: 0.593094]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.353295][G eval loss: 0.495972]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.338700][G train loss: 0.582083]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.349089][G eval loss: 0.492185]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.332187][G train loss: 0.567965]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.345005][G eval loss: 0.481401]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.322824][G train loss: 0.555842]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.340823][G eval loss: 0.467364]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.315186][G train loss: 0.539843]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.336415][G eval loss: 0.461981]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.313981][G train loss: 0.520290]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.331679][G eval loss: 0.462074]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.308439][G train loss: 0.521192]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.326693][G eval loss: 0.461731]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.303376][G train loss: 0.523765]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.321536][G eval loss: 0.459167]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.298367][G train loss: 0.525083]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.316241][G eval loss: 0.456851]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.293448][G train loss: 0.527858]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.310953][G eval loss: 0.460938]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.288642][G train loss: 0.535861]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.305694][G eval loss: 0.468967]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.283834][G train loss: 0.545258]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.300138][G eval loss: 0.475186]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.278957][G train loss: 0.550851]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.290771][G eval loss: 0.480226]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.274431][G train loss: 0.553724]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.274576][G eval loss: 0.489236]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.270633][G train loss: 0.558710]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.270873][G eval loss: 0.504661]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.267384][G train loss: 0.567708]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.266810][G eval loss: 0.518092]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.264093][G train loss: 0.577203]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.262109][G eval loss: 0.526474]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.259120][G train loss: 0.583848]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.257739][G eval loss: 0.531429]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.257280][G train loss: 0.589376]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.253492][G eval loss: 0.534990]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.256001][G train loss: 0.595350]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.250067][G eval loss: 0.538944]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.254952][G train loss: 0.602918]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.249229][G eval loss: 0.544068]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.254007][G train loss: 0.611320]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.249526][G eval loss: 0.550909]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.253316][G train loss: 0.619638]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.250254][G eval loss: 0.558545]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.252855][G train loss: 0.626910]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.251081][G eval loss: 0.565810]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.252759][G train loss: 0.632248]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.251950][G eval loss: 0.572678]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.252910][G train loss: 0.636957]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.252872][G eval loss: 0.581971]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.253129][G train loss: 0.644833]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.253714][G eval loss: 0.589705]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.254881][G train loss: 0.647207]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.254435][G eval loss: 0.596309]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.255035][G train loss: 0.652708]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.255141][G eval loss: 0.599272]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.254403][G train loss: 0.658660]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.255689][G eval loss: 0.596940]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.255175][G train loss: 0.656519]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.256132][G eval loss: 0.592571]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.252726][G train loss: 0.663011]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.256237][G eval loss: 0.586000]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.253256][G train loss: 0.657699]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.256265][G eval loss: 0.578952]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.252568][G train loss: 0.655040]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.256552][G eval loss: 0.569630]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.251354][G train loss: 0.651524]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.252972][G eval loss: 0.570066]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250765][G train loss: 0.644244]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.251708][G eval loss: 0.563739]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250546][G train loss: 0.634727]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.250852][G eval loss: 0.556152]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.249671][G train loss: 0.625725]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.250504][G eval loss: 0.547338]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.246947][G train loss: 0.622378]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.249346][G eval loss: 0.541849]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.245962][G train loss: 0.613731]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.248778][G eval loss: 0.534225]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.245791][G train loss: 0.603184]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.248886][G eval loss: 0.528403]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.243865][G train loss: 0.602756]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.247058][G eval loss: 0.530400]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.241951][G train loss: 0.601022]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.246288][G eval loss: 0.527495]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.241605][G train loss: 0.592549]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.245741][G eval loss: 0.527165]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.241492][G train loss: 0.586923]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.245373][G eval loss: 0.525885]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.241383][G train loss: 0.582716]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.245190][G eval loss: 0.524795]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.242060][G train loss: 0.577906]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.245014][G eval loss: 0.519925]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.244248][G train loss: 0.566747]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.244955][G eval loss: 0.515791]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.244877][G train loss: 0.562249]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.250555][G eval loss: 0.499239]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.246474][G train loss: 0.560405]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.250881][G eval loss: 0.501854]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.249945][G train loss: 0.557466]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.251077][G eval loss: 0.507926]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.0045 lr_d=0.0012 -> score=0.759003\n",
      "\n",
      "----- 0050: lr_g=0.005, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.115805]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.267481]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.106955]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.258495]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.106259]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.257826]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.104640]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.256299]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.104974]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.256692]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.106197]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249960][G train loss: 1.257936]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249957][G eval loss: 1.106663]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249946][G train loss: 1.258388]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 1.106144]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249927][G train loss: 1.257828]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249921][G eval loss: 1.104792]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249904][G train loss: 1.256410]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249898][G eval loss: 1.103150]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249876][G train loss: 1.254706]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249868][G eval loss: 1.101682]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249841][G train loss: 1.253182]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 1.100631]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249799][G train loss: 1.252086]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249785][G eval loss: 1.100009]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249744][G train loss: 1.251434]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249723][G eval loss: 1.099672]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249675][G train loss: 1.251066]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249645][G eval loss: 1.099494]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249587][G train loss: 1.250830]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249560][G eval loss: 1.099350]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249491][G train loss: 1.250582]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249447][G eval loss: 1.098595]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249368][G train loss: 1.249710]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249325][G eval loss: 1.096653]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249235][G train loss: 1.247653]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249190][G eval loss: 1.093080]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249092][G train loss: 1.243986]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249035][G eval loss: 1.086301]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248935][G train loss: 1.236947]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248869][G eval loss: 1.074322]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248778][G train loss: 1.224371]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248689][G eval loss: 1.056984]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248623][G train loss: 1.206454]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248566][G eval loss: 1.033288]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248534][G train loss: 1.182591]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248603][G eval loss: 1.006348]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.248578][G train loss: 1.156334]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.248767][G eval loss: 0.983610]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.248725][G train loss: 1.134441]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.248871][G eval loss: 0.965219]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.248780][G train loss: 1.116383]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.248750][G eval loss: 0.940096]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.248628][G train loss: 1.092832]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.248369][G eval loss: 0.906702]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.248297][G train loss: 1.060282]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.247552][G eval loss: 0.869206]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.247752][G train loss: 1.021887]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.246672][G eval loss: 0.832687]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.247447][G train loss: 0.981980]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.246485][G eval loss: 0.786665]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.247297][G train loss: 0.935728]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.246387][G eval loss: 0.741605]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.247400][G train loss: 0.884604]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.246275][G eval loss: 0.702802]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.247496][G train loss: 0.835069]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.246273][G eval loss: 0.672558]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.247486][G train loss: 0.792939]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.246080][G eval loss: 0.651728]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.247322][G train loss: 0.755946]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.245829][G eval loss: 0.648066]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.246934][G train loss: 0.731360]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.245432][G eval loss: 0.664639]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.246591][G train loss: 0.725616]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.246704][G eval loss: 0.688705]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.005 lr_d=0.0005 -> score=0.935409\n",
      "\n",
      "----- 0050: lr_g=0.005, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 1.113877]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250009][G train loss: 1.265553]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.106676]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249996][G train loss: 1.258215]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.107299]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.258866]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 1.106237]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.257896]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 1.106321]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249962][G train loss: 1.258039]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 1.106948]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249940][G train loss: 1.258687]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249924][G eval loss: 1.106742]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249906][G train loss: 1.258468]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249884][G eval loss: 1.105621]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249859][G train loss: 1.257305]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 1.103963]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249798][G train loss: 1.255582]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249764][G eval loss: 1.102221]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249717][G train loss: 1.253778]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249669][G eval loss: 1.100827]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249605][G train loss: 1.252328]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249540][G eval loss: 1.100009]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249455][G train loss: 1.251463]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249371][G eval loss: 1.099672]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249261][G train loss: 1.251095]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249143][G eval loss: 1.099681]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249002][G train loss: 1.251072]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248834][G eval loss: 1.099947]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248657][G train loss: 1.251276]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248434][G eval loss: 1.100375]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248217][G train loss: 1.251589]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247927][G eval loss: 1.100389]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247667][G train loss: 1.251470]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247403][G eval loss: 1.098961]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247095][G train loss: 1.249910]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246619][G eval loss: 1.095423]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246286][G train loss: 1.246208]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245733][G eval loss: 1.088351]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.245399][G train loss: 1.238745]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244772][G eval loss: 1.075880]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.244514][G train loss: 1.225428]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243937][G eval loss: 1.057434]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.243803][G train loss: 1.206105]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.243596][G eval loss: 1.031775]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.243662][G train loss: 1.179810]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.244053][G eval loss: 1.003136]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.244012][G train loss: 1.151745]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.244421][G eval loss: 0.981719]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.244310][G train loss: 1.130534]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.244435][G eval loss: 0.965370]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.244061][G train loss: 1.114332]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.243866][G eval loss: 0.940192]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.243190][G train loss: 1.090957]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.242443][G eval loss: 0.905966]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.241497][G train loss: 1.060179]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.239897][G eval loss: 0.870185]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.239097][G train loss: 1.023313]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.235628][G eval loss: 0.833423]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.236740][G train loss: 0.982146]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.230641][G eval loss: 0.795613]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.236888][G train loss: 0.931684]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.229191][G eval loss: 0.750668]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.238325][G train loss: 0.875758]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.229689][G eval loss: 0.710423]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.240396][G train loss: 0.824065]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.233037][G eval loss: 0.670319]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.244390][G train loss: 0.774967]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.235328][G eval loss: 0.639607]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248199][G train loss: 0.730635]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.243433][G eval loss: 0.604702]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.251560][G train loss: 0.690230]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.246706][G eval loss: 0.588842]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.253886][G train loss: 0.658435]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.256922][G eval loss: 0.577610]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.257786][G train loss: 0.641576]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.263957][G eval loss: 0.590192]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.261756][G train loss: 0.638462]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.272188][G eval loss: 0.606549]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.005 lr_d=0.0008 -> score=0.878737\n",
      "\n",
      "----- 0050: lr_g=0.005, lr_d=0.001 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250019][G eval loss: 1.112700]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250018][G train loss: 1.264376]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.106501]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.258041]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.108013]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.259580]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.107297]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.258956]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249959][G eval loss: 1.107249]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249946][G train loss: 1.258967]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249926][G eval loss: 1.107549]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249906][G train loss: 1.259288]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249873][G eval loss: 1.106968]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249845][G train loss: 1.258694]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249796][G eval loss: 1.105684]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249754][G train loss: 1.257369]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249691][G eval loss: 1.104007]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249630][G train loss: 1.255627]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249543][G eval loss: 1.102280]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249458][G train loss: 1.253836]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249342][G eval loss: 1.100904]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249224][G train loss: 1.252404]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249054][G eval loss: 1.100143]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248895][G train loss: 1.251596]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248637][G eval loss: 1.099954]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248431][G train loss: 1.251373]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248055][G eval loss: 1.100335]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247788][G train loss: 1.251717]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247253][G eval loss: 1.101030]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246911][G train loss: 1.252341]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246323][G eval loss: 1.101690]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245896][G train loss: 1.252871]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.244975][G eval loss: 1.101631]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244476][G train loss: 1.252642]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243279][G eval loss: 1.100525]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242737][G train loss: 1.251327]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241323][G eval loss: 1.097803]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.240786][G train loss: 1.248332]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.239207][G eval loss: 1.091447]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.238703][G train loss: 1.241413]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.237172][G eval loss: 1.078868]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.236874][G train loss: 1.227596]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.236162][G eval loss: 1.058105]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.236106][G train loss: 1.205518]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.237261][G eval loss: 1.027958]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.236895][G train loss: 1.175247]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238345][G eval loss: 0.999975]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238511][G train loss: 1.145638]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.239502][G eval loss: 0.983431]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.240059][G train loss: 1.127417]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.240677][G eval loss: 0.969360]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.240269][G train loss: 1.114083]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.239802][G eval loss: 0.946264]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.238922][G train loss: 1.092773]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.237281][G eval loss: 0.912374]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.235474][G train loss: 1.065061]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.232985][G eval loss: 0.878448]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.231363][G train loss: 1.034481]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.229210][G eval loss: 0.845387]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.226547][G train loss: 1.002141]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.227504][G eval loss: 0.805700]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.224958][G train loss: 0.956638]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.227063][G eval loss: 0.764400]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.231840][G train loss: 0.895249]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.226572][G eval loss: 0.734487]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.242232][G train loss: 0.837041]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.230946][G eval loss: 0.708532]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.254091][G train loss: 0.790973]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.235258][G eval loss: 0.682248]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.260872][G train loss: 0.757459]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.242245][G eval loss: 0.649629]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.268408][G train loss: 0.718733]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.252223][G eval loss: 0.603823]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.271440][G train loss: 0.679231]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.261120][G eval loss: 0.556500]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.274550][G train loss: 0.639375]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.269143][G eval loss: 0.527945]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.276678][G train loss: 0.610043]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.275823][G eval loss: 0.530517]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.279087][G train loss: 0.599748]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.282035][G eval loss: 0.543438]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.005 lr_d=0.001 -> score=0.825473\n",
      "\n",
      "----- 0050: lr_g=0.005, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250031][G eval loss: 1.111520]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250029][G train loss: 1.263196]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.106302]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 1.257842]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 1.108690]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.260256]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 1.108330]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249959][G train loss: 1.259988]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249946][G eval loss: 1.108203]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249928][G train loss: 1.259921]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249891][G eval loss: 1.108204]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249863][G train loss: 1.259943]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249808][G eval loss: 1.107301]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249768][G train loss: 1.259027]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249683][G eval loss: 1.106358]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249623][G train loss: 1.258043]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249511][G eval loss: 1.104834]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249418][G train loss: 1.256455]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249257][G eval loss: 1.103185]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249117][G train loss: 1.254741]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248841][G eval loss: 1.101785]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248651][G train loss: 1.253283]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248203][G eval loss: 1.101134]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247958][G train loss: 1.252581]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247364][G eval loss: 1.101345]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.247044][G train loss: 1.252753]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246386][G eval loss: 1.102011]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.005 lr_d=0.0012 -> score=1.348397\n",
      "\n",
      "----- 0050: lr_g=0.006, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 1.113095]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.264768]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.108249]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.259759]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.104881]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.256505]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.103945]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249978][G train loss: 1.255651]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 1.105849]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.257584]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 1.107439]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249961][G train loss: 1.259164]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 1.107299]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249946][G train loss: 1.258973]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 1.105654]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249926][G train loss: 1.257260]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 1.103426]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249903][G train loss: 1.254969]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249896][G eval loss: 1.101453]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249875][G train loss: 1.252971]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249866][G eval loss: 1.100129]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249839][G train loss: 1.251607]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249827][G eval loss: 1.099560]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249793][G train loss: 1.250955]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249777][G eval loss: 1.099172]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249736][G train loss: 1.250471]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249715][G eval loss: 1.097919]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249666][G train loss: 1.249072]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249640][G eval loss: 1.095869]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249581][G train loss: 1.246799]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249565][G eval loss: 1.092316]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249495][G train loss: 1.242914]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249454][G eval loss: 1.085780]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249373][G train loss: 1.235953]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249341][G eval loss: 1.074352]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249250][G train loss: 1.223854]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249218][G eval loss: 1.054722]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249116][G train loss: 1.202847]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249088][G eval loss: 1.023046]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248976][G train loss: 1.168766]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248962][G eval loss: 0.976513]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248839][G train loss: 1.119036]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248847][G eval loss: 0.915743]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248705][G train loss: 1.055002]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248728][G eval loss: 0.848670]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248574][G train loss: 0.984132]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248565][G eval loss: 0.781544]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.248403][G train loss: 0.911524]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.248355][G eval loss: 0.730797]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.248177][G train loss: 0.850342]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.248203][G eval loss: 0.726845]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.247957][G train loss: 0.822213]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.248179][G eval loss: 0.746319]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.247872][G train loss: 0.819597]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.248305][G eval loss: 0.759073]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.006 lr_d=0.0005 -> score=1.007377\n",
      "\n",
      "----- 0050: lr_g=0.006, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 1.111166]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250009][G train loss: 1.262840]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.107972]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249996][G train loss: 1.259482]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.105922]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.257545]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 1.105539]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.257245]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 1.107187]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249965][G train loss: 1.258921]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 1.108173]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249942][G train loss: 1.259898]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249926][G eval loss: 1.107350]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249909][G train loss: 1.259023]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249889][G eval loss: 1.105084]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249865][G train loss: 1.256690]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249839][G eval loss: 1.102539]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249807][G train loss: 1.254083]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249773][G eval loss: 1.100468]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249729][G train loss: 1.251986]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249681][G eval loss: 1.099229]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249620][G train loss: 1.250708]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249551][G eval loss: 1.098883]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249470][G train loss: 1.250280]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249372][G eval loss: 1.098823]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249268][G train loss: 1.250124]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249147][G eval loss: 1.097980]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249016][G train loss: 1.249133]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248866][G eval loss: 1.096417]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248701][G train loss: 1.247346]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248494][G eval loss: 1.093417]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248292][G train loss: 1.244009]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.248041][G eval loss: 1.087673]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247802][G train loss: 1.237830]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247582][G eval loss: 1.076681]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247297][G train loss: 1.226157]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246879][G eval loss: 1.056919]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246571][G train loss: 1.204938]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.246107][G eval loss: 1.024786]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.245800][G train loss: 1.170260]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.245383][G eval loss: 0.977703]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.245085][G train loss: 1.119810]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.244761][G eval loss: 0.916623]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.244458][G train loss: 1.055274]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.244181][G eval loss: 0.850070]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.243915][G train loss: 0.984561]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.243327][G eval loss: 0.783700]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.243169][G train loss: 0.912349]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.242068][G eval loss: 0.731762]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.242021][G train loss: 0.850607]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.241014][G eval loss: 0.724576]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.240839][G train loss: 0.820718]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.241122][G eval loss: 0.740573]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.240740][G train loss: 0.814943]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.242881][G eval loss: 0.748825]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.006 lr_d=0.0008 -> score=0.991706\n",
      "\n",
      "----- 0050: lr_g=0.006, lr_d=0.001 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250019][G eval loss: 1.109989]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250018][G train loss: 1.261662]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.107799]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.259308]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 1.106641]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249978][G train loss: 1.258265]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 1.106604]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.258311]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249962][G eval loss: 1.108120]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249950][G train loss: 1.259855]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249928][G eval loss: 1.108779]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249909][G train loss: 1.260504]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249874][G eval loss: 1.107582]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249847][G train loss: 1.259256]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249799][G eval loss: 1.105060]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249759][G train loss: 1.256667]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249700][G eval loss: 1.102401]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249643][G train loss: 1.253946]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249557][G eval loss: 1.100265]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249476][G train loss: 1.251783]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249367][G eval loss: 1.098976]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249256][G train loss: 1.250456]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249082][G eval loss: 1.098640]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248927][G train loss: 1.250038]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248663][G eval loss: 1.098725]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248457][G train loss: 1.250028]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248068][G eval loss: 1.098228]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247807][G train loss: 1.249381]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247297][G eval loss: 1.097025]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246979][G train loss: 1.247952]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246341][G eval loss: 1.094472]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245950][G train loss: 1.245055]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245087][G eval loss: 1.088677]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244622][G train loss: 1.238795]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243465][G eval loss: 1.078183]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242958][G train loss: 1.227525]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241568][G eval loss: 1.059597]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.241053][G train loss: 1.207345]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.239483][G eval loss: 1.028907]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.239038][G train loss: 1.173834]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.237396][G eval loss: 0.983272]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.237072][G train loss: 1.124368]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.235812][G eval loss: 0.923077]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.235568][G train loss: 1.060130]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.234742][G eval loss: 0.857964]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.234522][G train loss: 0.990192]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.233050][G eval loss: 0.792855]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.233075][G train loss: 0.918718]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.230146][G eval loss: 0.738850]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.230657][G train loss: 0.855301]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.227930][G eval loss: 0.724007]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.228406][G train loss: 0.820108]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231715][G eval loss: 0.727086]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.231345][G train loss: 0.802438]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246353][G eval loss: 0.713814]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.243373][G train loss: 0.776156]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.267740][G eval loss: 0.686490]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.260733][G train loss: 0.746461]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.283876][G eval loss: 0.660743]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.273713][G train loss: 0.724333]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.296443][G eval loss: 0.627050]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.283971][G train loss: 0.691715]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.306071][G eval loss: 0.584274]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.292731][G train loss: 0.650345]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.311761][G eval loss: 0.551161]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.299558][G train loss: 0.619360]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.314693][G eval loss: 0.535204]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.303624][G train loss: 0.604634]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.314955][G eval loss: 0.522424]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.304975][G train loss: 0.591741]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.313157][G eval loss: 0.511024]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.303931][G train loss: 0.582864]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.308796][G eval loss: 0.514047]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.300661][G train loss: 0.588083]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.302651][G eval loss: 0.520134]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.296343][G train loss: 0.592619]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.295987][G eval loss: 0.522932]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.291755][G train loss: 0.590525]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.289634][G eval loss: 0.528266]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.287019][G train loss: 0.591695]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.284703][G eval loss: 0.528848]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.283501][G train loss: 0.593589]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.281363][G eval loss: 0.518781]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.281463][G train loss: 0.588153]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.278333][G eval loss: 0.511494]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.279743][G train loss: 0.585223]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.274487][G eval loss: 0.515514]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.277325][G train loss: 0.590249]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.270825][G eval loss: 0.517348]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.274553][G train loss: 0.590483]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.267877][G eval loss: 0.514967]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.271441][G train loss: 0.585343]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.265195][G eval loss: 0.516220]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.268031][G train loss: 0.582489]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.262728][G eval loss: 0.521531]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.264798][G train loss: 0.582999]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.260733][G eval loss: 0.522083]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.262157][G train loss: 0.580801]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.259114][G eval loss: 0.516448]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.259991][G train loss: 0.576014]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.257565][G eval loss: 0.512849]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.258014][G train loss: 0.575503]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.255811][G eval loss: 0.510549]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.256119][G train loss: 0.575946]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.254228][G eval loss: 0.508452]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.254616][G train loss: 0.574241]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.253037][G eval loss: 0.505528]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.253571][G train loss: 0.570632]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.252257][G eval loss: 0.500530]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.252743][G train loss: 0.564973]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.251810][G eval loss: 0.494442]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.252109][G train loss: 0.558078]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.251489][G eval loss: 0.492288]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.251648][G train loss: 0.554426]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.251168][G eval loss: 0.494999]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.251263][G train loss: 0.555275]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.250788][G eval loss: 0.495880]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.250845][G train loss: 0.556338]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.250543][G eval loss: 0.494896]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.250543][G train loss: 0.557450]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.250370][G eval loss: 0.494045]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.250339][G train loss: 0.556719]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.250345][G eval loss: 0.494936]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.250250][G train loss: 0.552595]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.250305][G eval loss: 0.499492]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.006 lr_d=0.001 -> score=0.749798\n",
      "\n",
      "----- 0050: lr_g=0.006, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250031][G eval loss: 1.108809]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250030][G train loss: 1.260483]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.107601]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.259110]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 1.107319]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.258942]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 1.107637]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249963][G train loss: 1.259344]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249950][G eval loss: 1.109075]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249933][G train loss: 1.260810]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249892][G eval loss: 1.109442]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249866][G train loss: 1.261167]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249805][G eval loss: 1.107930]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249767][G train loss: 1.259605]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249686][G eval loss: 1.105830]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249627][G train loss: 1.257437]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249520][G eval loss: 1.103396]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249433][G train loss: 1.254940]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249274][G eval loss: 1.101408]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249140][G train loss: 1.252926]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248873][G eval loss: 1.100234]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248682][G train loss: 1.251714]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248206][G eval loss: 1.100104]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247958][G train loss: 1.251504]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247308][G eval loss: 1.100451]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246987][G train loss: 1.251753]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246311][G eval loss: 1.100150]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245884][G train loss: 1.251305]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244789][G eval loss: 1.099066]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.244249][G train loss: 1.249985]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242794][G eval loss: 1.096700]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.242139][G train loss: 1.247258]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240193][G eval loss: 1.091810]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.239450][G train loss: 1.241861]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.237062][G eval loss: 1.082213]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.236222][G train loss: 1.231480]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.233320][G eval loss: 1.064296]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.232463][G train loss: 1.211835]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.228961][G eval loss: 1.034343]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.228382][G train loss: 1.178603]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.224592][G eval loss: 0.989005]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.224914][G train loss: 1.128111]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.222269][G eval loss: 0.926815]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.223419][G train loss: 1.060748]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.223484][G eval loss: 0.858801]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.223589][G train loss: 0.989377]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.224040][G eval loss: 0.792147]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.223468][G train loss: 0.917646]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.222501][G eval loss: 0.729867]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.223366][G train loss: 0.846814]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.222782][G eval loss: 0.693874]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.227714][G train loss: 0.789883]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.243496][G eval loss: 0.664174]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.247848][G train loss: 0.743277]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.276256][G eval loss: 0.635444]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.275717][G train loss: 0.708184]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.303390][G eval loss: 0.613217]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.296986][G train loss: 0.685207]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.321182][G eval loss: 0.589966]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.311786][G train loss: 0.664372]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.333633][G eval loss: 0.561979]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.321203][G train loss: 0.637037]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.338086][G eval loss: 0.528622]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.326889][G train loss: 0.602912]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.338435][G eval loss: 0.503122]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.329549][G train loss: 0.575043]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.337221][G eval loss: 0.486056]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.329792][G train loss: 0.555047]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.335386][G eval loss: 0.470137]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.327317][G train loss: 0.537456]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.332838][G eval loss: 0.462600]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.324358][G train loss: 0.527647]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.329391][G eval loss: 0.461453]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.322293][G train loss: 0.523436]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.325315][G eval loss: 0.454048]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.319265][G train loss: 0.512628]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.320535][G eval loss: 0.448207]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.315673][G train loss: 0.505942]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.313457][G eval loss: 0.440953]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.311805][G train loss: 0.502735]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.306450][G eval loss: 0.431620]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.307578][G train loss: 0.498010]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.301104][G eval loss: 0.438457]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.302870][G train loss: 0.504152]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.296238][G eval loss: 0.439280]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.297768][G train loss: 0.505480]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.291410][G eval loss: 0.435801]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.292323][G train loss: 0.503955]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.286328][G eval loss: 0.438772]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.286605][G train loss: 0.503492]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.281111][G eval loss: 0.445329]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.280941][G train loss: 0.502930]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.275733][G eval loss: 0.449340]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0050 lr_g=0.006 lr_d=0.0012 -> score=0.725073\n",
      "\n",
      ">>> Best config for 0050: lr_g=0.004, lr_d=0.0008, score=0.700129\n",
      "\n",
      "========== Grid search for 0056 ==========\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.986775]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.089255]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.975964]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.078692]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.973747]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.076833]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.972862]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.076212]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.972447]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.075883]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.972522]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.075942]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.972790]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249939][G train loss: 1.076137]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 0.972991]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076238]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.972881]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.076022]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 0.972301]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249863][G train loss: 1.075346]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249865][G eval loss: 0.971321]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249825][G train loss: 1.074276]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.970161]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249779][G train loss: 1.073047]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249782][G eval loss: 0.969241]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249723][G train loss: 1.072081]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249727][G eval loss: 0.968739]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249655][G train loss: 1.071552]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249658][G eval loss: 0.968655]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249571][G train loss: 1.071466]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249579][G eval loss: 0.968837]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249474][G train loss: 1.071655]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249471][G eval loss: 0.969054]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249347][G train loss: 1.071879]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249344][G eval loss: 0.969378]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249197][G train loss: 1.072205]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249192][G eval loss: 0.969828]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249019][G train loss: 1.072653]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249014][G eval loss: 0.970294]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248810][G train loss: 1.073109]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248805][G eval loss: 0.970551]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248569][G train loss: 1.073356]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248562][G eval loss: 0.970452]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248293][G train loss: 1.073235]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248266][G eval loss: 0.969860]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.247964][G train loss: 1.072597]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.247929][G eval loss: 0.968859]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247594][G train loss: 1.071515]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247562][G eval loss: 0.967220]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247201][G train loss: 1.069715]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247152][G eval loss: 0.964711]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246785][G train loss: 1.066935]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246697][G eval loss: 0.960592]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246347][G train loss: 1.062362]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246193][G eval loss: 0.954414]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245880][G train loss: 1.055551]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245693][G eval loss: 0.945369]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.245474][G train loss: 1.045663]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245287][G eval loss: 0.931847]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.245271][G train loss: 1.030962]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.245104][G eval loss: 0.914033]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.245451][G train loss: 1.011408]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.245203][G eval loss: 0.895361]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.246055][G train loss: 0.990343]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.245440][G eval loss: 0.881844]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.246715][G train loss: 0.974119]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.245645][G eval loss: 0.877419]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.247181][G train loss: 0.966411]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.245656][G eval loss: 0.873742]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.247307][G train loss: 0.958256]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.245266][G eval loss: 0.862030]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.247062][G train loss: 0.942195]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.244450][G eval loss: 0.840141]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.246444][G train loss: 0.918488]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.243213][G eval loss: 0.814473]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.245380][G train loss: 0.894440]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.241843][G eval loss: 0.793898]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.244298][G train loss: 0.874266]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.240947][G eval loss: 0.769380]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.243627][G train loss: 0.853222]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.241040][G eval loss: 0.740603]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.243822][G train loss: 0.827518]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.241701][G eval loss: 0.711399]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.244008][G train loss: 0.798783]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.243544][G eval loss: 0.683486]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.244937][G train loss: 0.768174]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.245486][G eval loss: 0.659814]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.245884][G train loss: 0.739838]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.245892][G eval loss: 0.637654]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.246295][G train loss: 0.714855]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.245523][G eval loss: 0.611536]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.246104][G train loss: 0.687564]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.244737][G eval loss: 0.579775]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.245640][G train loss: 0.656237]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.242981][G eval loss: 0.547579]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.244731][G train loss: 0.622426]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.240408][G eval loss: 0.517886]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.243111][G train loss: 0.590352]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.237019][G eval loss: 0.498071]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.241237][G train loss: 0.570101]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.234386][G eval loss: 0.493093]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.239712][G train loss: 0.562843]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.234311][G eval loss: 0.493576]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.238707][G train loss: 0.563096]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.237833][G eval loss: 0.488874]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.239409][G train loss: 0.564645]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.242105][G eval loss: 0.480953]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.242725][G train loss: 0.558255]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.246890][G eval loss: 0.470073]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.248338][G train loss: 0.540034]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.254335][G eval loss: 0.454821]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.252720][G train loss: 0.525156]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.264269][G eval loss: 0.435458]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.256675][G train loss: 0.509873]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.266481][G eval loss: 0.435348]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.260045][G train loss: 0.504823]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.267556][G eval loss: 0.440662]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.0005 -> score=0.708218\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.984848]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.087328]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.975677]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.078404]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.974773]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.077859]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.974453]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.077803]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.973803]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249953][G train loss: 1.077240]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249947][G eval loss: 0.973300]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249926][G train loss: 1.076720]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249917][G eval loss: 0.972916]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249887][G train loss: 1.076263]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249875][G eval loss: 0.972530]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249835][G train loss: 1.075776]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249818][G eval loss: 0.972123]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249762][G train loss: 1.075264]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249741][G eval loss: 0.971434]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249665][G train loss: 1.074477]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249636][G eval loss: 0.970513]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249535][G train loss: 1.073466]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249500][G eval loss: 0.969577]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249365][G train loss: 1.072461]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249325][G eval loss: 0.968915]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249148][G train loss: 1.071752]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249094][G eval loss: 0.968727]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248867][G train loss: 1.071537]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248790][G eval loss: 0.969056]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248500][G train loss: 1.071860]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248386][G eval loss: 0.969812]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248019][G train loss: 1.072619]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247868][G eval loss: 0.970860]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247404][G train loss: 1.073669]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247288][G eval loss: 0.971835]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246706][G train loss: 1.074647]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246423][G eval loss: 0.972387]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245722][G train loss: 1.075181]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245291][G eval loss: 0.972911]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244482][G train loss: 1.075652]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.243985][G eval loss: 0.973293]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243072][G train loss: 1.075963]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242485][G eval loss: 0.973550]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241470][G train loss: 1.076116]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.240782][G eval loss: 0.973705]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239669][G train loss: 1.076073]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238884][G eval loss: 0.973830]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237706][G train loss: 1.075810]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236863][G eval loss: 0.973796]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235675][G train loss: 1.075098]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234806][G eval loss: 0.972839]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.233712][G train loss: 1.072989]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.232761][G eval loss: 0.969082]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.231886][G train loss: 1.067488]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.231113][G eval loss: 0.961081]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.230819][G train loss: 1.056575]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.230403][G eval loss: 0.947883]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.231742][G train loss: 1.038087]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.230465][G eval loss: 0.929777]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.233492][G train loss: 1.014470]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.231200][G eval loss: 0.909043]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.235480][G train loss: 0.989017]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.235745][G eval loss: 0.885975]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.238279][G train loss: 0.967363]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.240784][G eval loss: 0.875062]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.242252][G train loss: 0.955450]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.242788][G eval loss: 0.877915]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.245024][G train loss: 0.954848]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.243219][G eval loss: 0.881607]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.246681][G train loss: 0.955087]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.241874][G eval loss: 0.881142]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.248794][G train loss: 0.947589]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.233198][G eval loss: 0.883169]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.249183][G train loss: 0.935996]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.229005][G eval loss: 0.870633]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.247419][G train loss: 0.918323]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.225043][G eval loss: 0.853904]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.241932][G train loss: 0.900990]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.220607][G eval loss: 0.837426]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.238162][G train loss: 0.878793]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.215969][G eval loss: 0.821863]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.234581][G train loss: 0.858582]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.212448][G eval loss: 0.808102]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.231260][G train loss: 0.843136]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.212599][G eval loss: 0.787448]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.230720][G train loss: 0.825080]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.220784][G eval loss: 0.755982]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.235822][G train loss: 0.798802]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.231044][G eval loss: 0.723325]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.239106][G train loss: 0.777401]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.243640][G eval loss: 0.697724]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.245269][G train loss: 0.750687]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.247907][G eval loss: 0.690029]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.254809][G train loss: 0.726992]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.257222][G eval loss: 0.678162]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.258913][G train loss: 0.716931]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.258372][G eval loss: 0.674579]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.258473][G train loss: 0.713458]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.258907][G eval loss: 0.668751]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.257740][G train loss: 0.708102]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.259414][G eval loss: 0.657853]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.257837][G train loss: 0.696369]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.259113][G eval loss: 0.642307]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.258631][G train loss: 0.676877]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.259626][G eval loss: 0.617449]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.257860][G train loss: 0.655402]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.267260][G eval loss: 0.570233]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.256481][G train loss: 0.630167]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.270270][G eval loss: 0.539071]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.254977][G train loss: 0.603941]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.271633][G eval loss: 0.513165]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.253623][G train loss: 0.580870]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.273057][G eval loss: 0.496234]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.253284][G train loss: 0.562729]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.274336][G eval loss: 0.488495]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.254345][G train loss: 0.551770]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.275702][G eval loss: 0.489834]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.256656][G train loss: 0.552406]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.278095][G eval loss: 0.487520]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.258848][G train loss: 0.560423]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.283496][G eval loss: 0.481337]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.259869][G train loss: 0.565866]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.288989][G eval loss: 0.480085]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.261594][G train loss: 0.568335]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.289092][G eval loss: 0.481458]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.261544][G train loss: 0.568594]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.288968][G eval loss: 0.477374]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.260974][G train loss: 0.561682]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.288666][G eval loss: 0.470182]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.260039][G train loss: 0.551658]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.288192][G eval loss: 0.461762]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.258756][G train loss: 0.537878]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.287600][G eval loss: 0.450810]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.257200][G train loss: 0.523786]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.286771][G eval loss: 0.442139]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.255546][G train loss: 0.515316]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.285618][G eval loss: 0.438561]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.254105][G train loss: 0.514201]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.284115][G eval loss: 0.442519]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.252982][G train loss: 0.518914]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.281880][G eval loss: 0.451205]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.251658][G train loss: 0.526520]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.279233][G eval loss: 0.459441]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.250601][G train loss: 0.533559]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.276302][G eval loss: 0.464226]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.249966][G train loss: 0.537037]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.273094][G eval loss: 0.467690]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.249482][G train loss: 0.539174]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.270016][G eval loss: 0.473023]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.248930][G train loss: 0.542244]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.267961][G eval loss: 0.481602]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.248952][G train loss: 0.548020]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.266210][G eval loss: 0.496008]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.248894][G train loss: 0.559980]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.265691][G eval loss: 0.511525]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.249539][G train loss: 0.573870]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.265316][G eval loss: 0.521812]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250150][G train loss: 0.583338]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.264377][G eval loss: 0.526412]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250368][G train loss: 0.587082]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.262835][G eval loss: 0.524544]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.250341][G train loss: 0.584237]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.260814][G eval loss: 0.517876]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.249950][G train loss: 0.577385]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.258824][G eval loss: 0.508390]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.249259][G train loss: 0.569745]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.256529][G eval loss: 0.500352]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.248375][G train loss: 0.563678]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.253837][G eval loss: 0.493902]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.247554][G train loss: 0.558582]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.251299][G eval loss: 0.486435]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.246983][G train loss: 0.552496]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.249195][G eval loss: 0.479598]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.246724][G train loss: 0.546013]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.247819][G eval loss: 0.473443]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.246553][G train loss: 0.539878]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.247107][G eval loss: 0.468986]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.246263][G train loss: 0.534760]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.246424][G eval loss: 0.467152]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.245745][G train loss: 0.531318]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.245480][G eval loss: 0.468033]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.244893][G train loss: 0.530594]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.244092][G eval loss: 0.471415]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.0008 -> score=0.715508\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.001 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250019][G eval loss: 0.983671]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 1.086151]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.975500]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 1.078228]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.975483]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.078570]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.975515]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249961][G train loss: 1.078865]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 0.974746]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249932][G train loss: 1.078182]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249916][G eval loss: 0.973927]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249884][G train loss: 1.077347]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 0.973183]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249815][G train loss: 1.076530]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249780][G eval loss: 0.972630]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249713][G train loss: 1.075876]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249663][G eval loss: 0.972321]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249567][G train loss: 1.075460]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249505][G eval loss: 0.971728]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249371][G train loss: 1.074770]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249290][G eval loss: 0.970861]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249107][G train loss: 1.073813]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248988][G eval loss: 0.969977]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248737][G train loss: 1.072858]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248568][G eval loss: 0.969446]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248229][G train loss: 1.072279]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248002][G eval loss: 0.969575]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247551][G train loss: 1.072376]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247234][G eval loss: 0.970323]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246638][G train loss: 1.073113]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246348][G eval loss: 0.971379]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245572][G train loss: 1.074170]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245012][G eval loss: 0.972408]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244060][G train loss: 1.075191]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243198][G eval loss: 0.973818]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242062][G train loss: 1.076572]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.240950][G eval loss: 0.975577]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239588][G train loss: 1.078292]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238189][G eval loss: 0.977667]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236556][G train loss: 1.080314]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235031][G eval loss: 0.979633]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.233078][G train loss: 1.082215]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.231336][G eval loss: 0.981535]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.229063][G train loss: 1.083945]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.227046][G eval loss: 0.983414]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.224454][G train loss: 1.085498]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.222085][G eval loss: 0.985291]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.219219][G train loss: 1.086773]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.216788][G eval loss: 0.986633]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.213764][G train loss: 1.086910]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.211721][G eval loss: 0.986273]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.209496][G train loss: 1.082752]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.208338][G eval loss: 0.980257]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.208778][G train loss: 1.068933]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.207655][G eval loss: 0.965686]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.214243][G train loss: 1.041240]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.210259][G eval loss: 0.940868]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.220966][G train loss: 1.009028]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.223193][G eval loss: 0.897447]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.229051][G train loss: 0.973346]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.232912][G eval loss: 0.868837]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.239074][G train loss: 0.941063]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.241091][G eval loss: 0.857216]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.247985][G train loss: 0.924648]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.247161][G eval loss: 0.862603]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.256794][G train loss: 0.922957]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.250931][G eval loss: 0.873654]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.265013][G train loss: 0.926819]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.253570][G eval loss: 0.879112]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.269410][G train loss: 0.931766]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.259530][G eval loss: 0.872709]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.272588][G train loss: 0.931375]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.266910][G eval loss: 0.858560]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.272260][G train loss: 0.924792]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.252598][G eval loss: 0.866149]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.273816][G train loss: 0.903535]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.251274][G eval loss: 0.855032]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.270695][G train loss: 0.888863]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.251025][G eval loss: 0.831885]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.264206][G train loss: 0.875760]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.250770][G eval loss: 0.809874]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.259977][G train loss: 0.860428]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.253960][G eval loss: 0.787171]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.259890][G train loss: 0.840272]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.254001][G eval loss: 0.776792]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.258442][G train loss: 0.826653]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.252418][G eval loss: 0.767043]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.256509][G train loss: 0.813374]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.262562][G eval loss: 0.742723]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.266181][G train loss: 0.781678]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.263409][G eval loss: 0.737210]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.264132][G train loss: 0.776476]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.263420][G eval loss: 0.734782]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.259189][G train loss: 0.778808]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.264185][G eval loss: 0.731386]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.256972][G train loss: 0.775007]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.265934][G eval loss: 0.732559]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.260970][G train loss: 0.768639]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.266873][G eval loss: 0.736927]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.001 -> score=1.003800\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250031][G eval loss: 0.982492]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250028][G train loss: 1.084972]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.975300]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.078028]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.976157]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.079244]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.976549]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249951][G train loss: 1.079899]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249934][G eval loss: 0.975718]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249906][G train loss: 1.079154]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249875][G eval loss: 0.974618]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249831][G train loss: 1.078038]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249785][G eval loss: 0.973566]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249721][G train loss: 1.076913]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249654][G eval loss: 0.973439]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249559][G train loss: 1.076685]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249476][G eval loss: 0.973224]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249336][G train loss: 1.076362]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249210][G eval loss: 0.972683]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248996][G train loss: 1.075724]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248769][G eval loss: 0.971812]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248468][G train loss: 1.074763]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248109][G eval loss: 0.971014]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247703][G train loss: 1.073891]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247265][G eval loss: 0.970855]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246711][G train loss: 1.073680]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246334][G eval loss: 0.971327]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245582][G train loss: 1.074118]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244886][G eval loss: 0.972279]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243897][G train loss: 1.075052]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242916][G eval loss: 0.973792]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241649][G train loss: 1.076553]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240309][G eval loss: 0.975788]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238705][G train loss: 1.078535]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.237012][G eval loss: 0.978193]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.234987][G train loss: 1.080923]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232866][G eval loss: 0.980983]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230375][G train loss: 1.083610]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227654][G eval loss: 0.984635]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.224661][G train loss: 1.087171]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221643][G eval loss: 0.988394]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.218113][G train loss: 1.090822]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.214780][G eval loss: 0.992449]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.210570][G train loss: 1.094641]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.206813][G eval loss: 0.997282]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.201887][G train loss: 1.098915]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.197768][G eval loss: 1.002838]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.192232][G train loss: 1.103476]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.188391][G eval loss: 1.007678]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.182548][G train loss: 1.106460]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.181355][G eval loss: 1.006413]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.179561][G train loss: 1.093710]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.180811][G eval loss: 0.993322]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.194282][G train loss: 1.052959]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.189000][G eval loss: 0.961957]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.211850][G train loss: 1.012021]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.218286][G eval loss: 0.904550]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.230460][G train loss: 0.971210]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.234528][G eval loss: 0.876100]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.246055][G train loss: 0.938073]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.244629][G eval loss: 0.863359]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.260997][G train loss: 0.913223]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.253729][G eval loss: 0.860654]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.277117][G train loss: 0.902031]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.283320][G eval loss: 0.845027]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.288681][G train loss: 0.910226]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.297678][G eval loss: 0.865268]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.292932][G train loss: 0.927107]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.307809][G eval loss: 0.883651]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.301353][G train loss: 0.927920]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.313523][G eval loss: 0.891711]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.305036][G train loss: 0.928327]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.315643][G eval loss: 0.884165]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.303987][G train loss: 0.921614]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.314020][G eval loss: 0.866900]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.301515][G train loss: 0.903662]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.308698][G eval loss: 0.845078]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.300933][G train loss: 0.879652]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.278290][G eval loss: 0.859993]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.291354][G train loss: 0.869546]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.267840][G eval loss: 0.849809]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.283778][G train loss: 0.857591]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.255064][G eval loss: 0.848794]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.271115][G train loss: 0.854308]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.246463][G eval loss: 0.846828]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.259940][G train loss: 0.860623]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.258635][G eval loss: 0.812317]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.255182][G train loss: 0.859122]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.260642][G eval loss: 0.801171]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.255606][G train loss: 0.849249]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.257193][G eval loss: 0.800489]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.254213][G train loss: 0.845366]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.254490][G eval loss: 0.799858]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.256982][G train loss: 0.834624]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.254187][G eval loss: 0.799883]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.256980][G train loss: 0.832787]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.253386][G eval loss: 0.802135]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.253592][G train loss: 0.837647]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.252402][G eval loss: 0.806615]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.254773][G train loss: 0.835467]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.253124][G eval loss: 0.812401]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.255446][G train loss: 0.843933]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.253440][G eval loss: 0.823536]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.252348][G train loss: 0.859112]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.253176][G eval loss: 0.834612]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.249598][G train loss: 0.872380]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.252351][G eval loss: 0.843728]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.247677][G train loss: 0.881551]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.251882][G eval loss: 0.852513]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.248769][G train loss: 0.884585]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.251769][G eval loss: 0.860969]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.247009][G train loss: 0.892415]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.251600][G eval loss: 0.863507]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.245758][G train loss: 0.895395]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.251688][G eval loss: 0.862955]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.244435][G train loss: 0.896192]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.252176][G eval loss: 0.860666]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.242677][G train loss: 0.898178]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.253581][G eval loss: 0.856699]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.241257][G train loss: 0.899638]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.255878][G eval loss: 0.854005]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.240859][G train loss: 0.896526]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.259754][G eval loss: 0.850537]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.240424][G train loss: 0.891708]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.263598][G eval loss: 0.843930]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.239966][G train loss: 0.885352]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.268164][G eval loss: 0.836090]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.237211][G train loss: 0.889773]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.272189][G eval loss: 0.825278]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.237470][G train loss: 0.879691]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.275305][G eval loss: 0.810700]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.238168][G train loss: 0.867056]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.278525][G eval loss: 0.795248]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.232230][G train loss: 0.880980]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.280447][G eval loss: 0.779052]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.231906][G train loss: 0.875028]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.281782][G eval loss: 0.762362]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.230106][G train loss: 0.873473]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.282414][G eval loss: 0.746919]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.229856][G train loss: 0.864236]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.282191][G eval loss: 0.734509]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.232606][G train loss: 0.856225]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.278449][G eval loss: 0.740470]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.231112][G train loss: 0.858129]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.276968][G eval loss: 0.745824]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.234007][G train loss: 0.864435]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.275403][G eval loss: 0.749682]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.233933][G train loss: 0.867134]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.274165][G eval loss: 0.759505]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.234663][G train loss: 0.855679]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.273508][G eval loss: 0.755186]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.233191][G train loss: 0.859714]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.272473][G eval loss: 0.758004]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.233554][G train loss: 0.846075]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.271489][G eval loss: 0.756407]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.227886][G train loss: 0.838047]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.279820][G eval loss: 0.717740]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.224029][G train loss: 0.833865]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.278922][G eval loss: 0.714870]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.227115][G train loss: 0.813887]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.277758][G eval loss: 0.716681]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.227198][G train loss: 0.799802]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.276387][G eval loss: 0.708922]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.229092][G train loss: 0.769142]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.275247][G eval loss: 0.695058]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.228971][G train loss: 0.749268]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.279484][G eval loss: 0.666184]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.232405][G train loss: 0.714475]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.280464][G eval loss: 0.643362]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.232560][G train loss: 0.695427]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.283572][G eval loss: 0.588146]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.233170][G train loss: 0.680039]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.293262][G eval loss: 0.538539]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.237112][G train loss: 0.652892]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.295487][G eval loss: 0.523818]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.244091][G train loss: 0.609582]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.301038][G eval loss: 0.510725]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.246108][G train loss: 0.603465]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.302873][G eval loss: 0.505277]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.246107][G train loss: 0.601820]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.305346][G eval loss: 0.503138]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.248390][G train loss: 0.586810]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.306190][G eval loss: 0.498938]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.248336][G train loss: 0.580089]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.306894][G eval loss: 0.493418]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.248424][G train loss: 0.572059]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.306515][G eval loss: 0.486622]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.248290][G train loss: 0.560078]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.303738][G eval loss: 0.472102]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.248132][G train loss: 0.544912]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.278411][G eval loss: 0.457630]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.250669][G train loss: 0.516518]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.279076][G eval loss: 0.447496]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.250614][G train loss: 0.505881]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.279768][G eval loss: 0.443794]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.250628][G train loss: 0.498110]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.280407][G eval loss: 0.442896]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.247885][G train loss: 0.511607]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.280995][G eval loss: 0.444460]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.247874][G train loss: 0.512926]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.281367][G eval loss: 0.447158]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.247844][G train loss: 0.514336]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.281668][G eval loss: 0.446497]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.247886][G train loss: 0.515132]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.281888][G eval loss: 0.447004]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.250621][G train loss: 0.501216]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.282067][G eval loss: 0.449108]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.250624][G train loss: 0.499805]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.282306][G eval loss: 0.446893]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.250628][G train loss: 0.496821]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.282500][G eval loss: 0.442192]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.250624][G train loss: 0.492221]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.282639][G eval loss: 0.439860]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.250614][G train loss: 0.489124]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.282704][G eval loss: 0.438349]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.250603][G train loss: 0.486505]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.282701][G eval loss: 0.440504]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.250586][G train loss: 0.484961]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.282660][G eval loss: 0.445303]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.0012 -> score=0.727963\n",
      "\n",
      "----- 0056: lr_g=0.0045, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.984913]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.087403]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.975149]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.077962]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.973224]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.076433]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.972277]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.075698]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.972302]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.075748]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.972886]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.076269]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.973428]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249940][G train loss: 1.076703]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 0.973547]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076703]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.972985]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.076031]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249894][G eval loss: 0.971760]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249863][G train loss: 1.074708]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249864][G eval loss: 0.970271]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249826][G train loss: 1.073149]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.969101]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249780][G train loss: 1.071937]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249783][G eval loss: 0.968446]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249725][G train loss: 1.071260]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249726][G eval loss: 0.968242]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249655][G train loss: 1.071054]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249652][G eval loss: 0.968242]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249566][G train loss: 1.071053]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249569][G eval loss: 0.968438]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249465][G train loss: 1.071246]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249454][G eval loss: 0.968772]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249331][G train loss: 1.071575]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249323][G eval loss: 0.969238]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249177][G train loss: 1.072042]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249169][G eval loss: 0.969555]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.248999][G train loss: 1.072361]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249000][G eval loss: 0.969434]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248802][G train loss: 1.072225]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248806][G eval loss: 0.968757]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248582][G train loss: 1.071505]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248578][G eval loss: 0.967520]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248333][G train loss: 1.070182]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248299][G eval loss: 0.965627]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248041][G train loss: 1.068132]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.247976][G eval loss: 0.962486]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247717][G train loss: 1.064740]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247623][G eval loss: 0.956877]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247366][G train loss: 1.058716]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247267][G eval loss: 0.947977]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.247040][G train loss: 1.049217]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.247000][G eval loss: 0.933791]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246885][G train loss: 1.034126]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246889][G eval loss: 0.914802]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.247001][G train loss: 1.013696]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.246956][G eval loss: 0.896211]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.247405][G train loss: 0.992948]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.247089][G eval loss: 0.885946]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.247781][G train loss: 0.980265]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.247089][G eval loss: 0.883466]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.247960][G train loss: 0.975220]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.246821][G eval loss: 0.875852]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.247878][G train loss: 0.964341]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.246316][G eval loss: 0.857509]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.247499][G train loss: 0.943646]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.245516][G eval loss: 0.832472]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.246876][G train loss: 0.916827]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.244511][G eval loss: 0.807551]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.246067][G train loss: 0.891275]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.243563][G eval loss: 0.784439]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.245223][G train loss: 0.867971]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.242969][G eval loss: 0.756652]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.244580][G train loss: 0.841864]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.242918][G eval loss: 0.723503]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.244789][G train loss: 0.809697]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.243653][G eval loss: 0.690182]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.245623][G train loss: 0.776210]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.245108][G eval loss: 0.658314]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.246380][G train loss: 0.744865]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.246043][G eval loss: 0.629406]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.246757][G train loss: 0.715647]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.245728][G eval loss: 0.598259]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.246499][G train loss: 0.683791]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.244976][G eval loss: 0.561444]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.245665][G train loss: 0.648792]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.243589][G eval loss: 0.525369]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.244218][G train loss: 0.613906]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.241149][G eval loss: 0.505946]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.242012][G train loss: 0.590439]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.238520][G eval loss: 0.508830]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.239422][G train loss: 0.585569]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.236989][G eval loss: 0.519911]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.0045 lr_d=0.0005 -> score=0.756900\n",
      "\n",
      "----- 0056: lr_g=0.0045, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.982986]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.085476]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.974863]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.077676]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.974255]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.077463]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.973873]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.077293]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.973661]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.077107]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 0.973662]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249929][G train loss: 1.077045]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.973547]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249890][G train loss: 1.076822]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249877][G eval loss: 0.973077]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249838][G train loss: 1.076232]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249820][G eval loss: 0.972211]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249766][G train loss: 1.075256]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249746][G eval loss: 0.970873]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249673][G train loss: 1.073819]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249643][G eval loss: 0.969444]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249546][G train loss: 1.072320]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249512][G eval loss: 0.968489]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249381][G train loss: 1.071322]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249339][G eval loss: 0.968090]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249168][G train loss: 1.070900]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249109][G eval loss: 0.968213]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248888][G train loss: 1.071021]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248797][G eval loss: 0.968653]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248513][G train loss: 1.071456]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248383][G eval loss: 0.969451]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248024][G train loss: 1.072246]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247848][G eval loss: 0.970610]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247398][G train loss: 1.073393]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247286][G eval loss: 0.971650]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246723][G train loss: 1.074432]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246420][G eval loss: 0.972110]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245744][G train loss: 1.074868]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245335][G eval loss: 0.972047]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244566][G train loss: 1.074724]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244150][G eval loss: 0.971413]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243308][G train loss: 1.073933]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242846][G eval loss: 0.970452]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241980][G train loss: 1.072659]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241393][G eval loss: 0.969119]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240566][G train loss: 1.070779]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239765][G eval loss: 0.966420]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.239049][G train loss: 1.067215]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.238162][G eval loss: 0.960427]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.237637][G train loss: 1.059876]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.236988][G eval loss: 0.948851]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.237039][G train loss: 1.045902]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.236701][G eval loss: 0.930729]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.238035][G train loss: 1.023641]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.236731][G eval loss: 0.909579]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.239101][G train loss: 0.998454]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.237689][G eval loss: 0.891396]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.240326][G train loss: 0.977686]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.240804][G eval loss: 0.882499]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.242189][G train loss: 0.968739]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.242053][G eval loss: 0.884556]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.243590][G train loss: 0.967689]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.240501][G eval loss: 0.884251]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.243872][G train loss: 0.962329]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.235753][G eval loss: 0.878774]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.243620][G train loss: 0.946349]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.232654][G eval loss: 0.860050]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.241796][G train loss: 0.922476]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.229237][G eval loss: 0.836871]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.239088][G train loss: 0.895036]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.225584][G eval loss: 0.813248]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.236212][G train loss: 0.869089]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.222488][G eval loss: 0.791499]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.233194][G train loss: 0.847057]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.220890][G eval loss: 0.766442]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.231872][G train loss: 0.822385]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.225809][G eval loss: 0.728236]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.235999][G train loss: 0.788134]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.232087][G eval loss: 0.696781]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.239529][G train loss: 0.758863]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.244554][G eval loss: 0.662693]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.245307][G train loss: 0.731141]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.250439][G eval loss: 0.645938]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.252675][G train loss: 0.707476]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.257454][G eval loss: 0.630455]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.255905][G train loss: 0.691573]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.259699][G eval loss: 0.615286]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.255766][G train loss: 0.678870]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.259961][G eval loss: 0.596041]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.255196][G train loss: 0.660538]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.259512][G eval loss: 0.570294]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.254451][G train loss: 0.636584]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.259380][G eval loss: 0.539583]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.253409][G train loss: 0.610456]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.260907][G eval loss: 0.504001]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.251170][G train loss: 0.585979]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.262656][G eval loss: 0.473830]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.249355][G train loss: 0.566366]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.265120][G eval loss: 0.456278]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.250023][G train loss: 0.548896]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.268987][G eval loss: 0.450919]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.253031][G train loss: 0.537827]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.274122][G eval loss: 0.441956]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.258502][G train loss: 0.525454]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.281976][G eval loss: 0.432858]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.262737][G train loss: 0.516339]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.282881][G eval loss: 0.437852]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.263492][G train loss: 0.510743]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.282998][G eval loss: 0.439851]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.0045 lr_d=0.0008 -> score=0.722848\n",
      "\n",
      "----- 0056: lr_g=0.0045, lr_d=0.001 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250019][G eval loss: 0.981809]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 1.084299]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.974686]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.077499]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.974966]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.078175]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.974936]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249962][G train loss: 1.078356]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.974602]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249935][G train loss: 1.078048]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.974287]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249889][G train loss: 1.077671]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249862][G eval loss: 0.973813]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249818][G train loss: 1.077088]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249780][G eval loss: 0.973174]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249715][G train loss: 1.076328]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249666][G eval loss: 0.972302]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249573][G train loss: 1.075346]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249510][G eval loss: 0.970980]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249379][G train loss: 1.073926]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249297][G eval loss: 0.969565]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249117][G train loss: 1.072440]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248995][G eval loss: 0.968674]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248748][G train loss: 1.071505]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248574][G eval loss: 0.968494]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248241][G train loss: 1.071298]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247992][G eval loss: 0.969087]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247548][G train loss: 1.071884]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247210][G eval loss: 0.970225]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246614][G train loss: 1.073018]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246169][G eval loss: 0.970846]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245416][G train loss: 1.073623]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.244686][G eval loss: 0.971756]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.243787][G train loss: 1.074486]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.242825][G eval loss: 0.973173]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.241770][G train loss: 1.075855]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.240624][G eval loss: 0.974870]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239360][G train loss: 1.077496]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238070][G eval loss: 0.976471]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236555][G train loss: 1.078973]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235202][G eval loss: 0.977820]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.233424][G train loss: 1.080076]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.232062][G eval loss: 0.978859]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.230074][G train loss: 1.080520]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.228856][G eval loss: 0.979059]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.226932][G train loss: 1.079234]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.225888][G eval loss: 0.976033]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.224490][G train loss: 1.073344]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.224055][G eval loss: 0.966131]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.224464][G train loss: 1.057604]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.224379][G eval loss: 0.947314]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.227748][G train loss: 1.030536]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.227899][G eval loss: 0.917794]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.231611][G train loss: 0.998372]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.234956][G eval loss: 0.885303]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.236032][G train loss: 0.968168]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.240682][G eval loss: 0.869641]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.241947][G train loss: 0.948930]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.244125][G eval loss: 0.873984]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.246911][G train loss: 0.948099]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.244690][G eval loss: 0.883586]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.249068][G train loss: 0.953441]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.242927][G eval loss: 0.889490]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.249094][G train loss: 0.955504]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.240301][G eval loss: 0.889239]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249199][G train loss: 0.950493]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.236958][G eval loss: 0.881559]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.246979][G train loss: 0.940747]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.232589][G eval loss: 0.866471]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.243670][G train loss: 0.925584]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.214842][G eval loss: 0.877817]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.240933][G train loss: 0.906146]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.210258][G eval loss: 0.869924]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.233215][G train loss: 0.900222]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.205927][G eval loss: 0.861586]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.228936][G train loss: 0.889013]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.202989][G eval loss: 0.853712]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.225004][G train loss: 0.879846]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.204341][G eval loss: 0.840485]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.222296][G train loss: 0.870058]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.210708][G eval loss: 0.818212]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.227826][G train loss: 0.846228]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.223949][G eval loss: 0.783368]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.236543][G train loss: 0.822906]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.244891][G eval loss: 0.747195]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.245285][G train loss: 0.800728]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 0.747908]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.258577][G train loss: 0.774149]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.261329][G eval loss: 0.735468]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.265672][G train loss: 0.766214]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.260914][G eval loss: 0.742798]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.265068][G train loss: 0.768731]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.259500][G eval loss: 0.743661]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.259988][G train loss: 0.772272]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.258453][G eval loss: 0.741580]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.256137][G train loss: 0.772424]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.257736][G eval loss: 0.734670]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.255884][G train loss: 0.761734]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.257073][G eval loss: 0.721999]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.253667][G train loss: 0.751462]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.256355][G eval loss: 0.703778]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.249619][G train loss: 0.737506]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.256124][G eval loss: 0.682899]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.246574][G train loss: 0.721234]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.258290][G eval loss: 0.660022]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.244270][G train loss: 0.701867]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.261854][G eval loss: 0.637336]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.242245][G train loss: 0.682338]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.265306][G eval loss: 0.612249]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.241082][G train loss: 0.664052]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.268517][G eval loss: 0.594296]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.240959][G train loss: 0.650010]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.270400][G eval loss: 0.584808]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.242962][G train loss: 0.641466]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.262058][G eval loss: 0.607670]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.249431][G train loss: 0.629440]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.264953][G eval loss: 0.587937]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.248394][G train loss: 0.640055]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.283266][G eval loss: 0.523644]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.249903][G train loss: 0.635096]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.295864][G eval loss: 0.500427]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.254709][G train loss: 0.620342]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.295819][G eval loss: 0.504703]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.258698][G train loss: 0.600316]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.295651][G eval loss: 0.496645]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.259268][G train loss: 0.590846]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.295439][G eval loss: 0.484515]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.260660][G train loss: 0.580037]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.295246][G eval loss: 0.474601]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.260893][G train loss: 0.568960]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.295048][G eval loss: 0.469022]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.260575][G train loss: 0.557528]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.294777][G eval loss: 0.465122]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.259667][G train loss: 0.548358]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.294456][G eval loss: 0.463204]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.258335][G train loss: 0.542443]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.294112][G eval loss: 0.461794]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.257215][G train loss: 0.538351]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.293535][G eval loss: 0.461899]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.256301][G train loss: 0.536716]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.292427][G eval loss: 0.463790]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.255256][G train loss: 0.537612]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.290549][G eval loss: 0.466115]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.254003][G train loss: 0.539276]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.287873][G eval loss: 0.468159]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.252051][G train loss: 0.540121]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.284505][G eval loss: 0.468618]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.250902][G train loss: 0.539561]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.281515][G eval loss: 0.467659]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.251125][G train loss: 0.537757]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.276592][G eval loss: 0.468079]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.251105][G train loss: 0.536536]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.267657][G eval loss: 0.471883]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.250885][G train loss: 0.538319]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.251359][G eval loss: 0.478223]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.250723][G train loss: 0.543183]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.250678][G eval loss: 0.484445]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250699][G train loss: 0.548624]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.250663][G eval loss: 0.487792]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250679][G train loss: 0.551711]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.250583][G eval loss: 0.486609]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.250563][G train loss: 0.550594]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.250416][G eval loss: 0.481396]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.250353][G train loss: 0.545850]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.250403][G eval loss: 0.474222]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.250337][G train loss: 0.539659]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.250413][G eval loss: 0.467584]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.250349][G train loss: 0.534598]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.250422][G eval loss: 0.464214]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.250360][G train loss: 0.532478]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.250415][G eval loss: 0.464354]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.250360][G train loss: 0.532890]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.250433][G eval loss: 0.465810]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.250383][G train loss: 0.534249]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.250448][G eval loss: 0.467442]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.250403][G train loss: 0.535021]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.250445][G eval loss: 0.469186]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.250399][G train loss: 0.535095]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.250495][G eval loss: 0.471059]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.250452][G train loss: 0.534448]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.250538][G eval loss: 0.473143]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.250500][G train loss: 0.533951]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.250567][G eval loss: 0.475721]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.0045 lr_d=0.001 -> score=0.726289\n",
      "\n",
      "----- 0056: lr_g=0.0045, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250031][G eval loss: 0.980629]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250029][G train loss: 1.083119]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.974487]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.077300]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.975641]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.078850]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.975971]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249952][G train loss: 1.079391]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 0.975570]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249912][G train loss: 1.079016]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249880][G eval loss: 0.974967]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249838][G train loss: 1.078350]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249791][G eval loss: 0.974179]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249729][G train loss: 1.077453]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249658][G eval loss: 0.973932]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249565][G train loss: 1.077086]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249482][G eval loss: 0.973221]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249346][G train loss: 1.076265]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249221][G eval loss: 0.972012]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249017][G train loss: 1.074956]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248802][G eval loss: 0.970630]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248504][G train loss: 1.073502]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248148][G eval loss: 0.969825]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247746][G train loss: 1.072650]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247286][G eval loss: 0.969898]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246741][G train loss: 1.072693]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246320][G eval loss: 0.970689]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245586][G train loss: 1.073474]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244819][G eval loss: 0.971745]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243858][G train loss: 1.074505]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242759][G eval loss: 0.973318]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241528][G train loss: 1.076053]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240073][G eval loss: 0.975444]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238520][G train loss: 1.078137]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236799][G eval loss: 0.977882]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.234835][G train loss: 1.080551]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232686][G eval loss: 0.980609]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230293][G train loss: 1.083126]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227677][G eval loss: 0.983624]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.224847][G train loss: 1.085925]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221969][G eval loss: 0.986376]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.218715][G train loss: 1.088323]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.215618][G eval loss: 0.989146]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.211944][G train loss: 1.090234]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.209221][G eval loss: 0.990560]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.205809][G train loss: 1.088910]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.204725][G eval loss: 0.986187]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.204539][G train loss: 1.075568]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.204652][G eval loss: 0.969138]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.212937][G train loss: 1.041971]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.213169][G eval loss: 0.930599]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.222289][G train loss: 1.003558]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.227989][G eval loss: 0.886247]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.233846][G train loss: 0.961559]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.237781][G eval loss: 0.862488]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.244867][G train loss: 0.932073]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.244805][G eval loss: 0.861849]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.254990][G train loss: 0.922910]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.249268][G eval loss: 0.875284]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.0045 lr_d=0.0012 -> score=1.124552\n",
      "\n",
      "----- 0056: lr_g=0.005, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.983262]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.085763]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.974966]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.077878]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.972596]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.075903]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.971803]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.075256]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.972467]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.075900]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.973388]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.076722]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.973974]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249940][G train loss: 1.077180]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 0.973794]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076877]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.972600]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.075571]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249894][G eval loss: 0.970789]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249864][G train loss: 1.073671]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249865][G eval loss: 0.969249]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249827][G train loss: 1.072073]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249829][G eval loss: 0.968380]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249782][G train loss: 1.071177]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249782][G eval loss: 0.968018]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249725][G train loss: 1.070812]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249720][G eval loss: 0.967883]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249650][G train loss: 1.070683]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249641][G eval loss: 0.968051]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249556][G train loss: 1.070858]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249558][G eval loss: 0.968512]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249455][G train loss: 1.071324]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249444][G eval loss: 0.968845]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249323][G train loss: 1.071671]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249320][G eval loss: 0.968839]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249177][G train loss: 1.071693]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249176][G eval loss: 0.968364]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249011][G train loss: 1.071224]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249022][G eval loss: 0.967345]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248834][G train loss: 1.070163]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248842][G eval loss: 0.965676]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248638][G train loss: 1.068394]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248619][G eval loss: 0.962746]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248415][G train loss: 1.065264]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248335][G eval loss: 0.957577]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248149][G train loss: 1.059786]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248056][G eval loss: 0.948302]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247909][G train loss: 1.049943]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247864][G eval loss: 0.932287]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247809][G train loss: 1.033020]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247839][G eval loss: 0.912860]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.247940][G train loss: 1.012268]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.248026][G eval loss: 0.899421]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.248268][G train loss: 0.997239]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.248261][G eval loss: 0.899520]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.248500][G train loss: 0.995593]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.248231][G eval loss: 0.899579]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.005 lr_d=0.0005 -> score=1.147810\n",
      "\n",
      "----- 0056: lr_g=0.005, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250011][G eval loss: 0.981335]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.083836]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.974680]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.077592]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.973629]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.076936]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.973401]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.076853]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.973827]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.077259]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249951][G eval loss: 0.974162]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249930][G train loss: 1.077495]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.974089]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249891][G train loss: 1.077295]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249876][G eval loss: 0.973317]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249838][G train loss: 1.076400]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249821][G eval loss: 0.971812]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249769][G train loss: 1.074781]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249749][G eval loss: 0.969886]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249678][G train loss: 1.072767]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249650][G eval loss: 0.968404]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249554][G train loss: 1.071227]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249520][G eval loss: 0.967727]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249391][G train loss: 1.070521]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249341][G eval loss: 0.967623]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249173][G train loss: 1.070414]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249100][G eval loss: 0.967840]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248882][G train loss: 1.070635]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248771][G eval loss: 0.968480]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248494][G train loss: 1.071279]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248350][G eval loss: 0.969580]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248002][G train loss: 1.072376]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247821][G eval loss: 0.970787]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247388][G train loss: 1.073588]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247291][G eval loss: 0.971324]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246753][G train loss: 1.074144]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246467][G eval loss: 0.970965]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245831][G train loss: 1.073755]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245491][G eval loss: 0.969879]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244796][G train loss: 1.072496]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244453][G eval loss: 0.968085]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243761][G train loss: 1.070362]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243303][G eval loss: 0.965070]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242704][G train loss: 1.066733]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241967][G eval loss: 0.959119]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.241571][G train loss: 1.059775]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.240822][G eval loss: 0.947416]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.240849][G train loss: 1.046257]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.240461][G eval loss: 0.928494]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.241323][G train loss: 1.024277]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.240884][G eval loss: 0.907935]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.242056][G train loss: 1.001496]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.241886][G eval loss: 0.895650]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.242656][G train loss: 0.987957]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.242951][G eval loss: 0.896262]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.243486][G train loss: 0.986526]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.243028][G eval loss: 0.895993]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.243983][G train loss: 0.983061]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.241983][G eval loss: 0.885391]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243554][G train loss: 0.970031]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.237249][G eval loss: 0.869512]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.242466][G train loss: 0.947002]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.234017][G eval loss: 0.847412]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.240442][G train loss: 0.918085]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.230934][G eval loss: 0.824926]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.237619][G train loss: 0.892694]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.227891][G eval loss: 0.804856]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.235136][G train loss: 0.872163]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.225943][G eval loss: 0.785346]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.233920][G train loss: 0.850998]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.225819][G eval loss: 0.757344]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.234647][G train loss: 0.822189]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.231236][G eval loss: 0.720661]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.240531][G train loss: 0.784703]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.241436][G eval loss: 0.682875]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.245879][G train loss: 0.748613]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.247290][G eval loss: 0.658830]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.251209][G train loss: 0.719510]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.253205][G eval loss: 0.639946]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.255183][G train loss: 0.694146]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.256553][G eval loss: 0.613839]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.255022][G train loss: 0.674351]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.256214][G eval loss: 0.582160]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.253127][G train loss: 0.648754]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.254292][G eval loss: 0.548194]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.250091][G train loss: 0.618842]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.251636][G eval loss: 0.514499]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.246924][G train loss: 0.590070]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.248717][G eval loss: 0.491059]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.244201][G train loss: 0.571573]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.245483][G eval loss: 0.492778]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.240219][G train loss: 0.575253]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.247564][G eval loss: 0.503656]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.238654][G train loss: 0.591770]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.253504][G eval loss: 0.517778]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.241649][G train loss: 0.599908]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.264104][G eval loss: 0.510616]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.248680][G train loss: 0.593037]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.276490][G eval loss: 0.494613]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.256894][G train loss: 0.568472]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.277825][G eval loss: 0.491822]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.261136][G train loss: 0.555837]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.278225][G eval loss: 0.495803]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.260367][G train loss: 0.561498]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.277945][G eval loss: 0.505853]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.258958][G train loss: 0.573068]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.277295][G eval loss: 0.508025]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.257489][G train loss: 0.577251]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.276419][G eval loss: 0.499627]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.256142][G train loss: 0.569748]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.275350][G eval loss: 0.482878]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.254980][G train loss: 0.553111]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.274087][G eval loss: 0.462597]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.254029][G train loss: 0.533451]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.272072][G eval loss: 0.444828]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.253017][G train loss: 0.517437]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.269121][G eval loss: 0.433392]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.252168][G train loss: 0.508105]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.264708][G eval loss: 0.429821]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.251430][G train loss: 0.506687]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.259648][G eval loss: 0.435323]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.250573][G train loss: 0.513875]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.256538][G eval loss: 0.445938]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.250023][G train loss: 0.525816]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.255387][G eval loss: 0.458445]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.249630][G train loss: 0.539301]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.254485][G eval loss: 0.472462]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.249296][G train loss: 0.553663]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.253846][G eval loss: 0.486910]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.249258][G train loss: 0.568226]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.253528][G eval loss: 0.499735]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.249667][G train loss: 0.580869]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.253666][G eval loss: 0.508394]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.250445][G train loss: 0.588744]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.254001][G eval loss: 0.510903]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.251337][G train loss: 0.589885]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.254021][G eval loss: 0.510626]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.251869][G train loss: 0.587448]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.253656][G eval loss: 0.509939]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.251991][G train loss: 0.583959]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.252990][G eval loss: 0.508376]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.251789][G train loss: 0.580749]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.252326][G eval loss: 0.504683]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.251496][G train loss: 0.576641]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.251682][G eval loss: 0.499753]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.251071][G train loss: 0.570784]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.250958][G eval loss: 0.493880]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.250508][G train loss: 0.563523]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.250544][G eval loss: 0.486110]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.250285][G train loss: 0.555361]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.250550][G eval loss: 0.478365]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.250396][G train loss: 0.547230]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.250670][G eval loss: 0.471407]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.250560][G train loss: 0.540920]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.250741][G eval loss: 0.465520]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.250648][G train loss: 0.535783]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.250735][G eval loss: 0.460729]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250639][G train loss: 0.531036]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.250601][G eval loss: 0.456553]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250531][G train loss: 0.527613]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.250397][G eval loss: 0.453513]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.250315][G train loss: 0.524414]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.250014][G eval loss: 0.451333]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.249940][G train loss: 0.520915]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.249457][G eval loss: 0.449753]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.249385][G train loss: 0.517853]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.248781][G eval loss: 0.448543]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.248625][G train loss: 0.515811]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.248119][G eval loss: 0.449316]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.247728][G train loss: 0.515420]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.247679][G eval loss: 0.449656]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.005 lr_d=0.0008 -> score=0.697335\n",
      "\n",
      "----- 0056: lr_g=0.005, lr_d=0.001 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250020][G eval loss: 0.980158]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 1.082659]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.974504]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.077416]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.974342]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.077650]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.974466]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249963][G train loss: 1.077918]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249958][G eval loss: 0.974768]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249937][G train loss: 1.078200]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249921][G eval loss: 0.974787]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249891][G train loss: 1.078121]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249862][G eval loss: 0.974355]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249818][G train loss: 1.077560]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249780][G eval loss: 0.973408]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249717][G train loss: 1.076490]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249670][G eval loss: 0.971870]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249579][G train loss: 1.074839]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249522][G eval loss: 0.969939]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249393][G train loss: 1.072818]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249315][G eval loss: 0.968462]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249137][G train loss: 1.071283]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249019][G eval loss: 0.967869]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248777][G train loss: 1.070660]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248596][G eval loss: 0.967946]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248271][G train loss: 1.070730]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248003][G eval loss: 0.968575]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247571][G train loss: 1.071358]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247197][G eval loss: 0.969859]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246626][G train loss: 1.072642]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246175][G eval loss: 0.971017]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245447][G train loss: 1.073785]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.244748][G eval loss: 0.972124]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.243860][G train loss: 1.074871]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.242931][G eval loss: 0.973121]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.241891][G train loss: 1.075828]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.240794][G eval loss: 0.973813]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239602][G train loss: 1.076386]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238434][G eval loss: 0.974126]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.237143][G train loss: 1.076304]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.236017][G eval loss: 0.973875]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.234765][G train loss: 1.075161]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.233676][G eval loss: 0.971549]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.232754][G train loss: 1.071071]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.231798][G eval loss: 0.963219]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.231651][G train loss: 1.059469]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.231549][G eval loss: 0.945116]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.232979][G train loss: 1.036028]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.233572][G eval loss: 0.918981]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235103][G train loss: 1.007126]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.236436][G eval loss: 0.895858]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.237340][G train loss: 0.982475]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.240619][G eval loss: 0.883804]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.240277][G train loss: 0.969392]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.242769][G eval loss: 0.887873]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.243854][G train loss: 0.967824]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.242864][G eval loss: 0.892303]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.005 lr_d=0.001 -> score=1.135167\n",
      "\n",
      "----- 0056: lr_g=0.005, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250032][G eval loss: 0.978978]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250029][G train loss: 1.081479]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.974306]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.077218]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.975018]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.078326]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.975501]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249954][G train loss: 1.078953]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249943][G eval loss: 0.975733]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249916][G train loss: 1.079165]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 0.975462]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249841][G train loss: 1.078795]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249790][G eval loss: 0.974714]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249730][G train loss: 1.077919]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249659][G eval loss: 0.974162]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249568][G train loss: 1.077243]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249490][G eval loss: 0.972800]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249357][G train loss: 1.075767]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249237][G eval loss: 0.970997]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249038][G train loss: 1.073875]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248827][G eval loss: 0.969558]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248537][G train loss: 1.072375]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248187][G eval loss: 0.969040]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247795][G train loss: 1.071825]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247310][G eval loss: 0.969444]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246777][G train loss: 1.072217]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246319][G eval loss: 0.970312]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245602][G train loss: 1.073083]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244756][G eval loss: 0.971546]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243823][G train loss: 1.074297]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242701][G eval loss: 0.973297]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241514][G train loss: 1.076021]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240066][G eval loss: 0.975364]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238579][G train loss: 1.078052]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236887][G eval loss: 0.977306]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.235032][G train loss: 1.079949]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232949][G eval loss: 0.979133]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230713][G train loss: 1.081576]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.228228][G eval loss: 0.980938]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.225700][G train loss: 1.082930]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.223109][G eval loss: 0.981783]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.220618][G train loss: 1.082496]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.218593][G eval loss: 0.979581]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.217207][G train loss: 1.076225]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.216205][G eval loss: 0.968542]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.218586][G train loss: 1.055625]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.218570][G eval loss: 0.940972]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.224064][G train loss: 1.021119]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.225905][G eval loss: 0.903088]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.229118][G train loss: 0.985942]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.233708][G eval loss: 0.873651]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.235703][G train loss: 0.955089]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.239645][G eval loss: 0.868954]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.243777][G train loss: 0.942741]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.242575][G eval loss: 0.882383]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.248594][G train loss: 0.949158]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.243029][G eval loss: 0.895247]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.252129][G train loss: 0.954840]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.242250][G eval loss: 0.900580]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.253140][G train loss: 0.957495]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.240126][G eval loss: 0.897329]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.251148][G train loss: 0.952620]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.237329][G eval loss: 0.883671]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.247635][G train loss: 0.939292]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.234580][G eval loss: 0.864039]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.247035][G train loss: 0.916079]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.216315][G eval loss: 0.873452]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.243740][G train loss: 0.899137]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.213295][G eval loss: 0.866724]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.235036][G train loss: 0.896998]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.217227][G eval loss: 0.851456]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.236417][G train loss: 0.880132]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.238762][G eval loss: 0.805169]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.240802][G train loss: 0.866112]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.248453][G eval loss: 0.785933]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.247040][G train loss: 0.846931]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.251901][G eval loss: 0.781211]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.251153][G train loss: 0.832823]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.262163][G eval loss: 0.765973]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.263585][G train loss: 0.806706]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.261505][G eval loss: 0.770174]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.262745][G train loss: 0.807224]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.260099][G eval loss: 0.773360]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.257296][G train loss: 0.813478]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.258779][G eval loss: 0.779614]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.250325][G train loss: 0.829056]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.258520][G eval loss: 0.788050]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.247572][G train loss: 0.840257]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.260424][G eval loss: 0.794294]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.245329][G train loss: 0.845869]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.263312][G eval loss: 0.794193]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.241571][G train loss: 0.850393]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.265905][G eval loss: 0.797817]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.240240][G train loss: 0.851782]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.268429][G eval loss: 0.795903]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.240632][G train loss: 0.848459]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.269846][G eval loss: 0.791743]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.238549][G train loss: 0.844463]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.270490][G eval loss: 0.784512]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.236984][G train loss: 0.833510]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.270689][G eval loss: 0.767930]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.235332][G train loss: 0.818804]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.270884][G eval loss: 0.746080]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.233826][G train loss: 0.800653]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.271227][G eval loss: 0.721335]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.232806][G train loss: 0.782206]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.271630][G eval loss: 0.697186]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.230001][G train loss: 0.771984]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.272358][G eval loss: 0.681853]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.227353][G train loss: 0.771998]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.272621][G eval loss: 0.667593]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.227720][G train loss: 0.766284]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.272223][G eval loss: 0.659353]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.228672][G train loss: 0.763399]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.265820][G eval loss: 0.665875]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.224120][G train loss: 0.778679]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.259458][G eval loss: 0.683195]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.225037][G train loss: 0.774005]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.259415][G eval loss: 0.682012]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.224064][G train loss: 0.771681]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.266559][G eval loss: 0.657918]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.225505][G train loss: 0.758890]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.269751][G eval loss: 0.648520]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.228556][G train loss: 0.736123]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.279426][G eval loss: 0.601520]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.233790][G train loss: 0.715279]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.280212][G eval loss: 0.606566]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.240126][G train loss: 0.690183]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.281625][G eval loss: 0.619329]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.005 lr_d=0.0012 -> score=0.900954\n",
      "\n",
      "----- 0056: lr_g=0.006, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.980660]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.083180]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.975456]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.078601]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.971864]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.075342]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.971814]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.075296]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.973313]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.076672]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.974509]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.077710]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.974701]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249940][G train loss: 1.077758]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249938][G eval loss: 0.973502]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076436]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.971456]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.074301]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 0.969577]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249865][G train loss: 1.072361]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249866][G eval loss: 0.968497]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249829][G train loss: 1.071252]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.967944]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249782][G train loss: 1.070699]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249777][G eval loss: 0.967536]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249720][G train loss: 1.070304]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249711][G eval loss: 0.967422]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249641][G train loss: 1.070208]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249632][G eval loss: 0.967517]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249548][G train loss: 1.070334]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249541][G eval loss: 0.967339]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249442][G train loss: 1.070200]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249440][G eval loss: 0.966562]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249326][G train loss: 1.069419]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249326][G eval loss: 0.964654]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249200][G train loss: 1.067420]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249204][G eval loss: 0.960490]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249075][G train loss: 1.063098]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249069][G eval loss: 0.951554]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248952][G train loss: 1.053839]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248940][G eval loss: 0.934691]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248867][G train loss: 1.036220]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248901][G eval loss: 0.913240]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248916][G train loss: 1.013468]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.249012][G eval loss: 0.900804]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.249111][G train loss: 0.999367]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.249110][G eval loss: 0.904692]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.249231][G train loss: 1.001020]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.248946][G eval loss: 0.900952]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.249142][G train loss: 0.995242]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.248488][G eval loss: 0.882598]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.248896][G train loss: 0.974309]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.248012][G eval loss: 0.858508]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.248526][G train loss: 0.948989]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.247573][G eval loss: 0.838108]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.248162][G train loss: 0.929007]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.247278][G eval loss: 0.816444]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.247924][G train loss: 0.907744]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.247154][G eval loss: 0.786735]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.247891][G train loss: 0.877355]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.247397][G eval loss: 0.748519]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.248266][G train loss: 0.840046]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.248070][G eval loss: 0.711532]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.248912][G train loss: 0.800759]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.248689][G eval loss: 0.675359]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249272][G train loss: 0.762817]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.248681][G eval loss: 0.634926]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.249293][G train loss: 0.719836]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.248150][G eval loss: 0.581364]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248780][G train loss: 0.667915]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.247107][G eval loss: 0.530172]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.247631][G train loss: 0.615378]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.245213][G eval loss: 0.511087]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.245343][G train loss: 0.594116]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.243898][G eval loss: 0.535064]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.242972][G train loss: 0.625966]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.244054][G eval loss: 0.578054]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.242212][G train loss: 0.677520]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.245452][G eval loss: 0.617088]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.242844][G train loss: 0.704916]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.248241][G eval loss: 0.614690]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.244810][G train loss: 0.695443]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.252632][G eval loss: 0.582589]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.248001][G train loss: 0.667134]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.256386][G eval loss: 0.564990]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.251460][G train loss: 0.640003]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.258983][G eval loss: 0.566527]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.253893][G train loss: 0.634402]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.260693][G eval loss: 0.564644]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.255172][G train loss: 0.634009]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.261423][G eval loss: 0.554116]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.255643][G train loss: 0.625716]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.261239][G eval loss: 0.538302]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.255424][G train loss: 0.611935]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.259804][G eval loss: 0.523859]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.254530][G train loss: 0.600399]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.257379][G eval loss: 0.516479]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.253272][G train loss: 0.594538]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.256147][G eval loss: 0.513956]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.251923][G train loss: 0.592473]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.255167][G eval loss: 0.514866]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.251343][G train loss: 0.590985]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.254492][G eval loss: 0.514660]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.250978][G train loss: 0.589785]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.254063][G eval loss: 0.509897]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.250745][G train loss: 0.585879]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.253776][G eval loss: 0.501001]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.250685][G train loss: 0.577415]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.253507][G eval loss: 0.489221]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.250692][G train loss: 0.566382]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.253193][G eval loss: 0.481979]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.250647][G train loss: 0.558270]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.252827][G eval loss: 0.481994]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.250517][G train loss: 0.557839]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.252594][G eval loss: 0.485138]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.250374][G train loss: 0.562464]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.252443][G eval loss: 0.487020]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.250150][G train loss: 0.566145]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.252304][G eval loss: 0.484233]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.249931][G train loss: 0.564961]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.252153][G eval loss: 0.478397]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.249737][G train loss: 0.558417]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.252086][G eval loss: 0.470607]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.249556][G train loss: 0.550406]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.252051][G eval loss: 0.463682]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.249408][G train loss: 0.543487]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.252041][G eval loss: 0.455410]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.249252][G train loss: 0.535301]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.251970][G eval loss: 0.445360]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.249016][G train loss: 0.524311]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.251701][G eval loss: 0.437013]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.248667][G train loss: 0.514887]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.251231][G eval loss: 0.435543]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.248220][G train loss: 0.512076]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.250745][G eval loss: 0.435805]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.247681][G train loss: 0.511667]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.250193][G eval loss: 0.433646]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.247054][G train loss: 0.508309]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.249591][G eval loss: 0.428734]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.246418][G train loss: 0.501576]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.248997][G eval loss: 0.424169]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.245869][G train loss: 0.494497]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.248459][G eval loss: 0.422996]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.245302][G train loss: 0.490862]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.247896][G eval loss: 0.423845]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.244702][G train loss: 0.489711]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.247386][G eval loss: 0.423571]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.244060][G train loss: 0.488092]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.246866][G eval loss: 0.421075]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.243385][G train loss: 0.484489]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.246165][G eval loss: 0.420909]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.242692][G train loss: 0.480754]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.245302][G eval loss: 0.422815]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.241801][G train loss: 0.479334]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.244514][G eval loss: 0.424385]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.006 lr_d=0.0005 -> score=0.668898\n",
      "\n",
      "----- 0056: lr_g=0.006, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250011][G eval loss: 0.978732]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250009][G train loss: 1.081252]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.975172]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.078317]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.972897]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.076376]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.973407]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.076889]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 0.974661]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249960][G train loss: 1.078020]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 0.975262]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249934][G train loss: 1.078463]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 0.974779]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249896][G train loss: 1.077835]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 0.972965]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249847][G train loss: 1.075898]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 0.970596]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249782][G train loss: 1.073440]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249764][G eval loss: 0.968600]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249698][G train loss: 1.071384]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249672][G eval loss: 0.967582]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249583][G train loss: 1.070336]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249543][G eval loss: 0.967240]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249424][G train loss: 1.069992]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249357][G eval loss: 0.967147]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249203][G train loss: 1.069911]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249124][G eval loss: 0.967417]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248929][G train loss: 1.070195]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248827][G eval loss: 0.968004]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248580][G train loss: 1.070807]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248445][G eval loss: 0.968540]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248140][G train loss: 1.071373]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247994][G eval loss: 0.968583]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247627][G train loss: 1.071383]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247460][G eval loss: 0.966838]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247051][G train loss: 1.069471]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246819][G eval loss: 0.962497]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246421][G train loss: 1.064770]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.246113][G eval loss: 0.952740]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.245810][G train loss: 1.054285]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.245538][G eval loss: 0.934217]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.245501][G train loss: 1.034234]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.245519][G eval loss: 0.911043]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.245965][G train loss: 1.008754]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.245802][G eval loss: 0.898784]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.246331][G train loss: 0.994564]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.245846][G eval loss: 0.903773]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.246370][G train loss: 0.997249]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.245079][G eval loss: 0.900187]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.245834][G train loss: 0.991457]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.242555][G eval loss: 0.883439]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.244698][G train loss: 0.970124]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.239979][G eval loss: 0.859407]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.242915][G train loss: 0.942050]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.237599][G eval loss: 0.836579]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.241093][G train loss: 0.918964]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.235916][G eval loss: 0.814295]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.239948][G train loss: 0.896656]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.235158][G eval loss: 0.786013]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.239808][G train loss: 0.866688]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.236121][G eval loss: 0.748508]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.242217][G train loss: 0.826583]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.241553][G eval loss: 0.705579]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.246281][G train loss: 0.783325]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.247348][G eval loss: 0.670209]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249011][G train loss: 0.747778]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.250089][G eval loss: 0.638645]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.250470][G train loss: 0.710982]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.251596][G eval loss: 0.592263]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.250208][G train loss: 0.668464]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250620][G eval loss: 0.534656]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.248001][G train loss: 0.617422]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.245574][G eval loss: 0.490004]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.245286][G train loss: 0.569197]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.239058][G eval loss: 0.486831]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.239554][G train loss: 0.561765]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.236822][G eval loss: 0.518432]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.233426][G train loss: 0.605124]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.240787][G eval loss: 0.558382]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.235435][G train loss: 0.645793]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.251247][G eval loss: 0.572291]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.245044][G train loss: 0.658620]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.261141][G eval loss: 0.563652]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.257024][G train loss: 0.643194]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.274421][G eval loss: 0.544237]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.266246][G train loss: 0.622045]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.276644][G eval loss: 0.548919]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.270291][G train loss: 0.617695]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.277119][G eval loss: 0.558407]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.006 lr_d=0.0008 -> score=0.835526\n",
      "\n",
      "----- 0056: lr_g=0.006, lr_d=0.001 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250020][G eval loss: 0.977555]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250018][G train loss: 1.080075]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.974996]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.078142]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 0.973613]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.077091]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.974471]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.077953]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249963][G eval loss: 0.975593]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249944][G train loss: 1.078952]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249926][G eval loss: 0.975865]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249898][G train loss: 1.079066]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249871][G eval loss: 0.974997]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249831][G train loss: 1.078054]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249796][G eval loss: 0.972905]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249738][G train loss: 1.075838]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249696][G eval loss: 0.970437]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249614][G train loss: 1.073280]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249555][G eval loss: 0.968381]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249441][G train loss: 1.071164]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249363][G eval loss: 0.967335]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249208][G train loss: 1.070087]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249091][G eval loss: 0.967012]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248877][G train loss: 1.069760]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248688][G eval loss: 0.967053]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248396][G train loss: 1.069810]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248102][G eval loss: 0.967674]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247713][G train loss: 1.070439]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247310][G eval loss: 0.968644]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246834][G train loss: 1.071415]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246359][G eval loss: 0.969623]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245771][G train loss: 1.072413]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245144][G eval loss: 0.969589]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244463][G train loss: 1.072272]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243725][G eval loss: 0.968490]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.243023][G train loss: 1.070781]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.242233][G eval loss: 0.964731]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.241642][G train loss: 1.066234]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.240892][G eval loss: 0.954355]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.240654][G train loss: 1.054214]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.240432][G eval loss: 0.933459]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.241123][G train loss: 1.029921]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.241208][G eval loss: 0.909119]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242240][G train loss: 1.002985]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.242314][G eval loss: 0.898944]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.242984][G train loss: 0.990996]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.243135][G eval loss: 0.904882]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.243694][G train loss: 0.994103]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.242101][G eval loss: 0.902268]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.243373][G train loss: 0.988143]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.239469][G eval loss: 0.884598]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.242222][G train loss: 0.967932]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.232530][G eval loss: 0.866440]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.239522][G train loss: 0.939446]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.228329][G eval loss: 0.844637]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.235643][G train loss: 0.913999]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.224473][G eval loss: 0.825738]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.233422][G train loss: 0.892172]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.222193][G eval loss: 0.805521]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.233049][G train loss: 0.868447]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.223855][G eval loss: 0.775123]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.235412][G train loss: 0.836907]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.234055][G eval loss: 0.731993]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.243734][G train loss: 0.795714]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.246423][G eval loss: 0.695931]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.251953][G train loss: 0.758145]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.253298][G eval loss: 0.674025]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.256687][G train loss: 0.731739]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.256217][G eval loss: 0.648502]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.256913][G train loss: 0.707223]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.256309][G eval loss: 0.608756]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.254960][G train loss: 0.672774]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.254912][G eval loss: 0.558584]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.251238][G train loss: 0.629571]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.251602][G eval loss: 0.506770]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.246423][G train loss: 0.588347]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.244718][G eval loss: 0.489099]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.241341][G train loss: 0.568753]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.247199][G eval loss: 0.499958]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.237670][G train loss: 0.588861]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.254904][G eval loss: 0.520855]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.241139][G train loss: 0.612735]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.261614][G eval loss: 0.540118]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.250610][G train loss: 0.625330]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.276147][G eval loss: 0.524222]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.258518][G train loss: 0.617360]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.278187][G eval loss: 0.531805]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.262349][G train loss: 0.602434]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.278568][G eval loss: 0.540406]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.006 lr_d=0.001 -> score=0.818974\n",
      "\n",
      "----- 0056: lr_g=0.006, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250033][G eval loss: 0.976375]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250030][G train loss: 1.078895]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.974798]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.077944]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.974294]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.077772]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.975513]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249957][G train loss: 1.078995]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 0.976570]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249922][G train loss: 1.079929]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249886][G eval loss: 0.976565]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249846][G train loss: 1.079766]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249794][G eval loss: 0.975406]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249736][G train loss: 1.078463]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249670][G eval loss: 0.973783]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249582][G train loss: 1.076716]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249504][G eval loss: 0.971530]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249377][G train loss: 1.074372]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249256][G eval loss: 0.969638]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249066][G train loss: 1.072419]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248863][G eval loss: 0.968636]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248585][G train loss: 1.071383]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248225][G eval loss: 0.968541]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247846][G train loss: 1.071281]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247333][G eval loss: 0.969015]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246816][G train loss: 1.071760]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246201][G eval loss: 0.969605]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245525][G train loss: 1.072347]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244576][G eval loss: 0.970560]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243716][G train loss: 1.073282]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242486][G eval loss: 0.971664]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241432][G train loss: 1.074335]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240034][G eval loss: 0.972403]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238809][G train loss: 1.074798]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.237388][G eval loss: 0.971918]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.236116][G train loss: 1.073534]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.234856][G eval loss: 0.967479]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.233938][G train loss: 1.067132]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.233462][G eval loss: 0.952740]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.233711][G train loss: 1.048113]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.234775][G eval loss: 0.925114]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.236014][G train loss: 1.016008]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.237273][G eval loss: 0.899200]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.238118][G train loss: 0.987974]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.240724][G eval loss: 0.889207]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240270][G train loss: 0.976572]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.241719][G eval loss: 0.893509]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.242829][G train loss: 0.974928]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.240558][G eval loss: 0.892551]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.243194][G train loss: 0.970861]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.237686][G eval loss: 0.881105]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.242001][G train loss: 0.957443]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.233248][G eval loss: 0.861663]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.239516][G train loss: 0.936801]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.218897][G eval loss: 0.862620]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.234852][G train loss: 0.915566]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.212860][G eval loss: 0.854504]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.229639][G train loss: 0.899513]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.208720][G eval loss: 0.848552]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.228853][G train loss: 0.885866]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.207775][G eval loss: 0.838668]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.226652][G train loss: 0.877411]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.216657][G eval loss: 0.810594]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.229281][G train loss: 0.857690]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.235387][G eval loss: 0.763903]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.241036][G train loss: 0.823223]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.247496][G eval loss: 0.745312]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.254903][G train loss: 0.785256]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.257594][G eval loss: 0.727860]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.263057][G train loss: 0.766922]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.258000][G eval loss: 0.723282]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.258788][G train loss: 0.764426]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.257691][G eval loss: 0.715703]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.256253][G train loss: 0.758264]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.257393][G eval loss: 0.696924]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.253381][G train loss: 0.741677]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.257150][G eval loss: 0.667327]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.249317][G train loss: 0.716324]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.257641][G eval loss: 0.628163]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.245587][G train loss: 0.684287]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.260977][G eval loss: 0.581755]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.241199][G train loss: 0.649230]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.263236][G eval loss: 0.540298]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.238320][G train loss: 0.620472]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.265073][G eval loss: 0.535276]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.236166][G train loss: 0.617007]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.264358][G eval loss: 0.562923]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.237889][G train loss: 0.639107]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.271448][G eval loss: 0.565018]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.241298][G train loss: 0.657814]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.287306][G eval loss: 0.535818]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.248245][G train loss: 0.645282]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.292849][G eval loss: 0.531773]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.254557][G train loss: 0.625011]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.292666][G eval loss: 0.532582]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.255624][G train loss: 0.616259]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.292022][G eval loss: 0.540251]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 0056 lr_g=0.006 lr_d=0.0012 -> score=0.832273\n",
      "\n",
      ">>> Best config for 0056: lr_g=0.006, lr_d=0.0005, score=0.668898\n",
      "\n",
      "========== Grid search for 2330 ==========\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.492428]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.456734]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.481570]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 0.445985]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 0.479411]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.443979]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.479137]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.443824]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.478821]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.443501]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.478656]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249958][G train loss: 0.443256]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249957][G eval loss: 0.478775]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249943][G train loss: 0.443271]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249942][G eval loss: 0.478930]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249924][G train loss: 0.443326]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249923][G eval loss: 0.478793]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249900][G train loss: 0.443111]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 0.478188]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249871][G train loss: 0.442470]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 0.477212]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249836][G train loss: 0.441500]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249837][G eval loss: 0.476158]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249792][G train loss: 0.440490]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249792][G eval loss: 0.475355]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249737][G train loss: 0.439750]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249738][G eval loss: 0.474886]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249670][G train loss: 0.439343]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249672][G eval loss: 0.474737]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249588][G train loss: 0.439233]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249592][G eval loss: 0.474841]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249487][G train loss: 0.439338]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249492][G eval loss: 0.475208]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249363][G train loss: 0.439665]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249376][G eval loss: 0.475873]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249216][G train loss: 0.440269]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249230][G eval loss: 0.476661]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249037][G train loss: 0.440990]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249068][G eval loss: 0.477423]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248834][G train loss: 0.441707]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248882][G eval loss: 0.477996]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248598][G train loss: 0.442263]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248667][G eval loss: 0.478358]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248323][G train loss: 0.442636]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248417][G eval loss: 0.478646]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.247999][G train loss: 0.442956]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248126][G eval loss: 0.479003]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247618][G train loss: 0.443350]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247779][G eval loss: 0.479422]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247164][G train loss: 0.443784]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247374][G eval loss: 0.479907]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246640][G train loss: 0.444281]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246905][G eval loss: 0.480406]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246038][G train loss: 0.444806]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246372][G eval loss: 0.480631]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245363][G train loss: 0.445056]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245772][G eval loss: 0.480848]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244601][G train loss: 0.445291]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245130][G eval loss: 0.481233]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243781][G train loss: 0.445677]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.244402][G eval loss: 0.481869]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.0005 -> score=0.726271\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 0.490498]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250007][G train loss: 0.454804]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.481278]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 0.445692]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 0.480424]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 0.444992]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.480719]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249971][G train loss: 0.445405]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.480171]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249954][G train loss: 0.444852]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249945][G eval loss: 0.479437]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249927][G train loss: 0.444036]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.478919]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249889][G train loss: 0.443416]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 0.478578]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249835][G train loss: 0.442975]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249813][G eval loss: 0.478090]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249762][G train loss: 0.442408]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249735][G eval loss: 0.477506]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249666][G train loss: 0.441790]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249630][G eval loss: 0.476628]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249537][G train loss: 0.440917]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249487][G eval loss: 0.475726]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249360][G train loss: 0.440060]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249297][G eval loss: 0.475144]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249116][G train loss: 0.439541]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249037][G eval loss: 0.474986]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248789][G train loss: 0.439445]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248738][G eval loss: 0.475130]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248411][G train loss: 0.439629]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248379][G eval loss: 0.475665]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247950][G train loss: 0.440167]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247891][G eval loss: 0.476588]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247333][G train loss: 0.441050]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247276][G eval loss: 0.477981]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246555][G train loss: 0.442385]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246648][G eval loss: 0.479481]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245728][G train loss: 0.443822]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245765][G eval loss: 0.480725]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244612][G train loss: 0.445020]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244617][G eval loss: 0.481917]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243186][G train loss: 0.446195]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243252][G eval loss: 0.482861]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241494][G train loss: 0.447148]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241578][G eval loss: 0.483849]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239458][G train loss: 0.448163]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239561][G eval loss: 0.485037]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237029][G train loss: 0.449387]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.237145][G eval loss: 0.486421]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.0008 -> score=0.723566\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.001 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 0.489318]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250015][G train loss: 0.453624]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.481091]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 0.445506]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.481121]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249974][G train loss: 0.445689]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 0.481770]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249961][G train loss: 0.446457]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249952][G eval loss: 0.481103]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249933][G train loss: 0.445784]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.480056]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249886][G train loss: 0.444656]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 0.479194]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249816][G train loss: 0.443690]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249779][G eval loss: 0.478874]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249715][G train loss: 0.443271]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249662][G eval loss: 0.478439]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249568][G train loss: 0.442758]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249499][G eval loss: 0.477722]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249367][G train loss: 0.442008]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249274][G eval loss: 0.476830]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249083][G train loss: 0.441122]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248972][G eval loss: 0.475999]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248694][G train loss: 0.440335]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248546][G eval loss: 0.475630]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248147][G train loss: 0.440029]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247991][G eval loss: 0.475688]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247439][G train loss: 0.440148]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247282][G eval loss: 0.476437]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246534][G train loss: 0.440943]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246484][G eval loss: 0.477626]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245479][G train loss: 0.442139]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245317][G eval loss: 0.478658]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244003][G train loss: 0.443131]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243717][G eval loss: 0.480279]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242016][G train loss: 0.444691]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241734][G eval loss: 0.482319]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239571][G train loss: 0.446672]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.239271][G eval loss: 0.484545]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236563][G train loss: 0.448856]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.236333][G eval loss: 0.486857]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.232999][G train loss: 0.451155]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.232910][G eval loss: 0.489037]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.228902][G train loss: 0.453341]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.228884][G eval loss: 0.491371]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.224154][G train loss: 0.455689]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.224070][G eval loss: 0.494152]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.218588][G train loss: 0.458479]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.218245][G eval loss: 0.497497]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.212070][G train loss: 0.461848]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.211349][G eval loss: 0.501552]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.204575][G train loss: 0.465936]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.203484][G eval loss: 0.506332]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.196092][G train loss: 0.470747]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.194688][G eval loss: 0.511726]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.186719][G train loss: 0.476123]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.185189][G eval loss: 0.517525]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.176556][G train loss: 0.481873]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.174972][G eval loss: 0.523319]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.166008][G train loss: 0.487422]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.164216][G eval loss: 0.529277]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.155796][G train loss: 0.492825]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.154860][G eval loss: 0.532866]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.148098][G train loss: 0.494652]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.160746][G eval loss: 0.508911]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.159047][G train loss: 0.461219]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.201321][G eval loss: 0.448638]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.215481][G train loss: 0.376569]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.236119][G eval loss: 0.419287]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.262159][G train loss: 0.337896]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.272034][G eval loss: 0.386226]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.295668][G train loss: 0.313017]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.296164][G eval loss: 0.362606]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.315196][G train loss: 0.293487]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.348837][G eval loss: 0.291119]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.340552][G train loss: 0.264876]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.370642][G eval loss: 0.273249]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.348858][G train loss: 0.254444]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.377673][G eval loss: 0.271821]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.362208][G train loss: 0.242207]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.381181][G eval loss: 0.280206]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.371646][G train loss: 0.242469]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.382758][G eval loss: 0.291040]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.373480][G train loss: 0.252271]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.382841][G eval loss: 0.298441]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.375202][G train loss: 0.257988]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.382003][G eval loss: 0.299334]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.377422][G train loss: 0.256996]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.380158][G eval loss: 0.293659]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.377831][G train loss: 0.249626]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.377293][G eval loss: 0.283019]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.377160][G train loss: 0.237646]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.373848][G eval loss: 0.269673]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.373718][G train loss: 0.223994]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.369824][G eval loss: 0.259857]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.370476][G train loss: 0.212930]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.365194][G eval loss: 0.250197]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.366172][G train loss: 0.203023]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.360518][G eval loss: 0.241284]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.361905][G train loss: 0.194977]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.355831][G eval loss: 0.235251]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.357568][G train loss: 0.189520]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.350861][G eval loss: 0.231398]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.352868][G train loss: 0.186119]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.345684][G eval loss: 0.229311]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.346718][G train loss: 0.184886]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.340103][G eval loss: 0.228793]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.338089][G train loss: 0.185652]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.334453][G eval loss: 0.229514]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.331555][G train loss: 0.188373]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.328705][G eval loss: 0.230526]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.326454][G train loss: 0.193206]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.322892][G eval loss: 0.232935]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.321105][G train loss: 0.197866]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.317079][G eval loss: 0.236449]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.315571][G train loss: 0.201883]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.311279][G eval loss: 0.240619]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.309902][G train loss: 0.205567]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.305517][G eval loss: 0.243895]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.304190][G train loss: 0.208427]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.299810][G eval loss: 0.246365]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.298585][G train loss: 0.210748]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.294277][G eval loss: 0.249780]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.293248][G train loss: 0.214017]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.288882][G eval loss: 0.254312]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.288187][G train loss: 0.218539]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.283672][G eval loss: 0.260206]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.283350][G train loss: 0.223909]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.279066][G eval loss: 0.266929]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.001 -> score=0.545996\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.0012 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250029][G eval loss: 0.488135]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250027][G train loss: 0.452442]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.480884]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 0.445299]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.481789]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 0.446357]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.482801]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249951][G train loss: 0.447487]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249934][G eval loss: 0.482071]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249908][G train loss: 0.446751]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249875][G eval loss: 0.480786]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249831][G train loss: 0.445385]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249781][G eval loss: 0.479800]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249711][G train loss: 0.444297]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249635][G eval loss: 0.478969]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249527][G train loss: 0.443367]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249409][G eval loss: 0.478155]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249245][G train loss: 0.442477]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249113][G eval loss: 0.477260]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248863][G train loss: 0.441548]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248668][G eval loss: 0.476438]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248284][G train loss: 0.440733]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248036][G eval loss: 0.476033]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247475][G train loss: 0.440371]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247225][G eval loss: 0.476447]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246426][G train loss: 0.440850]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246232][G eval loss: 0.477718]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245103][G train loss: 0.442187]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244764][G eval loss: 0.479011]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243203][G train loss: 0.443530]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242717][G eval loss: 0.481003]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.240595][G train loss: 0.445527]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240149][G eval loss: 0.483543]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.237305][G train loss: 0.448033]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.237060][G eval loss: 0.486337]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.233356][G train loss: 0.450781]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.233171][G eval loss: 0.489349]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.228537][G train loss: 0.453725]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.228156][G eval loss: 0.493107]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.222578][G train loss: 0.457432]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.222078][G eval loss: 0.496804]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.215584][G train loss: 0.461108]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.214442][G eval loss: 0.501257]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.207135][G train loss: 0.465554]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.205286][G eval loss: 0.506636]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.197248][G train loss: 0.470939]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.194578][G eval loss: 0.513022]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.185932][G train loss: 0.477353]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.182770][G eval loss: 0.519771]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.173389][G train loss: 0.484073]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.169409][G eval loss: 0.527805]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.159898][G train loss: 0.492032]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.154896][G eval loss: 0.536974]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.146807][G train loss: 0.500874]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.141264][G eval loss: 0.545967]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.135456][G train loss: 0.509145]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.132065][G eval loss: 0.549414]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.128653][G train loss: 0.510367]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.171534][G eval loss: 0.470105]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.182172][G train loss: 0.406911]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.256844][G eval loss: 0.372172]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.279178][G train loss: 0.304466]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.338666][G eval loss: 0.300978]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.339144][G train loss: 0.264402]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.365665][G eval loss: 0.294163]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.364058][G train loss: 0.258025]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.376424][G eval loss: 0.285275]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.374429][G train loss: 0.249241]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.382327][G eval loss: 0.275208]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.380462][G train loss: 0.239893]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.385651][G eval loss: 0.270825]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.384197][G train loss: 0.235967]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.387314][G eval loss: 0.272514]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.386113][G train loss: 0.237345]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.387862][G eval loss: 0.275164]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.387402][G train loss: 0.239397]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.387545][G eval loss: 0.276629]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.389929][G train loss: 0.239833]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.386511][G eval loss: 0.275223]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.390213][G train loss: 0.237010]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.384834][G eval loss: 0.270751]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.389309][G train loss: 0.231039]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.382573][G eval loss: 0.264236]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.387319][G train loss: 0.223121]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.379790][G eval loss: 0.257046]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.384599][G train loss: 0.214907]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.376520][G eval loss: 0.250375]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.381384][G train loss: 0.207657]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.372781][G eval loss: 0.245062]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.377491][G train loss: 0.201673]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.368598][G eval loss: 0.241030]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.372122][G train loss: 0.196810]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.364030][G eval loss: 0.237832]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.363926][G train loss: 0.192761]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.359046][G eval loss: 0.235029]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.358881][G train loss: 0.189335]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.353758][G eval loss: 0.231900]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.353635][G train loss: 0.186451]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.347969][G eval loss: 0.228549]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.342659][G train loss: 0.191475]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.341786][G eval loss: 0.227882]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.341323][G train loss: 0.185106]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.335289][G eval loss: 0.228326]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.334753][G train loss: 0.185859]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.328512][G eval loss: 0.229661]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.327623][G train loss: 0.187226]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.321400][G eval loss: 0.233462]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.320148][G train loss: 0.189564]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.314149][G eval loss: 0.237732]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.312818][G train loss: 0.193083]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.307754][G eval loss: 0.240511]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.306328][G train loss: 0.196252]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.301104][G eval loss: 0.243729]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.300101][G train loss: 0.199871]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.294527][G eval loss: 0.248180]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.293768][G train loss: 0.205141]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.288195][G eval loss: 0.254685]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.287519][G train loss: 0.211557]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.282133][G eval loss: 0.263037]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.281634][G train loss: 0.219016]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.276426][G eval loss: 0.272504]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.0012 -> score=0.548930\n",
      "\n",
      "----- 2330: lr_g=0.0045, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.490584]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.454893]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.480607]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.445046]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.479098]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.443735]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.478807]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.443537]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.478677]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.443348]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.478893]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249958][G train loss: 0.443443]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.479305]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249943][G train loss: 0.443725]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.479420]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249923][G train loss: 0.443735]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 0.478809]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249900][G train loss: 0.443077]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249899][G eval loss: 0.477537]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249871][G train loss: 0.441817]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249871][G eval loss: 0.476207]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249835][G train loss: 0.440542]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249835][G eval loss: 0.475197]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249791][G train loss: 0.439599]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249790][G eval loss: 0.474572]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249736][G train loss: 0.439026]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249735][G eval loss: 0.474294]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249668][G train loss: 0.438780]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249665][G eval loss: 0.474307]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249581][G train loss: 0.438772]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249580][G eval loss: 0.474691]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249475][G train loss: 0.439097]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249478][G eval loss: 0.475442]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249349][G train loss: 0.439774]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249367][G eval loss: 0.476297]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249207][G train loss: 0.440571]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249221][G eval loss: 0.476926]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249027][G train loss: 0.441185]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249063][G eval loss: 0.477370]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248828][G train loss: 0.441652]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248885][G eval loss: 0.477730]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248601][G train loss: 0.442057]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248677][G eval loss: 0.478092]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248331][G train loss: 0.442460]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248428][G eval loss: 0.478452]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248009][G train loss: 0.442838]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248138][G eval loss: 0.478776]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247632][G train loss: 0.443169]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247792][G eval loss: 0.479096]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247182][G train loss: 0.443480]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247396][G eval loss: 0.479495]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246666][G train loss: 0.443879]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246929][G eval loss: 0.480044]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246066][G train loss: 0.444438]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246405][G eval loss: 0.480437]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245400][G train loss: 0.444822]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245777][G eval loss: 0.480869]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244612][G train loss: 0.445241]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245103][G eval loss: 0.481257]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243759][G train loss: 0.445628]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.244392][G eval loss: 0.481785]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.0045 lr_d=0.0005 -> score=0.726177\n",
      "\n",
      "----- 2330: lr_g=0.0045, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 0.488654]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250007][G train loss: 0.452963]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.480317]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.444755]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 0.480115]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 0.444753]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.480393]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249972][G train loss: 0.445123]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.480029]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249956][G train loss: 0.444700]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249947][G eval loss: 0.479672]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249930][G train loss: 0.444221]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249916][G eval loss: 0.479444]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249891][G train loss: 0.443864]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 0.479061]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249837][G train loss: 0.443377]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249814][G eval loss: 0.478097]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249766][G train loss: 0.442365]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249739][G eval loss: 0.476844]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249673][G train loss: 0.441123]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249637][G eval loss: 0.475610]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249547][G train loss: 0.439946]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249496][G eval loss: 0.474753]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249373][G train loss: 0.439155]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249309][G eval loss: 0.474343]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249133][G train loss: 0.438798]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249053][G eval loss: 0.474369]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248808][G train loss: 0.438858]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248745][G eval loss: 0.474683]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248420][G train loss: 0.439153]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248375][G eval loss: 0.475501]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247946][G train loss: 0.439912]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247892][G eval loss: 0.476789]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247334][G train loss: 0.441129]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247294][G eval loss: 0.478341]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246576][G train loss: 0.442622]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246686][G eval loss: 0.479657]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245767][G train loss: 0.443922]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245830][G eval loss: 0.480564]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244683][G train loss: 0.444851]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244729][G eval loss: 0.481513]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243306][G train loss: 0.445849]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243445][G eval loss: 0.482506]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241693][G train loss: 0.446891]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241821][G eval loss: 0.483600]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239698][G train loss: 0.448001]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239844][G eval loss: 0.484812]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237302][G train loss: 0.449217]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.237495][G eval loss: 0.486197]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.234502][G train loss: 0.450597]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234901][G eval loss: 0.487708]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.0045 lr_d=0.0008 -> score=0.722608\n",
      "\n",
      "----- 2330: lr_g=0.0045, lr_d=0.001 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 0.487474]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250016][G train loss: 0.451783]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.480130]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 0.444569]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.480814]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249975][G train loss: 0.445451]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.481445]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249963][G train loss: 0.446175]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249954][G eval loss: 0.480958]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249937][G train loss: 0.445629]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 0.480286]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249890][G train loss: 0.444836]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249862][G eval loss: 0.479712]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249820][G train loss: 0.444132]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249779][G eval loss: 0.479315]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249718][G train loss: 0.443630]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249664][G eval loss: 0.478374]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249575][G train loss: 0.442644]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249507][G eval loss: 0.476965]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249380][G train loss: 0.441247]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249287][G eval loss: 0.475701]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249103][G train loss: 0.440039]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248987][G eval loss: 0.474913]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248716][G train loss: 0.439317]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248559][G eval loss: 0.474735]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248167][G train loss: 0.439194]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248015][G eval loss: 0.475130]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247470][G train loss: 0.439625]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247286][G eval loss: 0.476100]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246546][G train loss: 0.440580]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246476][G eval loss: 0.477451]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245482][G train loss: 0.441876]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245284][G eval loss: 0.478880]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.243985][G train loss: 0.443231]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243696][G eval loss: 0.480673]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242020][G train loss: 0.444964]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241759][G eval loss: 0.482512]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239628][G train loss: 0.446787]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.239362][G eval loss: 0.484416]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236697][G train loss: 0.448712]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.236524][G eval loss: 0.486535]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.233240][G train loss: 0.450882]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.233202][G eval loss: 0.488782]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.229237][G train loss: 0.453179]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.229208][G eval loss: 0.491238]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.224514][G train loss: 0.455637]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.224428][G eval loss: 0.494069]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.218956][G train loss: 0.458457]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.218723][G eval loss: 0.497387]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.212531][G train loss: 0.461780]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.212003][G eval loss: 0.501425]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.205186][G train loss: 0.465842]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.204275][G eval loss: 0.506259]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.196830][G train loss: 0.470694]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.195498][G eval loss: 0.511693]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.187500][G train loss: 0.476115]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.186061][G eval loss: 0.517594]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.177358][G train loss: 0.482026]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.175888][G eval loss: 0.523689]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.166694][G train loss: 0.488090]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.165376][G eval loss: 0.529798]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.156413][G train loss: 0.494026]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.156395][G eval loss: 0.533155]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.148497][G train loss: 0.496870]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.157506][G eval loss: 0.517440]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.151404][G train loss: 0.479060]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.232164][G eval loss: 0.391521]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.231747][G train loss: 0.347500]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.301599][G eval loss: 0.333274]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.297995][G train loss: 0.295180]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.337479][G eval loss: 0.317379]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.331757][G train loss: 0.280467]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.353748][G eval loss: 0.299340]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.347641][G train loss: 0.263467]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.362668][G eval loss: 0.284709]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.357199][G train loss: 0.249229]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.367544][G eval loss: 0.273778]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.363034][G train loss: 0.238131]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.369908][G eval loss: 0.270752]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.366497][G train loss: 0.233881]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.370471][G eval loss: 0.274042]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.368302][G train loss: 0.234567]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.369885][G eval loss: 0.275546]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.367968][G train loss: 0.232543]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.368643][G eval loss: 0.271358]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.367569][G train loss: 0.225357]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.366992][G eval loss: 0.260448]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.368943][G train loss: 0.214115]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.348298][G eval loss: 0.253464]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.356043][G train loss: 0.219990]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.306436][G eval loss: 0.298624]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.350428][G train loss: 0.212928]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.288992][G eval loss: 0.319047]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.344229][G train loss: 0.208798]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.285324][G eval loss: 0.320711]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.342215][G train loss: 0.203370]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.282684][G eval loss: 0.319792]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.339503][G train loss: 0.200187]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.280877][G eval loss: 0.316259]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.336303][G train loss: 0.197652]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.281503][G eval loss: 0.306680]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.333381][G train loss: 0.194004]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.290731][G eval loss: 0.285461]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.330805][G train loss: 0.189968]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.300315][G eval loss: 0.265075]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.325367][G train loss: 0.187706]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.331310][G eval loss: 0.209196]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.327249][G train loss: 0.172873]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.327344][G eval loss: 0.209741]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.326566][G train loss: 0.167395]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.322151][G eval loss: 0.212010]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.321269][G train loss: 0.169878]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.316756][G eval loss: 0.215788]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.315863][G train loss: 0.174003]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.311237][G eval loss: 0.221761]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.310400][G train loss: 0.179114]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.305647][G eval loss: 0.229072]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.304909][G train loss: 0.185124]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.300059][G eval loss: 0.234080]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.299323][G train loss: 0.191763]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.294607][G eval loss: 0.239520]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.286147][G train loss: 0.214085]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.289293][G eval loss: 0.246759]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.281608][G train loss: 0.219936]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.284123][G eval loss: 0.255476]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.279870][G train loss: 0.223058]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.279546][G eval loss: 0.263768]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.275477][G train loss: 0.230663]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.275272][G eval loss: 0.271900]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.271397][G train loss: 0.238028]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.271242][G eval loss: 0.280026]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.267585][G train loss: 0.245684]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.267535][G eval loss: 0.288035]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.264074][G train loss: 0.253201]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.264112][G eval loss: 0.296236]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.260844][G train loss: 0.260927]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.260994][G eval loss: 0.304872]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.257923][G train loss: 0.269137]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.258212][G eval loss: 0.313930]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.255335][G train loss: 0.277703]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.255867][G eval loss: 0.323088]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.253157][G train loss: 0.286559]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.253936][G eval loss: 0.332581]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.251379][G train loss: 0.296011]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.252358][G eval loss: 0.342638]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.249970][G train loss: 0.305933]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.251218][G eval loss: 0.352193]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.248965][G train loss: 0.315589]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.250388][G eval loss: 0.361975]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.248265][G train loss: 0.325442]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.249853][G eval loss: 0.372170]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.247861][G train loss: 0.335676]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.249638][G eval loss: 0.382881]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.247769][G train loss: 0.346453]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.249700][G eval loss: 0.393696]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.247943][G train loss: 0.357436]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.250015][G eval loss: 0.404361]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.248353][G train loss: 0.368231]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.250497][G eval loss: 0.413919]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.248902][G train loss: 0.378042]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.251080][G eval loss: 0.422843]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.249556][G train loss: 0.387148]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.251706][G eval loss: 0.430324]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.250210][G train loss: 0.394785]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.252302][G eval loss: 0.436428]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.250809][G train loss: 0.401021]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.252838][G eval loss: 0.441544]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.251352][G train loss: 0.406275]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.253260][G eval loss: 0.445406]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.251757][G train loss: 0.410343]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.253597][G eval loss: 0.448339]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.251953][G train loss: 0.413562]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.253870][G eval loss: 0.450520]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.252008][G train loss: 0.415769]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.254059][G eval loss: 0.451799]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.252747][G train loss: 0.416864]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.254161][G eval loss: 0.452247]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.253180][G train loss: 0.417516]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.254260][G eval loss: 0.452356]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.253031][G train loss: 0.417841]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.254270][G eval loss: 0.451865]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.252556][G train loss: 0.416694]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.254210][G eval loss: 0.450973]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.252172][G train loss: 0.415052]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.254035][G eval loss: 0.449408]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.252145][G train loss: 0.413450]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.253818][G eval loss: 0.447643]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.252030][G train loss: 0.411573]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.253565][G eval loss: 0.445471]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.251819][G train loss: 0.409203]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.253280][G eval loss: 0.442911]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.251581][G train loss: 0.406370]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.252965][G eval loss: 0.440030]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.251280][G train loss: 0.403307]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.252639][G eval loss: 0.436472]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.250941][G train loss: 0.399991]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.252300][G eval loss: 0.432968]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.250579][G train loss: 0.396389]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.251959][G eval loss: 0.429211]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.250211][G train loss: 0.392543]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.251629][G eval loss: 0.425342]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.249847][G train loss: 0.388482]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.251311][G eval loss: 0.421625]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.249493][G train loss: 0.384256]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.251011][G eval loss: 0.417917]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.249158][G train loss: 0.379960]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.250738][G eval loss: 0.414101]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.248846][G train loss: 0.375583]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.250530][G eval loss: 0.411172]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.248597][G train loss: 0.371944]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.250364][G eval loss: 0.408567]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.248390][G train loss: 0.368587]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.250213][G eval loss: 0.405888]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.248205][G train loss: 0.365143]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.250076][G eval loss: 0.403061]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.248043][G train loss: 0.361695]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.400128]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.247906][G train loss: 0.358291]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.249896][G eval loss: 0.397152]\n",
      "[Epoch 111/200][Batch 1/1][D train loss: 0.247792][G train loss: 0.354985]\n",
      "[Epoch 111/200][Batch 1/1][D eval loss: 0.249837][G eval loss: 0.394066]\n",
      "[Epoch 112/200][Batch 1/1][D train loss: 0.247699][G train loss: 0.351644]\n",
      "[Epoch 112/200][Batch 1/1][D eval loss: 0.249797][G eval loss: 0.390917]\n",
      "[Epoch 113/200][Batch 1/1][D train loss: 0.247628][G train loss: 0.348216]\n",
      "[Epoch 113/200][Batch 1/1][D eval loss: 0.249778][G eval loss: 0.387731]\n",
      "[Epoch 114/200][Batch 1/1][D train loss: 0.247579][G train loss: 0.344888]\n",
      "[Epoch 114/200][Batch 1/1][D eval loss: 0.249784][G eval loss: 0.384656]\n",
      "[Epoch 115/200][Batch 1/1][D train loss: 0.247552][G train loss: 0.341660]\n",
      "[Epoch 115/200][Batch 1/1][D eval loss: 0.249805][G eval loss: 0.381688]\n",
      "[Epoch 116/200][Batch 1/1][D train loss: 0.247544][G train loss: 0.338572]\n",
      "[Epoch 116/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 0.378851]\n",
      "[Epoch 117/200][Batch 1/1][D train loss: 0.247554][G train loss: 0.335618]\n",
      "[Epoch 117/200][Batch 1/1][D eval loss: 0.249851][G eval loss: 0.376042]\n",
      "[Epoch 118/200][Batch 1/1][D train loss: 0.247576][G train loss: 0.332802]\n",
      "[Epoch 118/200][Batch 1/1][D eval loss: 0.249868][G eval loss: 0.373545]\n",
      "[Epoch 119/200][Batch 1/1][D train loss: 0.247601][G train loss: 0.330347]\n",
      "[Epoch 119/200][Batch 1/1][D eval loss: 0.249909][G eval loss: 0.371324]\n",
      "[Epoch 120/200][Batch 1/1][D train loss: 0.247637][G train loss: 0.328184]\n",
      "[Epoch 120/200][Batch 1/1][D eval loss: 0.249950][G eval loss: 0.369210]\n",
      "[Epoch 121/200][Batch 1/1][D train loss: 0.247675][G train loss: 0.326162]\n",
      "[Epoch 121/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.367344]\n",
      "[Epoch 122/200][Batch 1/1][D train loss: 0.247711][G train loss: 0.324482]\n",
      "[Epoch 122/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 0.366939]\n",
      "[Epoch 123/200][Batch 1/1][D train loss: 0.247713][G train loss: 0.324185]\n",
      "[Epoch 123/200][Batch 1/1][D eval loss: 0.250035][G eval loss: 0.366623]\n",
      "[Epoch 124/200][Batch 1/1][D train loss: 0.247704][G train loss: 0.324063]\n",
      "[Epoch 124/200][Batch 1/1][D eval loss: 0.250022][G eval loss: 0.366560]\n",
      "[Epoch 125/200][Batch 1/1][D train loss: 0.247682][G train loss: 0.324116]\n",
      "[Epoch 125/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.366639]\n",
      "[Epoch 126/200][Batch 1/1][D train loss: 0.247661][G train loss: 0.324240]\n",
      "[Epoch 126/200][Batch 1/1][D eval loss: 0.249903][G eval loss: 0.366961]\n",
      "[Epoch 127/200][Batch 1/1][D train loss: 0.247630][G train loss: 0.324556]\n",
      "[Epoch 127/200][Batch 1/1][D eval loss: 0.249827][G eval loss: 0.367342]\n",
      "[Epoch 128/200][Batch 1/1][D train loss: 0.247599][G train loss: 0.325008]\n",
      "[Epoch 128/200][Batch 1/1][D eval loss: 0.249716][G eval loss: 0.367786]\n",
      "[Epoch 129/200][Batch 1/1][D train loss: 0.247568][G train loss: 0.325446]\n",
      "[Epoch 129/200][Batch 1/1][D eval loss: 0.249617][G eval loss: 0.368859]\n",
      "[Epoch 130/200][Batch 1/1][D train loss: 0.247519][G train loss: 0.326316]\n",
      "[Epoch 130/200][Batch 1/1][D eval loss: 0.249538][G eval loss: 0.370349]\n",
      "[Epoch 131/200][Batch 1/1][D train loss: 0.247473][G train loss: 0.327497]\n",
      "[Epoch 131/200][Batch 1/1][D eval loss: 0.249468][G eval loss: 0.371283]\n",
      "[Epoch 132/200][Batch 1/1][D train loss: 0.247427][G train loss: 0.328879]\n",
      "[Epoch 132/200][Batch 1/1][D eval loss: 0.249394][G eval loss: 0.371945]\n",
      "[Epoch 133/200][Batch 1/1][D train loss: 0.247380][G train loss: 0.330342]\n",
      "[Epoch 133/200][Batch 1/1][D eval loss: 0.249317][G eval loss: 0.372550]\n",
      "[Epoch 134/200][Batch 1/1][D train loss: 0.247329][G train loss: 0.331750]\n",
      "[Epoch 134/200][Batch 1/1][D eval loss: 0.249209][G eval loss: 0.373332]\n",
      "[Epoch 135/200][Batch 1/1][D train loss: 0.247274][G train loss: 0.332931]\n",
      "[Epoch 135/200][Batch 1/1][D eval loss: 0.249106][G eval loss: 0.374430]\n",
      "[Epoch 136/200][Batch 1/1][D train loss: 0.247229][G train loss: 0.334114]\n",
      "[Epoch 136/200][Batch 1/1][D eval loss: 0.248985][G eval loss: 0.375811]\n",
      "[Epoch 137/200][Batch 1/1][D train loss: 0.247183][G train loss: 0.335243]\n",
      "[Epoch 137/200][Batch 1/1][D eval loss: 0.248836][G eval loss: 0.377297]\n",
      "[Epoch 138/200][Batch 1/1][D train loss: 0.247105][G train loss: 0.336369]\n",
      "[Epoch 138/200][Batch 1/1][D eval loss: 0.248670][G eval loss: 0.379016]\n",
      "[Epoch 139/200][Batch 1/1][D train loss: 0.247044][G train loss: 0.337653]\n",
      "[Epoch 139/200][Batch 1/1][D eval loss: 0.248455][G eval loss: 0.380525]\n",
      "[Epoch 140/200][Batch 1/1][D train loss: 0.246915][G train loss: 0.339226]\n",
      "[Epoch 140/200][Batch 1/1][D eval loss: 0.248310][G eval loss: 0.381877]\n",
      "[Epoch 141/200][Batch 1/1][D train loss: 0.246851][G train loss: 0.340541]\n",
      "[Epoch 141/200][Batch 1/1][D eval loss: 0.248095][G eval loss: 0.384051]\n",
      "[Epoch 142/200][Batch 1/1][D train loss: 0.246676][G train loss: 0.342272]\n",
      "[Epoch 142/200][Batch 1/1][D eval loss: 0.247882][G eval loss: 0.386396]\n",
      "[Epoch 143/200][Batch 1/1][D train loss: 0.246593][G train loss: 0.344223]\n",
      "[Epoch 143/200][Batch 1/1][D eval loss: 0.247737][G eval loss: 0.388331]\n",
      "[Epoch 144/200][Batch 1/1][D train loss: 0.246631][G train loss: 0.345926]\n",
      "[Epoch 144/200][Batch 1/1][D eval loss: 0.247647][G eval loss: 0.389771]\n",
      "[Epoch 145/200][Batch 1/1][D train loss: 0.246590][G train loss: 0.347531]\n",
      "[Epoch 145/200][Batch 1/1][D eval loss: 0.247413][G eval loss: 0.391276]\n",
      "[Epoch 146/200][Batch 1/1][D train loss: 0.246692][G train loss: 0.348432]\n",
      "[Epoch 146/200][Batch 1/1][D eval loss: 0.247346][G eval loss: 0.392587]\n",
      "[Epoch 147/200][Batch 1/1][D train loss: 0.246687][G train loss: 0.349903]\n",
      "[Epoch 147/200][Batch 1/1][D eval loss: 0.247361][G eval loss: 0.393939]\n",
      "[Epoch 148/200][Batch 1/1][D train loss: 0.246649][G train loss: 0.350706]\n",
      "[Epoch 148/200][Batch 1/1][D eval loss: 0.247181][G eval loss: 0.395810]\n",
      "[Epoch 149/200][Batch 1/1][D train loss: 0.246582][G train loss: 0.351709]\n",
      "[Epoch 149/200][Batch 1/1][D eval loss: 0.246682][G eval loss: 0.397806]\n",
      "[Epoch 150/200][Batch 1/1][D train loss: 0.246521][G train loss: 0.352589]\n",
      "[Epoch 150/200][Batch 1/1][D eval loss: 0.246229][G eval loss: 0.399321]\n",
      "[Epoch 151/200][Batch 1/1][D train loss: 0.246342][G train loss: 0.353408]\n",
      "[Epoch 151/200][Batch 1/1][D eval loss: 0.245499][G eval loss: 0.400962]\n",
      "[Epoch 152/200][Batch 1/1][D train loss: 0.246147][G train loss: 0.353973]\n",
      "[Epoch 152/200][Batch 1/1][D eval loss: 0.244807][G eval loss: 0.403215]\n",
      "[Epoch 153/200][Batch 1/1][D train loss: 0.245880][G train loss: 0.355053]\n",
      "[Epoch 153/200][Batch 1/1][D eval loss: 0.244252][G eval loss: 0.405163]\n",
      "[Epoch 154/200][Batch 1/1][D train loss: 0.245652][G train loss: 0.356129]\n",
      "[Epoch 154/200][Batch 1/1][D eval loss: 0.243572][G eval loss: 0.406967]\n",
      "[Epoch 155/200][Batch 1/1][D train loss: 0.245386][G train loss: 0.356471]\n",
      "[Epoch 155/200][Batch 1/1][D eval loss: 0.242784][G eval loss: 0.408374]\n",
      "[Epoch 156/200][Batch 1/1][D train loss: 0.244929][G train loss: 0.356642]\n",
      "[Epoch 156/200][Batch 1/1][D eval loss: 0.241832][G eval loss: 0.411921]\n",
      "[Epoch 157/200][Batch 1/1][D train loss: 0.244497][G train loss: 0.358643]\n",
      "[Epoch 157/200][Batch 1/1][D eval loss: 0.241294][G eval loss: 0.414070]\n",
      "[Epoch 158/200][Batch 1/1][D train loss: 0.244221][G train loss: 0.359444]\n",
      "[Epoch 158/200][Batch 1/1][D eval loss: 0.240672][G eval loss: 0.415436]\n",
      "[Epoch 159/200][Batch 1/1][D train loss: 0.243723][G train loss: 0.360467]\n",
      "[Epoch 159/200][Batch 1/1][D eval loss: 0.240017][G eval loss: 0.417177]\n",
      "[Epoch 160/200][Batch 1/1][D train loss: 0.243282][G train loss: 0.361345]\n",
      "[Epoch 160/200][Batch 1/1][D eval loss: 0.239031][G eval loss: 0.418216]\n",
      "[Epoch 161/200][Batch 1/1][D train loss: 0.242569][G train loss: 0.361890]\n",
      "[Epoch 161/200][Batch 1/1][D eval loss: 0.238033][G eval loss: 0.424316]\n",
      "[Epoch 162/200][Batch 1/1][D train loss: 0.241930][G train loss: 0.364442]\n",
      "[Epoch 162/200][Batch 1/1][D eval loss: 0.237466][G eval loss: 0.427914]\n",
      "[Epoch 163/200][Batch 1/1][D train loss: 0.241523][G train loss: 0.366965]\n",
      "[Epoch 163/200][Batch 1/1][D eval loss: 0.236399][G eval loss: 0.431361]\n",
      "[Epoch 164/200][Batch 1/1][D train loss: 0.241180][G train loss: 0.367701]\n",
      "[Epoch 164/200][Batch 1/1][D eval loss: 0.235747][G eval loss: 0.431144]\n",
      "[Epoch 165/200][Batch 1/1][D train loss: 0.240745][G train loss: 0.367132]\n",
      "[Epoch 165/200][Batch 1/1][D eval loss: 0.234604][G eval loss: 0.431697]\n",
      "[Epoch 166/200][Batch 1/1][D train loss: 0.240091][G train loss: 0.366831]\n",
      "[Epoch 166/200][Batch 1/1][D eval loss: 0.233325][G eval loss: 0.432766]\n",
      "[Epoch 167/200][Batch 1/1][D train loss: 0.239610][G train loss: 0.365519]\n",
      "[Epoch 167/200][Batch 1/1][D eval loss: 0.232395][G eval loss: 0.431385]\n",
      "[Epoch 168/200][Batch 1/1][D train loss: 0.238872][G train loss: 0.363847]\n",
      "[Epoch 168/200][Batch 1/1][D eval loss: 0.231512][G eval loss: 0.429229]\n",
      "[Epoch 169/200][Batch 1/1][D train loss: 0.237818][G train loss: 0.361755]\n",
      "[Epoch 169/200][Batch 1/1][D eval loss: 0.230690][G eval loss: 0.428703]\n",
      "[Epoch 170/200][Batch 1/1][D train loss: 0.237200][G train loss: 0.360283]\n",
      "[Epoch 170/200][Batch 1/1][D eval loss: 0.229340][G eval loss: 0.432157]\n",
      "[Epoch 171/200][Batch 1/1][D train loss: 0.236432][G train loss: 0.359974]\n",
      "[Epoch 171/200][Batch 1/1][D eval loss: 0.228558][G eval loss: 0.434020]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.0045 lr_d=0.001 -> score=0.662578\n",
      "\n",
      "----- 2330: lr_g=0.0045, lr_d=0.0012 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 0.486292]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250027][G train loss: 0.450600]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.479923]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 0.444362]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.481482]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.446120]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 0.482475]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249953][G train loss: 0.447205]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249937][G eval loss: 0.481927]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249912][G train loss: 0.446598]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249876][G eval loss: 0.481018]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249835][G train loss: 0.445568]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249785][G eval loss: 0.480284]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249723][G train loss: 0.444704]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249646][G eval loss: 0.480220]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249550][G train loss: 0.444537]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249458][G eval loss: 0.479573]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249315][G train loss: 0.443844]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249193][G eval loss: 0.478328]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248978][G train loss: 0.442610]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248807][G eval loss: 0.477089]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248471][G train loss: 0.441428]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248228][G eval loss: 0.476347]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247733][G train loss: 0.440754]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247507][G eval loss: 0.476152]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246800][G train loss: 0.440618]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246524][G eval loss: 0.475808]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245552][G train loss: 0.440313]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.245153][G eval loss: 0.476429]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243831][G train loss: 0.440921]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.243316][G eval loss: 0.478305]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241534][G train loss: 0.442749]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240931][G eval loss: 0.480987]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238556][G train loss: 0.445365]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.238057][G eval loss: 0.483880]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.234955][G train loss: 0.448192]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.234699][G eval loss: 0.486707]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230704][G train loss: 0.450998]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.230272][G eval loss: 0.490388]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.225316][G train loss: 0.454681]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.224907][G eval loss: 0.494166]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.218991][G train loss: 0.458498]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.218360][G eval loss: 0.498574]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.211545][G train loss: 0.462974]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.210325][G eval loss: 0.503805]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.202705][G train loss: 0.468228]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.200473][G eval loss: 0.510305]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.192220][G train loss: 0.474723]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.189452][G eval loss: 0.517455]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.180458][G train loss: 0.481873]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.177219][G eval loss: 0.525273]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.167460][G train loss: 0.489663]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.163555][G eval loss: 0.534419]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.154040][G train loss: 0.498758]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.148863][G eval loss: 0.545109]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.141038][G train loss: 0.509380]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.135718][G eval loss: 0.554413]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.130157][G train loss: 0.518363]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.130735][G eval loss: 0.549266]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.127296][G train loss: 0.511469]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.224962][G eval loss: 0.385034]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.233680][G train loss: 0.332835]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.323702][G eval loss: 0.307164]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.323383][G train loss: 0.270017]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.353376][G eval loss: 0.303166]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.351374][G train loss: 0.266807]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.365669][G eval loss: 0.294487]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.363509][G train loss: 0.258097]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.372865][G eval loss: 0.283133]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.370912][G train loss: 0.246987]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.377203][G eval loss: 0.273598]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.375619][G train loss: 0.237019]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.379643][G eval loss: 0.266727]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.378488][G train loss: 0.228800]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.380693][G eval loss: 0.262613]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.381451][G train loss: 0.222563]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.380809][G eval loss: 0.259467]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.383567][G train loss: 0.217323]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.379986][G eval loss: 0.253299]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.381912][G train loss: 0.211141]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.378717][G eval loss: 0.252664]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.382479][G train loss: 0.207974]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.376705][G eval loss: 0.249009]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.380605][G train loss: 0.202747]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.374023][G eval loss: 0.242573]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.377737][G train loss: 0.195526]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.370691][G eval loss: 0.234509]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.373895][G train loss: 0.187445]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.366952][G eval loss: 0.226386]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.368625][G train loss: 0.179756]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.362875][G eval loss: 0.218714]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.362781][G train loss: 0.173599]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.358369][G eval loss: 0.212533]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.358320][G train loss: 0.169914]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.353433][G eval loss: 0.208596]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.353138][G train loss: 0.168225]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.347832][G eval loss: 0.207256]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.346710][G train loss: 0.169180]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.341810][G eval loss: 0.208164]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.339302][G train loss: 0.172413]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.335417][G eval loss: 0.210681]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.331833][G train loss: 0.177041]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.328797][G eval loss: 0.214336]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.324647][G train loss: 0.180788]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.321925][G eval loss: 0.217915]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.317882][G train loss: 0.183787]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.314893][G eval loss: 0.221908]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.311574][G train loss: 0.186409]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.307778][G eval loss: 0.226386]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.305399][G train loss: 0.189210]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.300681][G eval loss: 0.232166]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299433][G train loss: 0.193134]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.293688][G eval loss: 0.239575]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.293331][G train loss: 0.198927]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.287007][G eval loss: 0.249333]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.286893][G train loss: 0.207356]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.280681][G eval loss: 0.261383]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.0045 lr_d=0.0012 -> score=0.542064\n",
      "\n",
      "----- 2330: lr_g=0.005, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.488970]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 0.453278]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.480091]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.444559]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.478667]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.443364]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.478368]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.443093]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.478586]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249970][G train loss: 0.443204]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.479261]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249959][G train loss: 0.443731]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.479844]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249943][G train loss: 0.444173]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.479557]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249924][G train loss: 0.443800]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 0.478221]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249901][G train loss: 0.442470]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 0.476488]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249872][G train loss: 0.440822]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249871][G eval loss: 0.475155]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249837][G train loss: 0.439593]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249835][G eval loss: 0.474289]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249792][G train loss: 0.438784]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249789][G eval loss: 0.473846]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249735][G train loss: 0.438334]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249730][G eval loss: 0.473887]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249663][G train loss: 0.438320]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249658][G eval loss: 0.474331]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.005 lr_d=0.0005 -> score=0.723989\n",
      "\n",
      "----- 2330: lr_g=0.005, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.487040]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250007][G train loss: 0.451348]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.479802]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.444269]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.479689]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 0.444385]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.479956]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 0.444681]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.479936]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249958][G train loss: 0.444553]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 0.480033]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249932][G train loss: 0.444503]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249917][G eval loss: 0.479969]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249893][G train loss: 0.444298]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249874][G eval loss: 0.479174]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249840][G train loss: 0.443417]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249818][G eval loss: 0.477483]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249771][G train loss: 0.441733]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249746][G eval loss: 0.475736]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249681][G train loss: 0.440071]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249646][G eval loss: 0.474486]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249558][G train loss: 0.438925]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249507][G eval loss: 0.473775]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249387][G train loss: 0.438272]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249318][G eval loss: 0.473567]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249150][G train loss: 0.438057]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249064][G eval loss: 0.473936]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248825][G train loss: 0.438371]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248729][G eval loss: 0.474770]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248410][G train loss: 0.439148]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248362][G eval loss: 0.475934]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247941][G train loss: 0.440278]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247888][G eval loss: 0.477311]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247337][G train loss: 0.441616]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247296][G eval loss: 0.478561]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246585][G train loss: 0.442864]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246727][G eval loss: 0.479551]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245818][G train loss: 0.443867]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245888][G eval loss: 0.480336]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244753][G train loss: 0.444685]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244817][G eval loss: 0.481186]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243413][G train loss: 0.445572]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243511][G eval loss: 0.482020]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241793][G train loss: 0.446433]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241891][G eval loss: 0.482964]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239818][G train loss: 0.447370]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239967][G eval loss: 0.484093]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.005 lr_d=0.0008 -> score=0.724060\n",
      "\n",
      "----- 2330: lr_g=0.005, lr_d=0.001 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 0.485860]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250016][G train loss: 0.450168]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.479617]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 0.444084]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.480389]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249975][G train loss: 0.445085]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.481009]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249964][G train loss: 0.445733]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.480867]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249939][G train loss: 0.445484]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 0.480650]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249892][G train loss: 0.445120]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249859][G eval loss: 0.480244]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249819][G train loss: 0.444574]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249778][G eval loss: 0.479373]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249720][G train loss: 0.443617]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249668][G eval loss: 0.477643]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249584][G train loss: 0.441893]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249514][G eval loss: 0.475740]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249393][G train loss: 0.440076]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249296][G eval loss: 0.474470]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249117][G train loss: 0.438909]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249001][G eval loss: 0.473882]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248735][G train loss: 0.438382]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248563][G eval loss: 0.473972]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248177][G train loss: 0.438466]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247989][G eval loss: 0.474809]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247449][G train loss: 0.439249]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247267][G eval loss: 0.476247]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246531][G train loss: 0.440633]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246484][G eval loss: 0.477967]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245493][G train loss: 0.442316]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245293][G eval loss: 0.479648]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.243986][G train loss: 0.443958]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243755][G eval loss: 0.481246]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242044][G train loss: 0.445559]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241895][G eval loss: 0.482793]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239711][G train loss: 0.447126]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.239577][G eval loss: 0.484614]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236830][G train loss: 0.448976]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.236819][G eval loss: 0.486549]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.233441][G train loss: 0.450943]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.233548][G eval loss: 0.488626]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.229474][G train loss: 0.453045]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.229583][G eval loss: 0.490961]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.224803][G train loss: 0.455375]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.224868][G eval loss: 0.493903]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.219322][G train loss: 0.458322]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.219419][G eval loss: 0.497231]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.213135][G train loss: 0.461670]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.212997][G eval loss: 0.501296]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.206034][G train loss: 0.465727]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.205648][G eval loss: 0.506074]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.198009][G train loss: 0.470462]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.197457][G eval loss: 0.511370]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.189253][G train loss: 0.475641]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.189266][G eval loss: 0.516114]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.180480][G train loss: 0.480083]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.185721][G eval loss: 0.511757]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.176821][G train loss: 0.474237]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.208441][G eval loss: 0.467015]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.206631][G train loss: 0.415230]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.243572][G eval loss: 0.420711]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.248228][G train loss: 0.361252]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.275235][G eval loss: 0.383985]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.283300][G train loss: 0.319280]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.311686][G eval loss: 0.328774]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.308891][G train loss: 0.282800]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.339386][G eval loss: 0.299544]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.328765][G train loss: 0.260052]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.349028][G eval loss: 0.303578]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.335946][G train loss: 0.265812]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.353426][G eval loss: 0.313568]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.339267][G train loss: 0.274417]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.355461][G eval loss: 0.316910]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.340963][G train loss: 0.275315]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.355913][G eval loss: 0.310692]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.341645][G train loss: 0.266869]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.355057][G eval loss: 0.298515]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.341661][G train loss: 0.252984]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.352793][G eval loss: 0.285006]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.341529][G train loss: 0.238277]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.348299][G eval loss: 0.275730]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.341390][G train loss: 0.228002]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.343332][G eval loss: 0.269700]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.339615][G train loss: 0.220529]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.338479][G eval loss: 0.262230]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.335719][G train loss: 0.212114]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.333618][G eval loss: 0.253367]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.331378][G train loss: 0.204384]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.328816][G eval loss: 0.244747]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.325975][G train loss: 0.200809]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.324034][G eval loss: 0.239174]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.320055][G train loss: 0.202410]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.318905][G eval loss: 0.237937]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.311143][G train loss: 0.211066]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.312801][G eval loss: 0.241890]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.308191][G train loss: 0.212265]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.304547][G eval loss: 0.249001]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.302272][G train loss: 0.216767]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.295496][G eval loss: 0.258607]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.298040][G train loss: 0.220149]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.287256][G eval loss: 0.269184]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.294333][G train loss: 0.222483]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.281281][G eval loss: 0.276962]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.005 lr_d=0.001 -> score=0.558243\n",
      "\n",
      "----- 2330: lr_g=0.005, lr_d=0.0012 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 0.484677]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250028][G train loss: 0.448985]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.479410]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 0.443878]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.481058]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.445755]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.482037]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249956][G train loss: 0.446762]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.481828]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249917][G train loss: 0.446445]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249880][G eval loss: 0.481366]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249840][G train loss: 0.445836]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249788][G eval loss: 0.480790]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249728][G train loss: 0.445121]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249653][G eval loss: 0.480314]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249561][G train loss: 0.444559]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249471][G eval loss: 0.478932]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249334][G train loss: 0.443184]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249216][G eval loss: 0.477219]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249009][G train loss: 0.441557]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248840][G eval loss: 0.475965]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248518][G train loss: 0.440406]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248274][G eval loss: 0.475351]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247798][G train loss: 0.439854]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247529][G eval loss: 0.475349]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246845][G train loss: 0.439850]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246461][G eval loss: 0.475731]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245510][G train loss: 0.440179]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244977][G eval loss: 0.476677]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243678][G train loss: 0.441069]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.243112][G eval loss: 0.478627]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.005 lr_d=0.0012 -> score=0.721739\n",
      "\n",
      "----- 2330: lr_g=0.006, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.486456]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 0.450758]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.479562]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.444128]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 0.478264]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.442998]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 0.478138]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249978][G train loss: 0.442797]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.478972]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249970][G train loss: 0.443473]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.480007]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249959][G train loss: 0.444351]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.479843]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249944][G train loss: 0.444110]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.478429]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249925][G train loss: 0.442732]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249923][G eval loss: 0.476772]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249902][G train loss: 0.441167]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 0.475340]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249873][G train loss: 0.439796]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249869][G eval loss: 0.474203]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249835][G train loss: 0.438634]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249829][G eval loss: 0.473613]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249787][G train loss: 0.437978]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249780][G eval loss: 0.473672]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249727][G train loss: 0.437996]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249721][G eval loss: 0.474206]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.006 lr_d=0.0005 -> score=0.723928\n",
      "\n",
      "----- 2330: lr_g=0.006, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.484525]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 0.448827]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.479276]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 0.443842]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.479288]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 0.444022]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.479723]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249974][G train loss: 0.444383]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.480315]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249960][G train loss: 0.444815]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 0.480766]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249932][G train loss: 0.445111]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 0.479945]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249895][G train loss: 0.444212]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249879][G eval loss: 0.478011]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249845][G train loss: 0.442314]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249827][G eval loss: 0.475982]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249780][G train loss: 0.440378]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249756][G eval loss: 0.474508]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249691][G train loss: 0.438966]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249653][G eval loss: 0.473451]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249563][G train loss: 0.437884]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249504][G eval loss: 0.473018]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249382][G train loss: 0.437387]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249310][G eval loss: 0.473312]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249139][G train loss: 0.437638]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249056][G eval loss: 0.474171]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248815][G train loss: 0.438479]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248715][G eval loss: 0.475124]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248392][G train loss: 0.439442]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248359][G eval loss: 0.476063]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247936][G train loss: 0.440412]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247913][G eval loss: 0.477107]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247366][G train loss: 0.441496]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247357][G eval loss: 0.478206]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246656][G train loss: 0.442616]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246818][G eval loss: 0.479134]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245927][G train loss: 0.443540]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.246035][G eval loss: 0.479696]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244923][G train loss: 0.444131]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244981][G eval loss: 0.480510]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243604][G train loss: 0.444977]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243711][G eval loss: 0.481497]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242011][G train loss: 0.445954]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.242201][G eval loss: 0.482513]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240130][G train loss: 0.446940]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.240406][G eval loss: 0.483647]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237910][G train loss: 0.448038]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.238285][G eval loss: 0.484812]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.006 lr_d=0.0008 -> score=0.723097\n",
      "\n",
      "----- 2330: lr_g=0.006, lr_d=0.001 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250019][G eval loss: 0.483345]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 0.447647]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.479092]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 0.443658]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.479990]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.444724]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.480774]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249968][G train loss: 0.445433]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249960][G eval loss: 0.481237]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249943][G train loss: 0.445738]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249921][G eval loss: 0.481364]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249895][G train loss: 0.445709]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249867][G eval loss: 0.480186]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249829][G train loss: 0.444453]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249793][G eval loss: 0.478056]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249737][G train loss: 0.442359]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249692][G eval loss: 0.475896]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249609][G train loss: 0.440293]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249544][G eval loss: 0.474233]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249424][G train loss: 0.438693]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249325][G eval loss: 0.473165]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249147][G train loss: 0.437601]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249001][G eval loss: 0.472919]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248736][G train loss: 0.437292]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248555][G eval loss: 0.473555]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248173][G train loss: 0.437885]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247978][G eval loss: 0.474937]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247453][G train loss: 0.439248]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247295][G eval loss: 0.476631]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246588][G train loss: 0.440952]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246600][G eval loss: 0.478180]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245653][G train loss: 0.442536]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245479][G eval loss: 0.479515]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244238][G train loss: 0.443909]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.244031][G eval loss: 0.480868]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242414][G train loss: 0.445286]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.242264][G eval loss: 0.482258]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.240196][G train loss: 0.446675]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.240107][G eval loss: 0.483721]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.237504][G train loss: 0.448176]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.237434][G eval loss: 0.485658]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.234216][G train loss: 0.450153]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.234267][G eval loss: 0.487756]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.230369][G train loss: 0.452263]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.230524][G eval loss: 0.490146]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.225903][G train loss: 0.454660]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.226081][G eval loss: 0.493110]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.220675][G train loss: 0.457623]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.220934][G eval loss: 0.496271]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.214711][G train loss: 0.460826]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.214861][G eval loss: 0.499848]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.207835][G train loss: 0.464470]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.208028][G eval loss: 0.503539]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.200162][G train loss: 0.468251]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.201768][G eval loss: 0.504377]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.193174][G train loss: 0.468885]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.207451][G eval loss: 0.482292]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.196487][G train loss: 0.449494]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.224358][G eval loss: 0.443893]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.210299][G train loss: 0.413509]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.242550][G eval loss: 0.397811]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.227149][G train loss: 0.368711]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.263870][G eval loss: 0.351413]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.241166][G train loss: 0.335628]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.285137][G eval loss: 0.322844]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249463][G train loss: 0.323954]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.295086][G eval loss: 0.327283]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.255129][G train loss: 0.333564]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.300788][G eval loss: 0.332653]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.261001][G train loss: 0.335581]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.303963][G eval loss: 0.330836]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.267876][G train loss: 0.321593]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.310601][G eval loss: 0.300581]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.277055][G train loss: 0.289228]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.336230][G eval loss: 0.253710]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.281112][G train loss: 0.267089]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.321416][G eval loss: 0.271826]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.278826][G train loss: 0.273510]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.315631][G eval loss: 0.280895]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.291471][G train loss: 0.255659]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.325493][G eval loss: 0.271581]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.306187][G train loss: 0.239850]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.328628][G eval loss: 0.265616]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.317660][G train loss: 0.223204]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.328397][G eval loss: 0.269415]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.325027][G train loss: 0.217931]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.326207][G eval loss: 0.266457]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.323328][G train loss: 0.213486]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.323134][G eval loss: 0.256509]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.319273][G train loss: 0.202630]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.319538][G eval loss: 0.247151]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.312854][G train loss: 0.196074]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.315807][G eval loss: 0.244943]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.301545][G train loss: 0.208135]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.312182][G eval loss: 0.245959]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.301474][G train loss: 0.203454]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.308512][G eval loss: 0.244727]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.298179][G train loss: 0.205550]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.295399][G eval loss: 0.254236]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.294744][G train loss: 0.208437]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.281824][G eval loss: 0.280985]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.291127][G train loss: 0.213436]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.268491][G eval loss: 0.299174]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.006 lr_d=0.001 -> score=0.567665\n",
      "\n",
      "----- 2330: lr_g=0.006, lr_d=0.0012 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250031][G eval loss: 0.482162]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250028][G train loss: 0.446464]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.478886]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 0.443452]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.480664]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249971][G train loss: 0.445398]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.481808]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249960][G train loss: 0.446468]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249945][G eval loss: 0.482203]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249922][G train loss: 0.446704]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 0.482085]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249846][G train loss: 0.446430]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249793][G eval loss: 0.480733]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249737][G train loss: 0.445001]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249669][G eval loss: 0.479108]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249582][G train loss: 0.443412]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249499][G eval loss: 0.477386]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249369][G train loss: 0.441785]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249252][G eval loss: 0.475971]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249054][G train loss: 0.440431]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248871][G eval loss: 0.474938]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248561][G train loss: 0.439374]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248294][G eval loss: 0.474599]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247835][G train loss: 0.438975]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247545][G eval loss: 0.475100]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246885][G train loss: 0.439434]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246492][G eval loss: 0.475647]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245585][G train loss: 0.439961]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.245168][G eval loss: 0.476701]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243944][G train loss: 0.441026]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.243498][G eval loss: 0.478523]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241845][G train loss: 0.442893]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.241312][G eval loss: 0.480881]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.239096][G train loss: 0.445289]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.238629][G eval loss: 0.483209]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.235726][G train loss: 0.447639]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.235445][G eval loss: 0.485637]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.231694][G train loss: 0.450058]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.231396][G eval loss: 0.488583]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.226730][G train loss: 0.453059]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.226434][G eval loss: 0.492306]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.220808][G train loss: 0.456846]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.220190][G eval loss: 0.496682]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.213629][G train loss: 0.461252]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.212669][G eval loss: 0.501827]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.205248][G train loss: 0.466406]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.203591][G eval loss: 0.508177]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.195423][G train loss: 0.472768]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.193160][G eval loss: 0.515377]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.184345][G train loss: 0.479982]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.182047][G eval loss: 0.522684]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.172289][G train loss: 0.487367]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.170748][G eval loss: 0.528595]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.160406][G train loss: 0.493504]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.175183][G eval loss: 0.502442]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.165780][G train loss: 0.467346]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.223222][G eval loss: 0.426127]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.210305][G train loss: 0.399542]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.279192][G eval loss: 0.356431]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.249837][G train loss: 0.348687]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.330939][G eval loss: 0.293641]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.287587][G train loss: 0.302091]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.350301][G eval loss: 0.280690]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.322284][G train loss: 0.262450]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.358164][G eval loss: 0.300041]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "[INFO] 2330 lr_g=0.006 lr_d=0.0012 -> score=0.658205\n",
      "\n",
      ">>> Best config for 2330: lr_g=0.0045, lr_d=0.0012, score=0.542064\n",
      "\n",
      "========== Grid search finished ==========\n",
      "0050: best lr_g=0.004, lr_d=0.0008, score=0.700129\n",
      "0056: best lr_g=0.006, lr_d=0.0005, score=0.668898\n",
      "2330: best lr_g=0.0045, lr_d=0.0012, score=0.542064\n"
     ]
    }
   ],
   "source": [
    "from LOB_GAN_training_raw import train_lob_gan\n",
    "\n",
    "# \n",
    "stock_list = [\"0050\", \"0056\", \"2330\"]\n",
    "\n",
    "# \n",
    "lr_g_candidates = [0.0040, 0.0045, 0.005,0.006]\n",
    "lr_d_candidates = [0.0005,0.0008, 0.0010, 0.0012]\n",
    "\n",
    "batch_size = 50\n",
    "seed = 307\n",
    "\n",
    "#  (stock, lr_g, lr_d) -> dict(...)\n",
    "all_results = {}\n",
    "\n",
    "# \n",
    "best_config = {}\n",
    "best_score = {}\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"\\n========== Grid search for {stock} ==========\")\n",
    "    best_config[stock] = None\n",
    "    best_score[stock] = float(\"inf\")\n",
    "\n",
    "    for lr_g in lr_g_candidates:\n",
    "        for lr_d in lr_d_candidates:\n",
    "            print(f\"\\n----- {stock}: lr_g={lr_g}, lr_d={lr_d} -----\")\n",
    "            res = train_lob_gan(\n",
    "                stock=stock,\n",
    "                lr_g=lr_g,\n",
    "                lr_d=lr_d,\n",
    "                batch_size=batch_size,\n",
    "                seed=seed,\n",
    "            )\n",
    "\n",
    "            if res is None:\n",
    "                print(f\"[WARN] {stock} lr_g={lr_g} lr_d={lr_d}  None\")\n",
    "                continue\n",
    "\n",
    "            eval_g = res[\"eval_g_loss\"]\n",
    "            eval_d = res[\"eval_d_loss\"]\n",
    "            if len(eval_g) == 0 or len(eval_d) == 0:\n",
    "                print(f\"[WARN] {stock} lr_g={lr_g} lr_d={lr_d}  eval loss\")\n",
    "                continue\n",
    "\n",
    "            score = eval_g[-1] + eval_d[-1]\n",
    "            print(f\"[INFO] {stock} lr_g={lr_g} lr_d={lr_d} -> score={score:.6f}\")\n",
    "\n",
    "            all_results[(stock, lr_g, lr_d)] = {\n",
    "                \"res\": res,\n",
    "                \"score\": score,\n",
    "            }\n",
    "\n",
    "            if score < best_score[stock]:\n",
    "                best_score[stock] = score\n",
    "                best_config[stock] = (lr_g, lr_d)\n",
    "\n",
    "    print(f\"\\n>>> Best config for {stock}: \"\n",
    "          f\"lr_g={best_config[stock][0]}, lr_d={best_config[stock][1]}, \"\n",
    "          f\"score={best_score[stock]:.6f}\")\n",
    "\n",
    "print(\"\\n========== Grid search finished ==========\")\n",
    "for stock in stock_list:\n",
    "    if best_config[stock] is not None:\n",
    "        print(f\"{stock}: best lr_g={best_config[stock][0]}, \"\n",
    "              f\"lr_d={best_config[stock][1]}, score={best_score[stock]:.6f}\")\n",
    "    else:\n",
    "        print(f\"{stock}: no valid config found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50415595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training ADJUSTED GAN for 0050 =====\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.297082]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.118483]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.270163]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.108594]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.260239]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 1.108031]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.259625]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 1.107663]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.259283]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 1.106205]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249957][G train loss: 1.257858]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249945][G eval loss: 1.105429]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249931][G train loss: 1.257097]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 1.105595]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249897][G train loss: 1.257266]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249881][G eval loss: 1.105764]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249853][G train loss: 1.257418]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 1.105285]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249790][G train loss: 1.256906]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249755][G eval loss: 1.104132]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249703][G train loss: 1.255709]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249655][G eval loss: 1.102789]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249586][G train loss: 1.254317]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249519][G eval loss: 1.101652]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249425][G train loss: 1.253123]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249342][G eval loss: 1.100688]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249220][G train loss: 1.252100]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249104][G eval loss: 1.100070]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248947][G train loss: 1.251417]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248789][G eval loss: 1.099955]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248589][G train loss: 1.251228]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248377][G eval loss: 1.100421]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248125][G train loss: 1.251626]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247867][G eval loss: 1.100872]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247550][G train loss: 1.252012]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247307][G eval loss: 1.100510]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246920][G train loss: 1.251514]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246465][G eval loss: 1.098973]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245998][G train loss: 1.249774]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245433][G eval loss: 1.096361]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244867][G train loss: 1.246884]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244225][G eval loss: 1.092349]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243558][G train loss: 1.242497]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242773][G eval loss: 1.086392]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242006][G train loss: 1.236056]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241052][G eval loss: 1.077646]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240170][G train loss: 1.226677]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239045][G eval loss: 1.065128]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238014][G train loss: 1.213254]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236814][G eval loss: 1.047958]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235562][G train loss: 1.194925]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234458][G eval loss: 1.025588]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.232879][G train loss: 1.171072]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231960][G eval loss: 0.996933]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.230013][G train loss: 1.140806]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.229413][G eval loss: 0.961122]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.226988][G train loss: 1.103639]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.227096][G eval loss: 0.918504]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.224001][G train loss: 1.059771]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.225587][G eval loss: 0.871323]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.221428][G train loss: 1.010458]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.226078][G eval loss: 0.821702]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.219976][G train loss: 0.957120]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.229495][G eval loss: 0.771464]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.220964][G train loss: 0.899839]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.235891][G eval loss: 0.727185]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.225621][G train loss: 0.842304]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.244287][G eval loss: 0.696898]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.234947][G train loss: 0.791124]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.256125][G eval loss: 0.679493]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248008][G train loss: 0.753258]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.269728][G eval loss: 0.672261]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.261403][G train loss: 0.730609]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.282324][G eval loss: 0.669270]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.273122][G train loss: 0.715017]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.292606][G eval loss: 0.660643]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.282797][G train loss: 0.697548]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.300592][G eval loss: 0.637728]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.290857][G train loss: 0.674207]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.306993][G eval loss: 0.604636]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.297357][G train loss: 0.643970]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.311947][G eval loss: 0.567230]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.302066][G train loss: 0.612784]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.315593][G eval loss: 0.531245]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.305331][G train loss: 0.587449]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.317739][G eval loss: 0.506741]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.307374][G train loss: 0.575154]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.318056][G eval loss: 0.495672]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.308173][G train loss: 0.572496]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.316369][G eval loss: 0.490602]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.307499][G train loss: 0.574058]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.313640][G eval loss: 0.488337]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.305870][G train loss: 0.573497]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.310144][G eval loss: 0.487382]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.303171][G train loss: 0.569973]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.306285][G eval loss: 0.486348]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.299788][G train loss: 0.562555]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.302282][G eval loss: 0.482679]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.296549][G train loss: 0.554732]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.297764][G eval loss: 0.481197]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.293635][G train loss: 0.549131]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.292555][G eval loss: 0.483055]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.290931][G train loss: 0.547108]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.287708][G eval loss: 0.483770]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.288009][G train loss: 0.545790]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.283412][G eval loss: 0.480921]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.284894][G train loss: 0.545219]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.279523][G eval loss: 0.479344]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.281514][G train loss: 0.545060]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.275876][G eval loss: 0.482349]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.277995][G train loss: 0.546494]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.273022][G eval loss: 0.486523]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve_adjusted.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1_adjusted.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1_adjusted.pth\n",
      "Done training ADJUSTED LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "===== Training ADJUSTED GAN for 0056 =====\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.006, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.115726]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.981110]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.083683]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.975169]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.078262]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.971721]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.075157]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.971442]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.074874]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.973303]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.076593]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.975201]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.078314]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.974955]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249941][G train loss: 1.077922]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 0.973092]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249920][G train loss: 1.075968]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.970842]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249896][G train loss: 1.073674]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249896][G eval loss: 0.969009]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249866][G train loss: 1.071847]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249865][G eval loss: 0.967904]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249827][G train loss: 1.070757]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249824][G eval loss: 0.967371]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249777][G train loss: 1.070202]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249771][G eval loss: 0.967650]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249713][G train loss: 1.070452]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249708][G eval loss: 0.967735]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249637][G train loss: 1.070535]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249633][G eval loss: 0.967628]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249548][G train loss: 1.070414]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249548][G eval loss: 0.967049]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249447][G train loss: 1.069774]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249454][G eval loss: 0.965682]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249334][G train loss: 1.068256]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249335][G eval loss: 0.963174]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249191][G train loss: 1.065523]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249195][G eval loss: 0.959323]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249021][G train loss: 1.061409]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249039][G eval loss: 0.953069]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248826][G train loss: 1.054853]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248864][G eval loss: 0.942309]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248603][G train loss: 1.043792]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248665][G eval loss: 0.924487]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248347][G train loss: 1.025701]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248449][G eval loss: 0.898480]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248068][G train loss: 0.999306]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248232][G eval loss: 0.862520]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247769][G train loss: 0.962614]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.248043][G eval loss: 0.814798]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247461][G train loss: 0.913970]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247913][G eval loss: 0.757776]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.247171][G train loss: 0.856289]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.247927][G eval loss: 0.695337]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.247014][G train loss: 0.792650]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.248227][G eval loss: 0.634249]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.247069][G train loss: 0.727884]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.248680][G eval loss: 0.593514]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.247286][G train loss: 0.678918]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.248966][G eval loss: 0.566917]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.247241][G train loss: 0.647101]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.249388][G eval loss: 0.567214]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.247254][G train loss: 0.643039]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.250405][G eval loss: 0.583749]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve_adjusted.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1_adjusted.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1_adjusted.pth\n",
      "Done training ADJUSTED LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "===== Training ADJUSTED GAN for 2330 =====\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.483047]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250029][G eval loss: 0.487091]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250027][G train loss: 0.451488]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.480732]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 0.445105]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.481630]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 0.446171]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.482074]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249951][G train loss: 0.446774]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249937][G eval loss: 0.481850]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249911][G train loss: 0.446582]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249880][G eval loss: 0.481423]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249837][G train loss: 0.446085]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249786][G eval loss: 0.480876]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249717][G train loss: 0.445414]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249639][G eval loss: 0.479943]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249533][G train loss: 0.444349]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249412][G eval loss: 0.478640]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249251][G train loss: 0.442943]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249114][G eval loss: 0.477040]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248870][G train loss: 0.441309]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248667][G eval loss: 0.475671]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248288][G train loss: 0.439971]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248019][G eval loss: 0.475138]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247463][G train loss: 0.439507]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247203][G eval loss: 0.475680]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246409][G train loss: 0.440118]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246175][G eval loss: 0.477194]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245053][G train loss: 0.441689]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244656][G eval loss: 0.478804]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243103][G train loss: 0.443308]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242547][G eval loss: 0.481147]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.240436][G train loss: 0.445610]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.239972][G eval loss: 0.483916]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.237150][G train loss: 0.448309]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236854][G eval loss: 0.486808]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.233199][G train loss: 0.451134]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232927][G eval loss: 0.489856]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.228380][G train loss: 0.454123]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227903][G eval loss: 0.493456]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.222427][G train loss: 0.457706]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221836][G eval loss: 0.496956]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.215438][G train loss: 0.461226]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.214310][G eval loss: 0.501308]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.207060][G train loss: 0.465615]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.205278][G eval loss: 0.506674]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.197270][G train loss: 0.471019]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.194673][G eval loss: 0.513109]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.186036][G train loss: 0.477491]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.183075][G eval loss: 0.519711]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.173645][G train loss: 0.484091]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.169927][G eval loss: 0.527421]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.160346][G train loss: 0.491730]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.156479][G eval loss: 0.535092]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.148102][G train loss: 0.499277]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.146365][G eval loss: 0.537620]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.140044][G train loss: 0.501249]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.174167][G eval loss: 0.470898]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.176105][G train loss: 0.420644]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.278184][G eval loss: 0.339345]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.280184][G train loss: 0.298060]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.333179][G eval loss: 0.313111]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.331183][G train loss: 0.276286]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.354454][G eval loss: 0.305463]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.351848][G train loss: 0.269125]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.364382][G eval loss: 0.291800]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.361718][G train loss: 0.255929]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.369828][G eval loss: 0.279181]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.367180][G train loss: 0.243518]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.372797][G eval loss: 0.272534]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.370493][G train loss: 0.236498]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.374096][G eval loss: 0.273423]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.372152][G train loss: 0.235872]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.374259][G eval loss: 0.274195]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.372738][G train loss: 0.234361]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.373494][G eval loss: 0.269871]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.374195][G train loss: 0.228242]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.372077][G eval loss: 0.260535]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.374891][G train loss: 0.217592]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.369578][G eval loss: 0.246985]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.361107][G train loss: 0.224838]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.336999][G eval loss: 0.275679]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.357734][G train loss: 0.212998]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.294883][G eval loss: 0.312974]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.349201][G train loss: 0.209239]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.285423][G eval loss: 0.330685]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.339945][G train loss: 0.209276]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.282549][G eval loss: 0.329925]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.337845][G train loss: 0.203990]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.282534][G eval loss: 0.319067]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.335530][G train loss: 0.200990]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.297440][G eval loss: 0.286931]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.334702][G train loss: 0.197156]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.311118][G eval loss: 0.265120]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.330575][G train loss: 0.196825]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.312768][G eval loss: 0.261228]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.325478][G train loss: 0.196490]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.307963][G eval loss: 0.263618]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.320113][G train loss: 0.195567]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.302558][G eval loss: 0.265475]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.320053][G train loss: 0.183999]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.296819][G eval loss: 0.268387]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.319047][G train loss: 0.175191]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.312717][G eval loss: 0.223109]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.312479][G train loss: 0.177992]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.306072][G eval loss: 0.228377]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.305841][G train loss: 0.181488]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.299461][G eval loss: 0.234192]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.299256][G train loss: 0.186041]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.293216][G eval loss: 0.239636]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.293052][G train loss: 0.191647]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.287125][G eval loss: 0.246160]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.286846][G train loss: 0.199007]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.281203][G eval loss: 0.254292]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.279922][G train loss: 0.209214]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.275821][G eval loss: 0.263167]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.273196][G train loss: 0.222341]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.271173][G eval loss: 0.273213]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.270946][G train loss: 0.227728]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.266868][G eval loss: 0.283512]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.267477][G train loss: 0.237015]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.262969][G eval loss: 0.294178]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.263701][G train loss: 0.247041]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.259498][G eval loss: 0.304332]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.260296][G train loss: 0.257235]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.256393][G eval loss: 0.314480]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.257332][G train loss: 0.267579]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.253794][G eval loss: 0.324775]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.254859][G train loss: 0.278338]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.251770][G eval loss: 0.335173]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.252924][G train loss: 0.289779]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.250191][G eval loss: 0.346521]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.251463][G train loss: 0.302270]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.249202][G eval loss: 0.358158]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.250534][G train loss: 0.315020]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.248638][G eval loss: 0.369986]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.250031][G train loss: 0.327598]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.248504][G eval loss: 0.381236]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.249872][G train loss: 0.339190]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.248631][G eval loss: 0.392251]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.349718]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.249020][G eval loss: 0.403021]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.250332][G train loss: 0.359424]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.249584][G eval loss: 0.413446]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.250874][G train loss: 0.368663]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.250261][G eval loss: 0.423370]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.251523][G train loss: 0.377227]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.251047][G eval loss: 0.432221]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.252188][G train loss: 0.385339]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.251679][G eval loss: 0.439089]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.252619][G train loss: 0.391280]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.252276][G eval loss: 0.444025]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.254450][G train loss: 0.395637]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.252831][G eval loss: 0.447792]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.254576][G train loss: 0.398927]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.253398][G eval loss: 0.450738]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.254047][G train loss: 0.401993]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.253809][G eval loss: 0.452614]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.254591][G train loss: 0.404321]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.254116][G eval loss: 0.453657]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.254658][G train loss: 0.406598]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.254281][G eval loss: 0.454368]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.254500][G train loss: 0.408377]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.254327][G eval loss: 0.454439]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.254560][G train loss: 0.409201]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.254363][G eval loss: 0.454834]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.254984][G train loss: 0.409072]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.254386][G eval loss: 0.455230]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.254649][G train loss: 0.408319]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.254307][G eval loss: 0.455454]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.254822][G train loss: 0.407495]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.254102][G eval loss: 0.453235]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.254510][G train loss: 0.405969]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.253786][G eval loss: 0.450262]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.254149][G train loss: 0.403212]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.253422][G eval loss: 0.446333]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.253757][G train loss: 0.399522]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.253010][G eval loss: 0.441754]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.253347][G train loss: 0.395328]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.252535][G eval loss: 0.436799]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.252923][G train loss: 0.390669]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.252049][G eval loss: 0.431578]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.252481][G train loss: 0.385711]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.251561][G eval loss: 0.426304]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.252045][G train loss: 0.380426]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.251045][G eval loss: 0.421286]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.251624][G train loss: 0.375078]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.250507][G eval loss: 0.416770]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.251245][G train loss: 0.369810]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.250032][G eval loss: 0.412074]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.250899][G train loss: 0.364380]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.249635][G eval loss: 0.408105]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.250625][G train loss: 0.359509]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.249328][G eval loss: 0.404509]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.250417][G train loss: 0.355192]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.249035][G eval loss: 0.400915]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.250243][G train loss: 0.350953]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.248758][G eval loss: 0.397346]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.250090][G train loss: 0.346849]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.248248][G eval loss: 0.394586]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.249963][G train loss: 0.342870]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.246835][G eval loss: 0.394471]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.249863][G train loss: 0.338888]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.237714][G eval loss: 0.430067]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.249792][G train loss: 0.334729]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.235778][G eval loss: 0.431439]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve_adjusted.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1_adjusted.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1_adjusted.pth\n",
      "Done training ADJUSTED LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n"
     ]
    }
   ],
   "source": [
    "from LOB_GAN_training_adjusted import train_lob_gan\n",
    "\n",
    "stock_list = [\"0050\", \"0056\", \"2330\"]\n",
    "\n",
    "lr_config = {\n",
    "    \"0050\": {\"lr_g\": 0.004, \"lr_d\": 0.0008},\n",
    "    \"0056\": {\"lr_g\": 0.006, \"lr_d\": 0.0005},\n",
    "    \"2330\": {\"lr_g\": 0.0045, \"lr_d\": 0.0012},\n",
    "}\n",
    "\n",
    "\n",
    "batch_config = {\n",
    "    \"0050\": 50,\n",
    "    \"0056\": 50,\n",
    "    \"2330\": 50,\n",
    "}\n",
    "\n",
    "results_adjusted = {}\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"===== Training ADJUSTED GAN for {stock} =====\")\n",
    "    cfg = lr_config[stock]\n",
    "    bs = batch_config[stock]\n",
    "    res = train_lob_gan(\n",
    "        stock=stock,\n",
    "        lr_g=cfg[\"lr_g\"],\n",
    "        lr_d=cfg[\"lr_d\"],\n",
    "        batch_size=bs,\n",
    "        seed=307,\n",
    "    )\n",
    "    results_adjusted[stock] = res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616cab6",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
