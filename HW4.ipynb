{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230eab1a",
   "metadata": {},
   "source": [
    "## HW4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c01e0e",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd34ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training GAN for 0050 =====\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.117348]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.269022]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.107848]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.259430]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.107934]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.259470]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 1.107752]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.259352]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.106480]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.258148]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 1.105823]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249934][G train loss: 1.257526]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 1.105642]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249900][G train loss: 1.257354]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 1.105422]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249856][G train loss: 1.257124]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249829][G eval loss: 1.104922]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249791][G train loss: 1.256597]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249755][G eval loss: 1.103952]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249704][G train loss: 1.255591]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249656][G eval loss: 1.102809]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249587][G train loss: 1.254401]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249521][G eval loss: 1.101852]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249428][G train loss: 1.253398]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249352][G eval loss: 1.101090]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249232][G train loss: 1.252594]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249129][G eval loss: 1.100708]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248976][G train loss: 1.252176]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248830][G eval loss: 1.100744]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248635][G train loss: 1.252164]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248435][G eval loss: 1.101221]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248189][G train loss: 1.252581]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247928][G eval loss: 1.102093]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247619][G train loss: 1.253393]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247369][G eval loss: 1.102612]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246991][G train loss: 1.253839]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246538][G eval loss: 1.101883]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246085][G train loss: 1.253011]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245511][G eval loss: 1.100232]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244968][G train loss: 1.251205]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244334][G eval loss: 1.097981]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243709][G train loss: 1.248718]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242917][G eval loss: 1.095047]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242209][G train loss: 1.245433]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241217][G eval loss: 1.090575]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240423][G train loss: 1.240480]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239243][G eval loss: 1.083274]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238351][G train loss: 1.232568]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236957][G eval loss: 1.072087]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235960][G train loss: 1.220541]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234424][G eval loss: 1.055990]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.233300][G train loss: 1.203394]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231631][G eval loss: 1.034966]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.230363][G train loss: 1.180851]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.228569][G eval loss: 1.008484]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.227142][G train loss: 1.152276]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.225200][G eval loss: 0.974675]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.223656][G train loss: 1.116490]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.221947][G eval loss: 0.932603]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.220142][G train loss: 1.073285]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.219502][G eval loss: 0.883689]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.217126][G train loss: 1.024095]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.218658][G eval loss: 0.832171]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.215622][G train loss: 0.971668]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.221429][G eval loss: 0.781474]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.217015][G train loss: 0.918553]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.228506][G eval loss: 0.735036]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.222167][G train loss: 0.866600]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.238255][G eval loss: 0.696277]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.230798][G train loss: 0.817407]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250014][G eval loss: 0.665469]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.242000][G train loss: 0.774323]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.263483][G eval loss: 0.639508]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.254340][G train loss: 0.738828]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.277342][G eval loss: 0.618302]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.267042][G train loss: 0.709377]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.290981][G eval loss: 0.597570]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.279433][G train loss: 0.682120]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.302760][G eval loss: 0.575034]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.290966][G train loss: 0.655376]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.312883][G eval loss: 0.549432]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.301308][G train loss: 0.628705]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.321247][G eval loss: 0.525353]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.309518][G train loss: 0.603509]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.327861][G eval loss: 0.505094]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.315519][G train loss: 0.582509]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.332504][G eval loss: 0.488767]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.319466][G train loss: 0.565839]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.334148][G eval loss: 0.477095]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.321850][G train loss: 0.553709]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.334225][G eval loss: 0.468145]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.323206][G train loss: 0.544595]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.333293][G eval loss: 0.461388]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.323749][G train loss: 0.537106]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.331393][G eval loss: 0.457375]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.322852][G train loss: 0.532372]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.328569][G eval loss: 0.455950]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.320276][G train loss: 0.530057]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.325559][G eval loss: 0.455366]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.317726][G train loss: 0.527674]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.322156][G eval loss: 0.454873]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.315422][G train loss: 0.524987]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.318218][G eval loss: 0.453617]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.312949][G train loss: 0.521860]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.314182][G eval loss: 0.450924]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.309992][G train loss: 0.517351]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.309999][G eval loss: 0.446332]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.307085][G train loss: 0.510987]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.304943][G eval loss: 0.443888]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.303634][G train loss: 0.506479]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.299970][G eval loss: 0.441739]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299643][G train loss: 0.501919]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.295039][G eval loss: 0.441201]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.295083][G train loss: 0.499082]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.290655][G eval loss: 0.443324]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.290636][G train loss: 0.499080]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.286974][G eval loss: 0.443465]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.286837][G train loss: 0.498879]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.283545][G eval loss: 0.444566]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.283169][G train loss: 0.500298]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.279999][G eval loss: 0.445908]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.279103][G train loss: 0.502009]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.276858][G eval loss: 0.445695]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.275081][G train loss: 0.502841]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.274115][G eval loss: 0.446005]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.271605][G train loss: 0.504608]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.271596][G eval loss: 0.446998]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.269004][G train loss: 0.506768]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.269076][G eval loss: 0.448190]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.266708][G train loss: 0.508424]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.266521][G eval loss: 0.449291]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.264536][G train loss: 0.509336]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.263590][G eval loss: 0.449091]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.262477][G train loss: 0.509300]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.260577][G eval loss: 0.447475]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.260595][G train loss: 0.508215]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.257835][G eval loss: 0.446247]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.258982][G train loss: 0.507135]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.256090][G eval loss: 0.446100]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.257574][G train loss: 0.507109]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.254893][G eval loss: 0.445716]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.256320][G train loss: 0.507197]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.253912][G eval loss: 0.445059]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.255211][G train loss: 0.506519]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.253114][G eval loss: 0.445002]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.254254][G train loss: 0.505375]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.252473][G eval loss: 0.446369]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.253438][G train loss: 0.504807]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.251914][G eval loss: 0.448215]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "===== Training GAN for 0056 =====\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250011][G eval loss: 0.981335]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.083836]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.974680]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.077592]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.973629]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.076936]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.973401]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.076853]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.973827]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.077259]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249951][G eval loss: 0.974162]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249930][G train loss: 1.077495]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.974089]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249891][G train loss: 1.077295]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249876][G eval loss: 0.973317]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249838][G train loss: 1.076400]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249821][G eval loss: 0.971812]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249769][G train loss: 1.074781]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249749][G eval loss: 0.969886]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249678][G train loss: 1.072767]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249650][G eval loss: 0.968404]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249554][G train loss: 1.071227]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249520][G eval loss: 0.967727]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249391][G train loss: 1.070521]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249341][G eval loss: 0.967623]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249173][G train loss: 1.070414]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249100][G eval loss: 0.967840]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248882][G train loss: 1.070635]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248771][G eval loss: 0.968480]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248494][G train loss: 1.071279]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248350][G eval loss: 0.969580]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248002][G train loss: 1.072376]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247821][G eval loss: 0.970787]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247388][G train loss: 1.073588]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247291][G eval loss: 0.971324]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246753][G train loss: 1.074144]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246467][G eval loss: 0.970965]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245831][G train loss: 1.073755]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245491][G eval loss: 0.969879]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244796][G train loss: 1.072496]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244453][G eval loss: 0.968085]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243761][G train loss: 1.070362]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243303][G eval loss: 0.965070]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242704][G train loss: 1.066733]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241967][G eval loss: 0.959119]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.241571][G train loss: 1.059775]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.240822][G eval loss: 0.947416]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.240849][G train loss: 1.046257]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.240461][G eval loss: 0.928494]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.241323][G train loss: 1.024277]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.240884][G eval loss: 0.907935]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.242056][G train loss: 1.001496]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.241886][G eval loss: 0.895650]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.242656][G train loss: 0.987957]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.242951][G eval loss: 0.896262]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.243486][G train loss: 0.986526]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.243028][G eval loss: 0.895993]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.243983][G train loss: 0.983061]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.241983][G eval loss: 0.885391]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243554][G train loss: 0.970031]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.237249][G eval loss: 0.869512]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.242466][G train loss: 0.947002]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.234017][G eval loss: 0.847412]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.240442][G train loss: 0.918085]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.230934][G eval loss: 0.824926]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.237619][G train loss: 0.892694]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.227891][G eval loss: 0.804856]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.235136][G train loss: 0.872163]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.225943][G eval loss: 0.785346]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.233920][G train loss: 0.850998]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.225819][G eval loss: 0.757344]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.234647][G train loss: 0.822189]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.231236][G eval loss: 0.720661]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.240531][G train loss: 0.784703]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.241436][G eval loss: 0.682875]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.245879][G train loss: 0.748613]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.247290][G eval loss: 0.658830]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.251209][G train loss: 0.719510]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.253205][G eval loss: 0.639946]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.255183][G train loss: 0.694146]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.256553][G eval loss: 0.613839]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.255022][G train loss: 0.674351]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.256214][G eval loss: 0.582160]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.253127][G train loss: 0.648754]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.254292][G eval loss: 0.548194]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.250091][G train loss: 0.618842]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.251636][G eval loss: 0.514499]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.246924][G train loss: 0.590070]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.248717][G eval loss: 0.491059]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.244201][G train loss: 0.571573]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.245483][G eval loss: 0.492778]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.240219][G train loss: 0.575253]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.247564][G eval loss: 0.503656]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.238654][G train loss: 0.591770]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.253504][G eval loss: 0.517778]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.241649][G train loss: 0.599908]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.264104][G eval loss: 0.510616]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.248680][G train loss: 0.593037]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.276490][G eval loss: 0.494613]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.256894][G train loss: 0.568472]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.277825][G eval loss: 0.491822]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.261136][G train loss: 0.555837]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.278225][G eval loss: 0.495803]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.260367][G train loss: 0.561498]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.277945][G eval loss: 0.505853]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.258958][G train loss: 0.573068]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.277295][G eval loss: 0.508025]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.257489][G train loss: 0.577251]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.276419][G eval loss: 0.499627]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.256142][G train loss: 0.569748]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.275350][G eval loss: 0.482878]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.254980][G train loss: 0.553111]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.274087][G eval loss: 0.462597]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.254029][G train loss: 0.533451]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.272072][G eval loss: 0.444828]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.253017][G train loss: 0.517437]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.269121][G eval loss: 0.433392]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.252168][G train loss: 0.508105]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.264708][G eval loss: 0.429821]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.251430][G train loss: 0.506687]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.259648][G eval loss: 0.435323]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.250573][G train loss: 0.513875]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.256538][G eval loss: 0.445938]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.250023][G train loss: 0.525816]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.255387][G eval loss: 0.458445]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.249630][G train loss: 0.539301]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.254485][G eval loss: 0.472462]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.249296][G train loss: 0.553663]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.253846][G eval loss: 0.486910]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.249258][G train loss: 0.568226]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.253528][G eval loss: 0.499735]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.249667][G train loss: 0.580869]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.253666][G eval loss: 0.508394]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.250445][G train loss: 0.588744]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.254001][G eval loss: 0.510903]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.251337][G train loss: 0.589885]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.254021][G eval loss: 0.510626]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.251869][G train loss: 0.587448]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.253656][G eval loss: 0.509939]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.251991][G train loss: 0.583959]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.252990][G eval loss: 0.508376]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.251789][G train loss: 0.580749]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.252326][G eval loss: 0.504683]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.251496][G train loss: 0.576641]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.251682][G eval loss: 0.499753]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.251071][G train loss: 0.570784]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.250958][G eval loss: 0.493880]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.250508][G train loss: 0.563523]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.250544][G eval loss: 0.486110]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.250285][G train loss: 0.555361]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.250550][G eval loss: 0.478365]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.250396][G train loss: 0.547230]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.250670][G eval loss: 0.471407]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.250560][G train loss: 0.540920]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.250741][G eval loss: 0.465520]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.250648][G train loss: 0.535783]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.250735][G eval loss: 0.460729]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250639][G train loss: 0.531036]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.250601][G eval loss: 0.456553]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250531][G train loss: 0.527613]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.250397][G eval loss: 0.453513]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.250315][G train loss: 0.524414]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.250014][G eval loss: 0.451333]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.249940][G train loss: 0.520915]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.249457][G eval loss: 0.449753]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.249385][G train loss: 0.517853]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.248781][G eval loss: 0.448543]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.248625][G train loss: 0.515811]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.248119][G eval loss: 0.449316]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.247728][G train loss: 0.515420]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.247679][G eval loss: 0.449656]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n",
      "===== Training GAN for 2330 =====\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 0.486292]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250027][G train loss: 0.450600]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.479923]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 0.444362]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.481482]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.446120]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 0.482475]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249953][G train loss: 0.447205]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249937][G eval loss: 0.481927]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249912][G train loss: 0.446598]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249876][G eval loss: 0.481018]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249835][G train loss: 0.445568]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249785][G eval loss: 0.480284]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249723][G train loss: 0.444704]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249646][G eval loss: 0.480220]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249550][G train loss: 0.444537]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249458][G eval loss: 0.479573]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249315][G train loss: 0.443844]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249193][G eval loss: 0.478328]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248978][G train loss: 0.442610]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248807][G eval loss: 0.477089]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248471][G train loss: 0.441428]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248228][G eval loss: 0.476347]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247733][G train loss: 0.440754]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247507][G eval loss: 0.476152]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246800][G train loss: 0.440618]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246524][G eval loss: 0.475808]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245552][G train loss: 0.440313]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.245153][G eval loss: 0.476429]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243831][G train loss: 0.440921]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.243316][G eval loss: 0.478305]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241534][G train loss: 0.442749]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240931][G eval loss: 0.480987]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238556][G train loss: 0.445365]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.238057][G eval loss: 0.483880]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.234955][G train loss: 0.448192]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.234699][G eval loss: 0.486707]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230704][G train loss: 0.450998]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.230272][G eval loss: 0.490388]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.225316][G train loss: 0.454681]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.224907][G eval loss: 0.494166]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.218991][G train loss: 0.458498]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.218360][G eval loss: 0.498574]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.211545][G train loss: 0.462974]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.210325][G eval loss: 0.503805]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.202705][G train loss: 0.468228]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.200473][G eval loss: 0.510305]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.192220][G train loss: 0.474723]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.189452][G eval loss: 0.517455]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.180458][G train loss: 0.481873]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.177219][G eval loss: 0.525273]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.167460][G train loss: 0.489663]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.163555][G eval loss: 0.534419]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.154040][G train loss: 0.498758]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.148863][G eval loss: 0.545109]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.141038][G train loss: 0.509380]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.135718][G eval loss: 0.554413]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.130157][G train loss: 0.518363]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.130735][G eval loss: 0.549266]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.127296][G train loss: 0.511469]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.224962][G eval loss: 0.385034]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.233680][G train loss: 0.332835]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.323702][G eval loss: 0.307164]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.323383][G train loss: 0.270017]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.353376][G eval loss: 0.303166]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.351374][G train loss: 0.266807]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.365669][G eval loss: 0.294487]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.363509][G train loss: 0.258097]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.372865][G eval loss: 0.283133]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.370912][G train loss: 0.246987]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.377203][G eval loss: 0.273598]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.375619][G train loss: 0.237019]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.379643][G eval loss: 0.266727]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.378488][G train loss: 0.228800]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.380693][G eval loss: 0.262613]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.381451][G train loss: 0.222563]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.380809][G eval loss: 0.259467]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.383567][G train loss: 0.217323]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.379986][G eval loss: 0.253299]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.381912][G train loss: 0.211141]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.378717][G eval loss: 0.252664]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.382479][G train loss: 0.207974]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.376705][G eval loss: 0.249009]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.380605][G train loss: 0.202747]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.374023][G eval loss: 0.242573]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.377737][G train loss: 0.195526]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.370691][G eval loss: 0.234509]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.373895][G train loss: 0.187445]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.366952][G eval loss: 0.226386]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.368625][G train loss: 0.179756]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.362875][G eval loss: 0.218714]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.362781][G train loss: 0.173599]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.358369][G eval loss: 0.212533]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.358320][G train loss: 0.169914]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.353433][G eval loss: 0.208596]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.353138][G train loss: 0.168225]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.347832][G eval loss: 0.207256]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.346710][G train loss: 0.169180]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.341810][G eval loss: 0.208164]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.339302][G train loss: 0.172413]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.335417][G eval loss: 0.210681]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.331833][G train loss: 0.177041]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.328797][G eval loss: 0.214336]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.324647][G train loss: 0.180788]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.321925][G eval loss: 0.217915]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.317882][G train loss: 0.183787]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.314893][G eval loss: 0.221908]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.311574][G train loss: 0.186409]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.307778][G eval loss: 0.226386]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.305399][G train loss: 0.189210]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.300681][G eval loss: 0.232166]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299433][G train loss: 0.193134]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.293688][G eval loss: 0.239575]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.293331][G train loss: 0.198927]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.287007][G eval loss: 0.249333]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.286893][G train loss: 0.207356]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.280681][G eval loss: 0.261383]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Anomaly-Detection\\Results\n"
     ]
    }
   ],
   "source": [
    "from LOB_GAN_training_raw import train_lob_gan\n",
    "\n",
    "stock_list = [\"0050\", \"0056\", \"2330\"]\n",
    "\n",
    "lr_config = {\n",
    "    \"0050\": {\"lr_g\": 0.004, \"lr_d\": 0.0008},\n",
    "    \"0056\": {\"lr_g\": 0.005, \"lr_d\": 0.0008},\n",
    "    \"2330\": {\"lr_g\": 0.0045, \"lr_d\": 0.0012},\n",
    "}\n",
    "\n",
    "batch_config = {\n",
    "    \"0050\": 50,\n",
    "    \"0056\": 50,\n",
    "    \"2330\": 50,   \n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"===== Training GAN for {stock} =====\")\n",
    "    cfg = lr_config[stock]\n",
    "    bs = batch_config[stock]\n",
    "    res = train_lob_gan(\n",
    "        stock=stock,\n",
    "        lr_g=cfg[\"lr_g\"],\n",
    "        lr_d=cfg[\"lr_d\"],\n",
    "        batch_size=bs,\n",
    "        seed=307,\n",
    "    )\n",
    "    results[stock] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96d028d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Grid search for 0050 ==========\n",
      "\n",
      "----- 0050: lr_g=0.003, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.123487]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.275158]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.112164]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.263789]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.107800]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249986][G train loss: 1.259358]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.107516]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249978][G train loss: 1.259065]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 1.106864]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.258459]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 1.105586]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249957][G train loss: 1.257229]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 1.104653]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249941][G train loss: 1.256329]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249938][G eval loss: 1.104292]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249923][G train loss: 1.255986]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249921][G eval loss: 1.104256]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249901][G train loss: 1.255957]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249899][G eval loss: 1.104313]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249875][G train loss: 1.256009]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249871][G eval loss: 1.104345]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249841][G train loss: 1.256025]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249835][G eval loss: 1.104306]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249798][G train loss: 1.255965]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249790][G eval loss: 1.104168]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249745][G train loss: 1.255803]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249734][G eval loss: 1.103944]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249680][G train loss: 1.255552]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249667][G eval loss: 1.103657]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249601][G train loss: 1.255237]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249586][G eval loss: 1.103372]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249507][G train loss: 1.254924]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249503][G eval loss: 1.103151]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249409][G train loss: 1.254677]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249382][G eval loss: 1.102994]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249271][G train loss: 1.254498]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249248][G eval loss: 1.102959]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249117][G train loss: 1.254456]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249088][G eval loss: 1.103035]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248933][G train loss: 1.254507]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248895][G eval loss: 1.103153]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248712][G train loss: 1.254588]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248660][G eval loss: 1.103285]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248447][G train loss: 1.254671]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248355][G eval loss: 1.103389]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248110][G train loss: 1.254715]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.247992][G eval loss: 1.103363]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247711][G train loss: 1.254596]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247583][G eval loss: 1.103060]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247261][G train loss: 1.254197]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247197][G eval loss: 1.102325]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246825][G train loss: 1.253365]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246710][G eval loss: 1.100869]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246290][G train loss: 1.251775]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246125][G eval loss: 1.098691]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245658][G train loss: 1.249417]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245451][G eval loss: 1.095582]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244947][G train loss: 1.246076]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.244738][G eval loss: 1.091513]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.244199][G train loss: 1.241715]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.243929][G eval loss: 1.086309]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.243349][G train loss: 1.236127]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.243011][G eval loss: 1.079476]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.242398][G train loss: 1.228796]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.241986][G eval loss: 1.070564]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.241356][G train loss: 1.219185]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.240891][G eval loss: 1.058827]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.240256][G train loss: 1.206439]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.239761][G eval loss: 1.043327]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.239155][G train loss: 1.189496]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.238680][G eval loss: 1.023747]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.238131][G train loss: 1.168138]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.237716][G eval loss: 1.000146]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.237248][G train loss: 1.142624]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.236943][G eval loss: 0.972580]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.236553][G train loss: 1.113071]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.236357][G eval loss: 0.941497]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.236134][G train loss: 1.079723]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.235934][G eval loss: 0.907527]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.235955][G train loss: 1.043502]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.235496][G eval loss: 0.871599]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.235745][G train loss: 1.006050]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.234746][G eval loss: 0.835406]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.235245][G train loss: 0.969186]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.233395][G eval loss: 0.801566]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.234383][G train loss: 0.934695]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.231657][G eval loss: 0.773055]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.233352][G train loss: 0.903572]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.230378][G eval loss: 0.749732]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.232606][G train loss: 0.874820]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.230259][G eval loss: 0.729803]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.232525][G train loss: 0.847717]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.231898][G eval loss: 0.713370]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.233375][G train loss: 0.823710]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.235493][G eval loss: 0.700297]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.235511][G train loss: 0.803719]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.239823][G eval loss: 0.691970]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.238881][G train loss: 0.787473]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.242551][G eval loss: 0.689178]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.242880][G train loss: 0.773210]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.245373][G eval loss: 0.684602]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.247547][G train loss: 0.757295]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.250668][G eval loss: 0.673618]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.252603][G train loss: 0.739059]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.259248][G eval loss: 0.653520]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.257738][G train loss: 0.719128]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.268383][G eval loss: 0.631638]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.263301][G train loss: 0.696634]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.276827][G eval loss: 0.604463]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.268705][G train loss: 0.671740]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.283806][G eval loss: 0.574729]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.274262][G train loss: 0.644748]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.288933][G eval loss: 0.550070]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.279682][G train loss: 0.619083]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.292893][G eval loss: 0.528984]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.283815][G train loss: 0.599109]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.295268][G eval loss: 0.512971]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.286485][G train loss: 0.584122]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.296427][G eval loss: 0.501590]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.288410][G train loss: 0.574494]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.297391][G eval loss: 0.494806]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.291855][G train loss: 0.566246]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.297952][G eval loss: 0.491540]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.293127][G train loss: 0.564315]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.297622][G eval loss: 0.489903]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.293094][G train loss: 0.563252]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.296350][G eval loss: 0.487401]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.292432][G train loss: 0.559913]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.294200][G eval loss: 0.483768]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.291478][G train loss: 0.554506]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.291667][G eval loss: 0.480671]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.290426][G train loss: 0.549380]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.289084][G eval loss: 0.482180]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.289199][G train loss: 0.548276]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.286359][G eval loss: 0.490931]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.287747][G train loss: 0.554058]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.283694][G eval loss: 0.503337]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.286194][G train loss: 0.563831]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.281235][G eval loss: 0.513722]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.284632][G train loss: 0.573470]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.279025][G eval loss: 0.518852]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.283081][G train loss: 0.578854]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.277059][G eval loss: 0.517400]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.281540][G train loss: 0.578092]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.275392][G eval loss: 0.509879]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.280107][G train loss: 0.571750]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.274000][G eval loss: 0.498764]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.278741][G train loss: 0.562299]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.272631][G eval loss: 0.489175]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.277219][G train loss: 0.554216]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.271200][G eval loss: 0.485762]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.275537][G train loss: 0.550697]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.269742][G eval loss: 0.488210]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.273789][G train loss: 0.549860]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.268082][G eval loss: 0.490590]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.003 lr_d=0.0005 -> score=0.758671\n",
      "\n",
      "----- 0050: lr_g=0.003, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250008][G eval loss: 1.121560]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.273231]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.111881]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.263507]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 1.108826]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.260384]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 1.109099]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.260648]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249965][G eval loss: 1.108207]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249954][G train loss: 1.259802]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 1.106356]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249925][G train loss: 1.257999]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249908][G eval loss: 1.104776]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249887][G train loss: 1.256452]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 1.103824]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249842][G train loss: 1.255519]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249821][G eval loss: 1.103498]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249779][G train loss: 1.255199]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249749][G eval loss: 1.103458]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249693][G train loss: 1.255155]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249647][G eval loss: 1.103632]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249571][G train loss: 1.255316]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249503][G eval loss: 1.103836]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249401][G train loss: 1.255498]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249324][G eval loss: 1.103971]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249192][G train loss: 1.255610]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249088][G eval loss: 1.104073]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248920][G train loss: 1.255685]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248775][G eval loss: 1.104213]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248565][G train loss: 1.255798]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248370][G eval loss: 1.104503]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248105][G train loss: 1.256061]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247862][G eval loss: 1.105032]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247529][G train loss: 1.256565]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247316][G eval loss: 1.105454]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246907][G train loss: 1.256966]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246483][G eval loss: 1.105503]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245998][G train loss: 1.257008]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245405][G eval loss: 1.105588]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244843][G train loss: 1.257072]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244151][G eval loss: 1.105929]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243512][G train loss: 1.257383]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242618][G eval loss: 1.106754]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241897][G train loss: 1.258167]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.240782][G eval loss: 1.107915]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239971][G train loss: 1.259272]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238642][G eval loss: 1.109034]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237739][G train loss: 1.260322]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236075][G eval loss: 1.110286]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235081][G train loss: 1.261484]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.233197][G eval loss: 1.111201]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.232088][G train loss: 1.262274]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.229909][G eval loss: 1.111543]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.228696][G train loss: 1.262451]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.226029][G eval loss: 1.111518]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.224755][G train loss: 1.262203]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.221568][G eval loss: 1.111001]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.220282][G train loss: 1.261423]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.216526][G eval loss: 1.109679]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.215261][G train loss: 1.259730]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.210777][G eval loss: 1.107436]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.209592][G train loss: 1.256932]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.204448][G eval loss: 1.103674]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.203407][G train loss: 1.252347]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.197521][G eval loss: 1.097834]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.196747][G train loss: 1.245188]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.190269][G eval loss: 1.088627]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.190274][G train loss: 1.233260]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.183255][G eval loss: 1.074536]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.186481][G train loss: 1.211322]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.180382][G eval loss: 1.048041]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.188151][G train loss: 1.176578]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.191635][G eval loss: 0.994958]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.193796][G train loss: 1.134955]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.212165][G eval loss: 0.940171]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.206880][G train loss: 1.085595]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.224341][G eval loss: 0.917268]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.226784][G train loss: 1.039582]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.232042][G eval loss: 0.920038]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.249602][G train loss: 1.009072]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.237626][G eval loss: 0.921301]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.003 lr_d=0.0008 -> score=1.158927\n",
      "\n",
      "----- 0050: lr_g=0.003, lr_d=0.001 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250017][G eval loss: 1.120383]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250016][G train loss: 1.272054]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.111705]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 1.263331]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 1.109535]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.261093]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 1.110154]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249963][G train loss: 1.261703]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249947][G eval loss: 1.109139]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249933][G train loss: 1.260733]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249903][G eval loss: 1.106977]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249881][G train loss: 1.258621]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249849][G eval loss: 1.105032]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249817][G train loss: 1.256708]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249777][G eval loss: 1.103928]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249729][G train loss: 1.255624]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249677][G eval loss: 1.103787]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249607][G train loss: 1.255489]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249524][G eval loss: 1.103927]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249426][G train loss: 1.255625]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249308][G eval loss: 1.104135]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249172][G train loss: 1.255820]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248998][G eval loss: 1.104395]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248815][G train loss: 1.256060]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248575][G eval loss: 1.104685]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248332][G train loss: 1.256328]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247993][G eval loss: 1.104907]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247674][G train loss: 1.256524]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247201][G eval loss: 1.105291]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246799][G train loss: 1.256881]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246313][G eval loss: 1.105740]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245814][G train loss: 1.257303]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.244990][G eval loss: 1.106019]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244392][G train loss: 1.257556]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243248][G eval loss: 1.106641]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242546][G train loss: 1.258161]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241050][G eval loss: 1.107771]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.240241][G train loss: 1.259285]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238346][G eval loss: 1.109345]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.237416][G train loss: 1.260836]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235006][G eval loss: 1.111428]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.233937][G train loss: 1.262890]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.231113][G eval loss: 1.113521]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.229903][G train loss: 1.264943]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.226434][G eval loss: 1.115968]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.225147][G train loss: 1.267330]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.220856][G eval loss: 1.118943]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.219536][G train loss: 1.270227]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.214403][G eval loss: 1.122118]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.213027][G train loss: 1.273274]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.206931][G eval loss: 1.125721]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.205552][G train loss: 1.276752]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.198432][G eval loss: 1.129549]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.197080][G train loss: 1.280367]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.188840][G eval loss: 1.133774]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.187542][G train loss: 1.284338]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.178479][G eval loss: 1.137542]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.177257][G train loss: 1.287777]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.167157][G eval loss: 1.141446]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.166149][G train loss: 1.291072]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.155043][G eval loss: 1.145600]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.154559][G train loss: 1.294114]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.142811][G eval loss: 1.149359]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.143269][G train loss: 1.295731]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.131982][G eval loss: 1.149879]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.136859][G train loss: 1.286293]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.133746][G eval loss: 1.123502]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.153557][G train loss: 1.239742]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.188128][G eval loss: 1.024329]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.185015][G train loss: 1.179018]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.224377][G eval loss: 0.981386]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.218260][G train loss: 1.123210]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.234869][G eval loss: 0.971366]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.256971][G train loss: 1.063377]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.241340][G eval loss: 0.980904]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.294868][G train loss: 1.030917]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.267883][G eval loss: 0.981917]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.003 lr_d=0.001 -> score=1.249800\n",
      "\n",
      "----- 0050: lr_g=0.003, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250029][G eval loss: 1.119203]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250028][G train loss: 1.270875]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 1.111506]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.263131]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.110209]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.261767]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249965][G eval loss: 1.111184]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249954][G train loss: 1.262733]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249926][G eval loss: 1.110107]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249906][G train loss: 1.261702]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249859][G eval loss: 1.107660]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249828][G train loss: 1.259303]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249775][G eval loss: 1.105398]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249729][G train loss: 1.257075]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249654][G eval loss: 1.104671]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249582][G train loss: 1.256368]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249473][G eval loss: 1.104481]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249360][G train loss: 1.256184]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249178][G eval loss: 1.104526]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249013][G train loss: 1.256226]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248724][G eval loss: 1.104594]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248508][G train loss: 1.256282]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248117][G eval loss: 1.104765]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247838][G train loss: 1.256433]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247247][G eval loss: 1.105366]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246885][G train loss: 1.257014]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246185][G eval loss: 1.106272]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245702][G train loss: 1.257893]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244660][G eval loss: 1.107039]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.244033][G train loss: 1.258635]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242581][G eval loss: 1.108116]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241800][G train loss: 1.259687]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.239872][G eval loss: 1.109572]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238918][G train loss: 1.261115]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236434][G eval loss: 1.111549]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.235263][G train loss: 1.263075]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232221][G eval loss: 1.113726]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230853][G train loss: 1.265244]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.226786][G eval loss: 1.116845]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.225335][G train loss: 1.268335]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.220234][G eval loss: 1.120472]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.218800][G train loss: 1.271932]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.212337][G eval loss: 1.124632]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.210971][G train loss: 1.276049]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.203110][G eval loss: 1.129591]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.201841][G train loss: 1.280941]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.192427][G eval loss: 1.135863]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.191204][G train loss: 1.287107]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.180547][G eval loss: 1.142818]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.179339][G train loss: 1.293940]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.167422][G eval loss: 1.150788]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.166320][G train loss: 1.301705]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.153166][G eval loss: 1.160639]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.152413][G train loss: 1.311184]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.138215][G eval loss: 1.173398]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.137907][G train loss: 1.323394]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.123742][G eval loss: 1.188127]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.123953][G train loss: 1.337316]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.110465][G eval loss: 1.203865]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.111329][G train loss: 1.351580]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.099417][G eval loss: 1.217610]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.101879][G train loss: 1.360894]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.105966][G eval loss: 1.195589]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.128732][G train loss: 1.309027]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.190493][G eval loss: 1.078761]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.187266][G train loss: 1.227345]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.235256][G eval loss: 1.034400]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.237737][G train loss: 1.166604]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.244254][G eval loss: 1.026672]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.279716][G train loss: 1.108407]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250474][G eval loss: 1.017013]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.330388][G train loss: 1.047991]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.292182][G eval loss: 0.961384]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.356513][G train loss: 1.025900]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.338320][G eval loss: 0.954078]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.398886][G train loss: 1.005661]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.349469][G eval loss: 0.965850]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.003 lr_d=0.0012 -> score=1.315319\n",
      "\n",
      "----- 0050: lr_g=0.003, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250053][G eval loss: 1.117430]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250052][G train loss: 1.269101]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 1.111068]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.262693]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 1.111074]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.262632]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249985][G eval loss: 1.112573]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.264122]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 1.111375]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249939][G train loss: 1.262969]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 1.108981]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249841][G train loss: 1.260625]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249766][G eval loss: 1.107021]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249715][G train loss: 1.258698]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249612][G eval loss: 1.105697]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249532][G train loss: 1.257394]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249358][G eval loss: 1.104989]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249237][G train loss: 1.256693]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248920][G eval loss: 1.104875]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248727][G train loss: 1.256577]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248178][G eval loss: 1.105142]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.247836][G train loss: 1.256832]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.246989][G eval loss: 1.105589]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246470][G train loss: 1.257261]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.245204][G eval loss: 1.106211]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.244524][G train loss: 1.257865]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.242891][G eval loss: 1.106998]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.241981][G train loss: 1.258631]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.239867][G eval loss: 1.108729]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.238644][G train loss: 1.260337]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.235789][G eval loss: 1.111681]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.234192][G train loss: 1.263260]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.230422][G eval loss: 1.116258]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.228460][G train loss: 1.267809]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.223747][G eval loss: 1.122123]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.221490][G train loss: 1.273662]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.215399][G eval loss: 1.128604]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.213115][G train loss: 1.280130]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.205076][G eval loss: 1.135746]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.203094][G train loss: 1.287251]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.192479][G eval loss: 1.143759]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.190933][G train loss: 1.295223]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.177909][G eval loss: 1.153591]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.176594][G train loss: 1.304997]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.161532][G eval loss: 1.165940]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.160410][G train loss: 1.317256]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.143411][G eval loss: 1.183299]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.142657][G train loss: 1.334447]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.125617][G eval loss: 1.204105]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.125302][G train loss: 1.355036]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.109158][G eval loss: 1.227408]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.109210][G train loss: 1.378049]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.094477][G eval loss: 1.252287]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.094810][G train loss: 1.402578]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.081765][G eval loss: 1.277684]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.082355][G train loss: 1.427368]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.071169][G eval loss: 1.301863]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.072198][G train loss: 1.449995]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.074221][G eval loss: 1.295721]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.090352][G train loss: 1.419289]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.174047][G eval loss: 1.157413]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.169609][G train loss: 1.306356]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.215519][G eval loss: 1.124668]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.237107][G train loss: 1.230373]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.247973][G eval loss: 1.088665]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.282329][G train loss: 1.178051]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.252374][G eval loss: 1.084349]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.342971][G train loss: 1.094217]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.300899][G eval loss: 0.989075]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.372431][G train loss: 1.061859]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.352302][G eval loss: 0.947296]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.408053][G train loss: 1.014902]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.358665][G eval loss: 0.942164]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.432689][G train loss: 1.000365]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.398486][G eval loss: 0.892470]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.434918][G train loss: 1.011544]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.429676][G eval loss: 0.882761]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.435925][G train loss: 1.019604]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.475377][G eval loss: 0.885546]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.436083][G train loss: 1.021144]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.478595][G eval loss: 0.879866]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.435714][G train loss: 1.015237]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.478064][G eval loss: 0.867377]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.435068][G train loss: 1.003388]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.476581][G eval loss: 0.850748]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.438590][G train loss: 0.988044]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.465531][G eval loss: 0.835923]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.432535][G train loss: 0.971365]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.435529][G eval loss: 0.869475]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.429866][G train loss: 0.954960]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.410002][G eval loss: 0.885284]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.003 lr_d=0.0015 -> score=1.295287\n",
      "\n",
      "----- 0050: lr_g=0.0035, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.121292]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.272964]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.109790]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.261394]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 1.107085]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249986][G train loss: 1.258625]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.106951]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.258524]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 1.105860]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.257499]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 1.105073]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249957][G train loss: 1.256760]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249954][G eval loss: 1.104975]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249943][G train loss: 1.256684]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 1.105103]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249925][G train loss: 1.256817]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 1.105103]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249903][G train loss: 1.256807]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249899][G eval loss: 1.104873]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249876][G train loss: 1.256557]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249871][G eval loss: 1.104440]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249842][G train loss: 1.256095]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249834][G eval loss: 1.103854]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249799][G train loss: 1.255477]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249789][G eval loss: 1.103167]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249746][G train loss: 1.254755]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249733][G eval loss: 1.102557]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249681][G train loss: 1.254112]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249666][G eval loss: 1.102155]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249604][G train loss: 1.253681]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249587][G eval loss: 1.102015]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249512][G train loss: 1.253525]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249493][G eval loss: 1.101978]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249404][G train loss: 1.253478]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249389][G eval loss: 1.102137]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249284][G train loss: 1.253616]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249251][G eval loss: 1.102387]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249126][G train loss: 1.253843]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249077][G eval loss: 1.102625]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248930][G train loss: 1.254037]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248873][G eval loss: 1.102791]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248702][G train loss: 1.254142]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248630][G eval loss: 1.102851]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248434][G train loss: 1.254124]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248317][G eval loss: 1.102668]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248096][G train loss: 1.253844]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.247946][G eval loss: 1.102155]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247699][G train loss: 1.253222]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247533][G eval loss: 1.101066]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247252][G train loss: 1.252022]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247092][G eval loss: 1.098956]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246779][G train loss: 1.249775]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246591][G eval loss: 1.095800]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246251][G train loss: 1.246439]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246037][G eval loss: 1.091522]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245668][G train loss: 1.241906]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245414][G eval loss: 1.085315]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.245033][G train loss: 1.235229]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.244733][G eval loss: 1.076261]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.244358][G train loss: 1.225360]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.243999][G eval loss: 1.063584]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.243671][G train loss: 1.211437]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.243286][G eval loss: 1.046382]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.243051][G train loss: 1.192726]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.242741][G eval loss: 1.024038]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.242599][G train loss: 1.169164]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.242550][G eval loss: 0.997852]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.242460][G train loss: 1.142332]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.242803][G eval loss: 0.972183]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.242696][G train loss: 1.115879]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.243221][G eval loss: 0.950455]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.243000][G train loss: 1.092670]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.243213][G eval loss: 0.927871]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.242860][G train loss: 1.069137]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.242404][G eval loss: 0.899312]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.242188][G train loss: 1.040995]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.240617][G eval loss: 0.867143]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.241108][G train loss: 1.010068]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.238844][G eval loss: 0.837833]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.239779][G train loss: 0.982184]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.237749][G eval loss: 0.812313]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.238681][G train loss: 0.956712]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.237244][G eval loss: 0.785865]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.238307][G train loss: 0.927302]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.237471][G eval loss: 0.756629]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.238948][G train loss: 0.892312]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.238502][G eval loss: 0.728626]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.240874][G train loss: 0.854698]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.240707][G eval loss: 0.707286]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.244063][G train loss: 0.821036]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.245635][G eval loss: 0.692535]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.247787][G train loss: 0.796540]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.250383][G eval loss: 0.684248]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.251580][G train loss: 0.778246]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.254594][G eval loss: 0.676555]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.255306][G train loss: 0.761263]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.259014][G eval loss: 0.669465]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.258263][G train loss: 0.744837]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.261719][G eval loss: 0.663483]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.260629][G train loss: 0.730210]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.263673][G eval loss: 0.659927]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.262628][G train loss: 0.717493]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.265947][G eval loss: 0.652608]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.264615][G train loss: 0.701999]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.268280][G eval loss: 0.638407]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.266672][G train loss: 0.683525]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.271721][G eval loss: 0.619708]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.268214][G train loss: 0.664229]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.272804][G eval loss: 0.600497]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.269417][G train loss: 0.646151]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.272967][G eval loss: 0.582424]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.270185][G train loss: 0.631797]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.272298][G eval loss: 0.568725]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.270008][G train loss: 0.624164]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.270933][G eval loss: 0.562358]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.269214][G train loss: 0.622736]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.269008][G eval loss: 0.561782]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.268023][G train loss: 0.625132]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.266879][G eval loss: 0.563987]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.266711][G train loss: 0.628965]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.264914][G eval loss: 0.567710]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.265514][G train loss: 0.632741]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.263191][G eval loss: 0.571175]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.264337][G train loss: 0.636488]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.261641][G eval loss: 0.572805]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.263273][G train loss: 0.639530]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.260368][G eval loss: 0.572405]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.262345][G train loss: 0.640623]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.259302][G eval loss: 0.569966]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.261565][G train loss: 0.638395]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.258250][G eval loss: 0.565710]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.260785][G train loss: 0.632852]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.257247][G eval loss: 0.561348]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.259954][G train loss: 0.625461]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.256322][G eval loss: 0.559202]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.259071][G train loss: 0.618562]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.255532][G eval loss: 0.559816]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.258127][G train loss: 0.613824]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.254845][G eval loss: 0.562160]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.0035 lr_d=0.0005 -> score=0.817005\n",
      "\n",
      "----- 0050: lr_g=0.0035, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.119364]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.271036]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.109508]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.261112]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 1.108114]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.259653]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 1.108538]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.260111]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 1.107205]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.258843]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249944][G eval loss: 1.105836]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249929][G train loss: 1.257524]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249916][G eval loss: 1.105079]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249895][G train loss: 1.256789]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249879][G eval loss: 1.104615]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249851][G train loss: 1.256329]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249827][G eval loss: 1.104324]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249788][G train loss: 1.256030]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249753][G eval loss: 1.104005]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249699][G train loss: 1.255690]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249650][G eval loss: 1.103648]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249579][G train loss: 1.255306]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249514][G eval loss: 1.103316]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249418][G train loss: 1.254941]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249345][G eval loss: 1.102912]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249221][G train loss: 1.254502]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249122][G eval loss: 1.102621]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248965][G train loss: 1.254178]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248827][G eval loss: 1.102619]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248628][G train loss: 1.254147]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248438][G eval loss: 1.103009]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248189][G train loss: 1.254520]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247958][G eval loss: 1.103776]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247645][G train loss: 1.255275]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247411][G eval loss: 1.104563]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247025][G train loss: 1.256042]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246593][G eval loss: 1.104864]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246136][G train loss: 1.256320]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245541][G eval loss: 1.104997]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.245014][G train loss: 1.256398]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244280][G eval loss: 1.105331]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243690][G train loss: 1.256659]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242749][G eval loss: 1.105988]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242095][G train loss: 1.257218]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.240948][G eval loss: 1.106713]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240222][G train loss: 1.257834]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238863][G eval loss: 1.107287]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238062][G train loss: 1.258263]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236398][G eval loss: 1.107607]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235532][G train loss: 1.258413]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.233645][G eval loss: 1.107211]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.232687][G train loss: 1.257827]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.230535][G eval loss: 1.105923]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.229528][G train loss: 1.256246]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.226937][G eval loss: 1.103603]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.225913][G train loss: 1.253462]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.222838][G eval loss: 1.099179]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.221950][G train loss: 1.248150]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.218291][G eval loss: 1.091688]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.217939][G train loss: 1.238653]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.213971][G eval loss: 1.079050]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.214857][G train loss: 1.222316]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.212029][G eval loss: 1.056584]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.214827][G train loss: 1.195474]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.216785][G eval loss: 1.018853]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.218044][G train loss: 1.160806]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.225397][G eval loss: 0.979970]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.222410][G train loss: 1.127975]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.230301][G eval loss: 0.961821]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.227640][G train loss: 1.105145]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.232717][G eval loss: 0.955222]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.231313][G train loss: 1.091723]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.234133][G eval loss: 0.943690]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.233603][G train loss: 1.075517]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.234911][G eval loss: 0.921591]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.234758][G train loss: 1.051925]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.235207][G eval loss: 0.890219]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.235169][G train loss: 1.021664]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.235234][G eval loss: 0.856126]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.235654][G train loss: 0.988248]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.235712][G eval loss: 0.825970]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.238077][G train loss: 0.954317]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.237485][G eval loss: 0.798833]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.244751][G train loss: 0.918364]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.241436][G eval loss: 0.771190]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.259284][G train loss: 0.874941]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.252048][G eval loss: 0.733760]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.274901][G train loss: 0.834684]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.269327][G eval loss: 0.691571]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.289971][G train loss: 0.798575]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.287467][G eval loss: 0.658325]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.305204][G train loss: 0.764476]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.304000][G eval loss: 0.635004]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.314464][G train loss: 0.740931]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.319167][G eval loss: 0.615236]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.320069][G train loss: 0.723897]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.343091][G eval loss: 0.577271]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.324629][G train loss: 0.706374]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.347747][G eval loss: 0.563611]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.330846][G train loss: 0.688291]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.349900][G eval loss: 0.547859]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.337574][G train loss: 0.668445]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.351535][G eval loss: 0.532317]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.343924][G train loss: 0.647695]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.352821][G eval loss: 0.523075]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.350066][G train loss: 0.628407]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.353556][G eval loss: 0.519488]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.354432][G train loss: 0.610498]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.354115][G eval loss: 0.517431]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.357334][G train loss: 0.595249]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.353683][G eval loss: 0.512563]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.357747][G train loss: 0.581306]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.351848][G eval loss: 0.505300]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.356508][G train loss: 0.568195]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.348691][G eval loss: 0.493660]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.353934][G train loss: 0.555239]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.344638][G eval loss: 0.485838]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.349711][G train loss: 0.544026]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.340346][G eval loss: 0.487020]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.343809][G train loss: 0.539161]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.336024][G eval loss: 0.489122]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.336502][G train loss: 0.537690]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.332085][G eval loss: 0.487545]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.328567][G train loss: 0.535225]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.328577][G eval loss: 0.480285]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.322908][G train loss: 0.530524]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.325168][G eval loss: 0.468193]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.316458][G train loss: 0.523455]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.321644][G eval loss: 0.455592]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.308027][G train loss: 0.516339]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.317772][G eval loss: 0.448838]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.301226][G train loss: 0.513064]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.312560][G eval loss: 0.450010]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.297858][G train loss: 0.516496]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.303129][G eval loss: 0.455803]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.294693][G train loss: 0.524672]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.291776][G eval loss: 0.461301]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.291641][G train loss: 0.532688]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.287527][G eval loss: 0.464232]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.288639][G train loss: 0.537323]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.283812][G eval loss: 0.464858]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.285677][G train loss: 0.538580]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.280171][G eval loss: 0.464415]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.282763][G train loss: 0.538005]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.276749][G eval loss: 0.464160]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.279964][G train loss: 0.536848]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.273546][G eval loss: 0.465149]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.277267][G train loss: 0.535976]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.270361][G eval loss: 0.467676]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.274811][G train loss: 0.536818]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.267262][G eval loss: 0.472954]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.0035 lr_d=0.0008 -> score=0.740216\n",
      "\n",
      "----- 0050: lr_g=0.0035, lr_d=0.001 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 1.118187]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 1.269859]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.109332]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.260937]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 1.108824]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.260363]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 1.109594]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249964][G train loss: 1.261168]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249950][G eval loss: 1.108137]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249936][G train loss: 1.259776]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249911][G eval loss: 1.106450]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249889][G train loss: 1.258138]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 1.105329]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249828][G train loss: 1.257039]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249787][G eval loss: 1.104709]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249740][G train loss: 1.256424]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249681][G eval loss: 1.104519]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249613][G train loss: 1.256225]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249521][G eval loss: 1.104315]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249426][G train loss: 1.256001]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249310][G eval loss: 1.104006]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249179][G train loss: 1.255665]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249011][G eval loss: 1.103688]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248835][G train loss: 1.255315]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248593][G eval loss: 1.103450]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248361][G train loss: 1.255041]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248018][G eval loss: 1.103468]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247715][G train loss: 1.255025]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247232][G eval loss: 1.103800]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246849][G train loss: 1.255327]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246338][G eval loss: 1.104362]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245858][G train loss: 1.255872]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245017][G eval loss: 1.104971]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244439][G train loss: 1.256467]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243260][G eval loss: 1.106017]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242585][G train loss: 1.257485]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241053][G eval loss: 1.107489]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.240274][G train loss: 1.258934]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238325][G eval loss: 1.109280]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.237433][G train loss: 1.260670]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235029][G eval loss: 1.111334]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.234008][G train loss: 1.262648]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.231161][G eval loss: 1.113362]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.230023][G train loss: 1.264568]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.226517][G eval loss: 1.115539]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.225330][G train loss: 1.266560]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.221071][G eval loss: 1.117762]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.219891][G train loss: 1.268603]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.214680][G eval loss: 1.120026]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.213492][G train loss: 1.270606]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.207352][G eval loss: 1.122155]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.206232][G train loss: 1.272396]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.199095][G eval loss: 1.124077]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.198098][G train loss: 1.273837]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.190063][G eval loss: 1.124956]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.189359][G train loss: 1.273829]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.180571][G eval loss: 1.122856]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.181059][G train loss: 1.268595]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.173898][G eval loss: 1.111214]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.179551][G train loss: 1.246695]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.184024][G eval loss: 1.067527]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.189525][G train loss: 1.204454]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.204296][G eval loss: 1.014659]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.205297][G train loss: 1.157337]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.221870][G eval loss: 0.978566]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.223079][G train loss: 1.116741]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.229872][G eval loss: 0.974873]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.238784][G train loss: 1.095806]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.233939][G eval loss: 0.999746]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.257482][G train loss: 1.092301]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.236136][G eval loss: 1.033215]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.273924][G train loss: 1.095721]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.237887][G eval loss: 1.063127]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.287239][G train loss: 1.104204]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.239796][G eval loss: 1.071339]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.294603][G train loss: 1.101279]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.243278][G eval loss: 1.059017]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.299921][G train loss: 1.086470]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.244568][G eval loss: 1.038764]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.301892][G train loss: 1.060388]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.243755][G eval loss: 1.010307]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.302835][G train loss: 1.026884]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.244157][G eval loss: 0.973401]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.302725][G train loss: 0.991701]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.245078][G eval loss: 0.938392]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.300981][G train loss: 0.961018]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.252054][G eval loss: 0.910401]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.297387][G train loss: 0.942283]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.261254][G eval loss: 0.887139]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.299045][G train loss: 0.925389]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.280278][G eval loss: 0.847658]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.307967][G train loss: 0.904668]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.304562][G eval loss: 0.802479]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.319190][G train loss: 0.886211]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.322584][G eval loss: 0.773896]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.335531][G train loss: 0.864989]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.342481][G eval loss: 0.732686]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.345896][G train loss: 0.854152]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.362348][G eval loss: 0.689757]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.350749][G train loss: 0.840575]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.360952][G eval loss: 0.677700]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.349671][G train loss: 0.827680]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.358780][G eval loss: 0.660158]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.347077][G train loss: 0.809332]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.356360][G eval loss: 0.640977]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.343231][G train loss: 0.787351]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.354047][G eval loss: 0.620778]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.338229][G train loss: 0.763112]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.351338][G eval loss: 0.601450]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.332034][G train loss: 0.738349]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.348222][G eval loss: 0.583461]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.321717][G train loss: 0.714268]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.344739][G eval loss: 0.567265]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.313054][G train loss: 0.690995]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.340836][G eval loss: 0.554254]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.303547][G train loss: 0.669120]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.336558][G eval loss: 0.543898]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.298825][G train loss: 0.649753]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.332082][G eval loss: 0.536998]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.294384][G train loss: 0.633281]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.327417][G eval loss: 0.534928]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.290103][G train loss: 0.620469]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.322503][G eval loss: 0.538500]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.286062][G train loss: 0.614512]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.316890][G eval loss: 0.549964]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.0035 lr_d=0.001 -> score=0.866853\n",
      "\n",
      "----- 0050: lr_g=0.0035, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 1.117008]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250028][G train loss: 1.268680]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 1.109133]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.260737]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 1.109499]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.261038]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 1.110624]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.262197]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249930][G eval loss: 1.109102]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249911][G train loss: 1.260740]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249871][G eval loss: 1.107122]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249840][G train loss: 1.258810]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249791][G eval loss: 1.105678]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249746][G train loss: 1.257389]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249673][G eval loss: 1.105424]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249604][G train loss: 1.257139]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249492][G eval loss: 1.105271]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249385][G train loss: 1.256979]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249203][G eval loss: 1.105038]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249044][G train loss: 1.256725]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248742][G eval loss: 1.104653]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248534][G train loss: 1.256314]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248107][G eval loss: 1.104378]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247841][G train loss: 1.256006]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247245][G eval loss: 1.104498]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246893][G train loss: 1.256092]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246240][G eval loss: 1.104987]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245768][G train loss: 1.256546]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244749][G eval loss: 1.105590]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.244145][G train loss: 1.257117]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242734][G eval loss: 1.106671]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241992][G train loss: 1.258178]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240086][G eval loss: 1.108275]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.239193][G train loss: 1.259764]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236686][G eval loss: 1.110520]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.235609][G train loss: 1.261988]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232509][G eval loss: 1.112936]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.231256][G train loss: 1.264375]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227133][G eval loss: 1.116131]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.225807][G train loss: 1.267497]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.220669][G eval loss: 1.119760]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.219374][G train loss: 1.271050]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.212987][G eval loss: 1.123771]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.211749][G train loss: 1.274928]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.203945][G eval loss: 1.128455]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.202835][G train loss: 1.279393]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.193467][G eval loss: 1.134184]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.192461][G train loss: 1.284842]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.181783][G eval loss: 1.140431]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.180867][G train loss: 1.290744]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.169029][G eval loss: 1.147045]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.168302][G train loss: 1.296818]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.155292][G eval loss: 1.154672]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.155169][G train loss: 1.303281]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.142041][G eval loss: 1.160729]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.143820][G train loss: 1.305176]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.141537][G eval loss: 1.142670]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.153094][G train loss: 1.271316]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.179952][G eval loss: 1.070218]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.182264][G train loss: 1.213469]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.206410][G eval loss: 1.033910]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.215339][G train loss: 1.161524]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.228983][G eval loss: 1.001112]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.243774][G train loss: 1.119047]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.235168][G eval loss: 1.001961]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.261947][G train loss: 1.099074]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.237988][G eval loss: 1.021693]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.0035 lr_d=0.0012 -> score=1.259681\n",
      "\n",
      "----- 0050: lr_g=0.0035, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250054][G eval loss: 1.115234]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250052][G train loss: 1.266906]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 1.108691]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.260295]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 1.110354]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.261894]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 1.112004]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.263577]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249959][G eval loss: 1.110368]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249942][G train loss: 1.262006]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249892][G eval loss: 1.108351]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249863][G train loss: 1.260039]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249796][G eval loss: 1.107094]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249750][G train loss: 1.258804]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249653][G eval loss: 1.106183]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249579][G train loss: 1.257898]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249407][G eval loss: 1.105435]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249295][G train loss: 1.257142]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249010][G eval loss: 1.104898]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248830][G train loss: 1.256586]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248343][G eval loss: 1.104358]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248018][G train loss: 1.256020]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247116][G eval loss: 1.103845]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246674][G train loss: 1.255475]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.245513][G eval loss: 1.103235]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.244932][G train loss: 1.254829]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.243303][G eval loss: 1.103641]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242529][G train loss: 1.255198]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.240274][G eval loss: 1.105509]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239224][G train loss: 1.257029]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.236481][G eval loss: 1.108742]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235053][G train loss: 1.260245]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.231544][G eval loss: 1.113602]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.229734][G train loss: 1.265091]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.225159][G eval loss: 1.120026]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223011][G train loss: 1.271487]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.217336][G eval loss: 1.126797]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.214994][G train loss: 1.278230]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.207324][G eval loss: 1.134255]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.205169][G train loss: 1.285659]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.195067][G eval loss: 1.141835]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.193316][G train loss: 1.293134]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.180411][G eval loss: 1.151106]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.179089][G train loss: 1.302242]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.163875][G eval loss: 1.162576]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.162852][G train loss: 1.313460]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.145651][G eval loss: 1.178140]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.145029][G train loss: 1.328592]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.127727][G eval loss: 1.196607]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.127647][G train loss: 1.346527]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.111473][G eval loss: 1.216506]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.111935][G train loss: 1.365588]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.097914][G eval loss: 1.235153]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.099116][G train loss: 1.382476]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.102361][G eval loss: 1.221272]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.116794][G train loss: 1.345682]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.182515][G eval loss: 1.101940]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.182886][G train loss: 1.247245]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.211620][G eval loss: 1.080604]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.229835][G train loss: 1.192751]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.240499][G eval loss: 1.041206]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.264869][G train loss: 1.149050]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.243872][G eval loss: 1.038680]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.286011][G train loss: 1.114930]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.246321][G eval loss: 1.042795]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.317829][G train loss: 1.075529]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.263249][G eval loss: 1.029979]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.347738][G train loss: 1.065399]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.282793][G eval loss: 1.051216]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.357692][G train loss: 1.088424]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.332174][G eval loss: 1.038742]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.396263][G train loss: 1.079326]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.339950][G eval loss: 1.070136]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.400442][G train loss: 1.105956]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.357223][G eval loss: 1.072664]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.401015][G train loss: 1.117438]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.373879][G eval loss: 1.047006]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.401782][G train loss: 1.113679]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.407807][G eval loss: 1.013793]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.403141][G train loss: 1.097775]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.417987][G eval loss: 0.969375]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.402751][G train loss: 1.076836]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.420686][G eval loss: 0.935562]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.399309][G train loss: 1.053104]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.410988][G eval loss: 0.915005]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.400316][G train loss: 1.028796]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.395931][G eval loss: 0.910993]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.387221][G train loss: 1.020325]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.383386][G eval loss: 0.909437]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.376323][G train loss: 1.003194]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.365928][G eval loss: 0.927250]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.370183][G train loss: 0.990987]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.357760][G eval loss: 0.938793]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.0035 lr_d=0.0015 -> score=1.296554\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.119275]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.270949]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.108129]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.259711]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.106901]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249986][G train loss: 1.258437]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.106161]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.257762]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.105135]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.256803]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 1.105068]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249959][G train loss: 1.256771]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 1.105553]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249944][G train loss: 1.257264]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 1.105930]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249926][G train loss: 1.257632]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 1.105726]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249904][G train loss: 1.257400]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249899][G eval loss: 1.104848]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249876][G train loss: 1.256486]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249869][G eval loss: 1.103633]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249840][G train loss: 1.255223]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 1.102427]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249797][G train loss: 1.253971]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249786][G eval loss: 1.101383]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249743][G train loss: 1.252883]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249729][G eval loss: 1.100673]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249678][G train loss: 1.252138]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249659][G eval loss: 1.100297]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249597][G train loss: 1.251712]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249577][G eval loss: 1.100231]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249501][G train loss: 1.251582]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249477][G eval loss: 1.100348]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249387][G train loss: 1.251635]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249357][G eval loss: 1.100292]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249250][G train loss: 1.251506]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249217][G eval loss: 1.099502]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249089][G train loss: 1.250608]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249054][G eval loss: 1.097937]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248903][G train loss: 1.248861]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248861][G eval loss: 1.095587]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248682][G train loss: 1.246260]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248626][G eval loss: 1.092116]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248418][G train loss: 1.242439]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248327][G eval loss: 1.086703]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248088][G train loss: 1.236572]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.247977][G eval loss: 1.078333]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247706][G train loss: 1.227635]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247595][G eval loss: 1.065985]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247284][G train loss: 1.214485]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247200][G eval loss: 1.048447]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246843][G train loss: 1.195944]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246778][G eval loss: 1.025737]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246371][G train loss: 1.171796]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246377][G eval loss: 0.997435]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245899][G train loss: 1.141487]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.246000][G eval loss: 0.961977]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.245434][G train loss: 1.104153]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245643][G eval loss: 0.919482]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.244990][G train loss: 1.060108]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.245313][G eval loss: 0.872464]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.244558][G train loss: 1.011675]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.245055][G eval loss: 0.825606]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.244149][G train loss: 0.962630]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.244898][G eval loss: 0.785120]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.243767][G train loss: 0.917053]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.244772][G eval loss: 0.754281]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.243367][G train loss: 0.876966]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.244608][G eval loss: 0.736057]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.242894][G train loss: 0.845592]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.244392][G eval loss: 0.732642]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.242360][G train loss: 0.826491]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.244304][G eval loss: 0.740996]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.241911][G train loss: 0.817197]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.244569][G eval loss: 0.750104]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.0005 -> score=0.994673\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.117348]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.269022]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.107848]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.259430]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.107934]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.259470]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 1.107752]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.259352]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.106480]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.258148]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 1.105823]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249934][G train loss: 1.257526]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 1.105642]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249900][G train loss: 1.257354]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 1.105422]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249856][G train loss: 1.257124]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249829][G eval loss: 1.104922]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249791][G train loss: 1.256597]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249755][G eval loss: 1.103952]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249704][G train loss: 1.255591]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249656][G eval loss: 1.102809]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249587][G train loss: 1.254401]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249521][G eval loss: 1.101852]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249428][G train loss: 1.253398]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249352][G eval loss: 1.101090]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249232][G train loss: 1.252594]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249129][G eval loss: 1.100708]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248976][G train loss: 1.252176]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248830][G eval loss: 1.100744]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248635][G train loss: 1.252164]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248435][G eval loss: 1.101221]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248189][G train loss: 1.252581]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247928][G eval loss: 1.102093]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247619][G train loss: 1.253393]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247369][G eval loss: 1.102612]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246991][G train loss: 1.253839]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246538][G eval loss: 1.101883]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246085][G train loss: 1.253011]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245511][G eval loss: 1.100232]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244968][G train loss: 1.251205]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244334][G eval loss: 1.097981]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243709][G train loss: 1.248718]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242917][G eval loss: 1.095047]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242209][G train loss: 1.245433]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241217][G eval loss: 1.090575]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240423][G train loss: 1.240480]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239243][G eval loss: 1.083274]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238351][G train loss: 1.232568]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236957][G eval loss: 1.072087]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235960][G train loss: 1.220541]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234424][G eval loss: 1.055990]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.233300][G train loss: 1.203394]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231631][G eval loss: 1.034966]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.230363][G train loss: 1.180851]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.228569][G eval loss: 1.008484]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.227142][G train loss: 1.152276]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.225200][G eval loss: 0.974675]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.223656][G train loss: 1.116490]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.221947][G eval loss: 0.932603]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.220142][G train loss: 1.073285]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.219502][G eval loss: 0.883689]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.217126][G train loss: 1.024095]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.218658][G eval loss: 0.832171]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.215622][G train loss: 0.971668]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.221429][G eval loss: 0.781474]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.217015][G train loss: 0.918553]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.228506][G eval loss: 0.735036]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.222167][G train loss: 0.866600]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.238255][G eval loss: 0.696277]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.230798][G train loss: 0.817407]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.250014][G eval loss: 0.665469]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.242000][G train loss: 0.774323]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.263483][G eval loss: 0.639508]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.254340][G train loss: 0.738828]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.277342][G eval loss: 0.618302]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.267042][G train loss: 0.709377]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.290981][G eval loss: 0.597570]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.279433][G train loss: 0.682120]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.302760][G eval loss: 0.575034]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.290966][G train loss: 0.655376]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.312883][G eval loss: 0.549432]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.301308][G train loss: 0.628705]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.321247][G eval loss: 0.525353]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.309518][G train loss: 0.603509]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.327861][G eval loss: 0.505094]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.315519][G train loss: 0.582509]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.332504][G eval loss: 0.488767]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.319466][G train loss: 0.565839]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.334148][G eval loss: 0.477095]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.321850][G train loss: 0.553709]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.334225][G eval loss: 0.468145]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.323206][G train loss: 0.544595]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.333293][G eval loss: 0.461388]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.323749][G train loss: 0.537106]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.331393][G eval loss: 0.457375]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.322852][G train loss: 0.532372]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.328569][G eval loss: 0.455950]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.320276][G train loss: 0.530057]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.325559][G eval loss: 0.455366]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.317726][G train loss: 0.527674]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.322156][G eval loss: 0.454873]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.315422][G train loss: 0.524987]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.318218][G eval loss: 0.453617]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.312949][G train loss: 0.521860]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.314182][G eval loss: 0.450924]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.309992][G train loss: 0.517351]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.309999][G eval loss: 0.446332]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.307085][G train loss: 0.510987]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.304943][G eval loss: 0.443888]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.303634][G train loss: 0.506479]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.299970][G eval loss: 0.441739]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299643][G train loss: 0.501919]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.295039][G eval loss: 0.441201]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.295083][G train loss: 0.499082]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.290655][G eval loss: 0.443324]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.290636][G train loss: 0.499080]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.286974][G eval loss: 0.443465]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.286837][G train loss: 0.498879]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.283545][G eval loss: 0.444566]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.283169][G train loss: 0.500298]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.279999][G eval loss: 0.445908]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.279103][G train loss: 0.502009]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.276858][G eval loss: 0.445695]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.275081][G train loss: 0.502841]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.274115][G eval loss: 0.446005]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.271605][G train loss: 0.504608]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.271596][G eval loss: 0.446998]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.269004][G train loss: 0.506768]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.269076][G eval loss: 0.448190]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.266708][G train loss: 0.508424]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.266521][G eval loss: 0.449291]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.264536][G train loss: 0.509336]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.263590][G eval loss: 0.449091]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.262477][G train loss: 0.509300]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.260577][G eval loss: 0.447475]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.260595][G train loss: 0.508215]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.257835][G eval loss: 0.446247]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.258982][G train loss: 0.507135]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.256090][G eval loss: 0.446100]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.257574][G train loss: 0.507109]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.254893][G eval loss: 0.445716]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.256320][G train loss: 0.507197]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.253912][G eval loss: 0.445059]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.255211][G train loss: 0.506519]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.253114][G eval loss: 0.445002]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.254254][G train loss: 0.505375]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.252473][G eval loss: 0.446369]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.253438][G train loss: 0.504807]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.251914][G eval loss: 0.448215]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.0008 -> score=0.700129\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.001 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 1.116171]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 1.267844]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 1.107673]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.259255]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.108644]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.260181]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 1.108809]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249965][G train loss: 1.260410]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249952][G eval loss: 1.107412]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249939][G train loss: 1.259080]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 1.106431]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249897][G train loss: 1.258134]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249867][G eval loss: 1.105880]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249837][G train loss: 1.257592]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249794][G eval loss: 1.105497]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249749][G train loss: 1.257199]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249687][G eval loss: 1.105077]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249622][G train loss: 1.256753]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249530][G eval loss: 1.104199]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249438][G train loss: 1.255840]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249321][G eval loss: 1.103112]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249194][G train loss: 1.254706]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249023][G eval loss: 1.102191]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248851][G train loss: 1.253739]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248601][G eval loss: 1.101598]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248376][G train loss: 1.253104]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248028][G eval loss: 1.101521]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247733][G train loss: 1.252994]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247238][G eval loss: 1.101930]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246859][G train loss: 1.253360]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246351][G eval loss: 1.102577]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245876][G train loss: 1.253946]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245017][G eval loss: 1.103332]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244436][G train loss: 1.254655]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243256][G eval loss: 1.104121]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242574][G train loss: 1.255380]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241084][G eval loss: 1.104424]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.240303][G train loss: 1.255584]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238426][G eval loss: 1.104204]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.237534][G train loss: 1.255196]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235225][G eval loss: 1.103539]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.234215][G train loss: 1.254282]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.231486][G eval loss: 1.101820]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.230359][G train loss: 1.252208]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.227055][G eval loss: 1.098445]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.225835][G train loss: 1.248333]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.221749][G eval loss: 1.092525]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.220516][G train loss: 1.241765]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.215699][G eval loss: 1.082622]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.214417][G train loss: 1.230926]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.208709][G eval loss: 1.068415]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.207431][G train loss: 1.215564]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.201098][G eval loss: 1.049602]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.199810][G train loss: 1.195059]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.192841][G eval loss: 1.025746]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.191597][G train loss: 1.168901]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.184222][G eval loss: 0.994314]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.183145][G train loss: 1.134928]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.176124][G eval loss: 0.953846]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.175771][G train loss: 1.091273]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.172868][G eval loss: 0.899435]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.174521][G train loss: 1.032238]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.190882][G eval loss: 0.809265]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.190961][G train loss: 0.944590]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.235776][G eval loss: 0.695268]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.233583][G train loss: 0.831161]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.276027][G eval loss: 0.621473]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.273203][G train loss: 0.752678]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.303321][G eval loss: 0.581332]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.300691][G train loss: 0.704992]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.319442][G eval loss: 0.558190]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.317377][G train loss: 0.673251]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.329859][G eval loss: 0.543727]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.327954][G train loss: 0.650012]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.337413][G eval loss: 0.535044]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.335438][G train loss: 0.631360]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.343368][G eval loss: 0.525369]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.340358][G train loss: 0.613188]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.348508][G eval loss: 0.513134]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.343520][G train loss: 0.594278]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.353581][G eval loss: 0.498469]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.345339][G train loss: 0.574358]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.361860][G eval loss: 0.481383]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.346035][G train loss: 0.553724]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.364554][G eval loss: 0.462986]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.345854][G train loss: 0.532096]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.365285][G eval loss: 0.444435]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.345459][G train loss: 0.512527]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.364652][G eval loss: 0.427719]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.345134][G train loss: 0.495720]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.363032][G eval loss: 0.411602]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.343420][G train loss: 0.481685]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.360639][G eval loss: 0.400016]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.341208][G train loss: 0.472159]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.357705][G eval loss: 0.394061]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.339123][G train loss: 0.467725]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.354546][G eval loss: 0.395455]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.335431][G train loss: 0.469892]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.351153][G eval loss: 0.400807]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.329888][G train loss: 0.474030]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.347674][G eval loss: 0.403550]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.323496][G train loss: 0.474855]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.343843][G eval loss: 0.403587]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.318321][G train loss: 0.472042]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.339656][G eval loss: 0.403018]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.313590][G train loss: 0.467441]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.335190][G eval loss: 0.407081]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.308850][G train loss: 0.466702]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.330535][G eval loss: 0.410541]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.304108][G train loss: 0.466949]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.325730][G eval loss: 0.409296]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299352][G train loss: 0.464822]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.320753][G eval loss: 0.406817]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.294562][G train loss: 0.462928]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.315598][G eval loss: 0.408766]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.289750][G train loss: 0.465406]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.310232][G eval loss: 0.414239]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.284977][G train loss: 0.471132]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.304687][G eval loss: 0.418085]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.280343][G train loss: 0.475688]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.299075][G eval loss: 0.420728]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.276022][G train loss: 0.479792]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.293564][G eval loss: 0.425662]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.272106][G train loss: 0.486750]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.287961][G eval loss: 0.432691]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.268258][G train loss: 0.496458]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.282567][G eval loss: 0.438829]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.263938][G train loss: 0.507395]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.277875][G eval loss: 0.445092]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.259136][G train loss: 0.520184]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.274304][G eval loss: 0.454134]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.258991][G train loss: 0.527500]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.271368][G eval loss: 0.462104]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.257277][G train loss: 0.536719]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.268918][G eval loss: 0.468243]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.255743][G train loss: 0.543748]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.267013][G eval loss: 0.474976]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.253852][G train loss: 0.551611]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.265515][G eval loss: 0.483342]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.253689][G train loss: 0.558543]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.264464][G eval loss: 0.489744]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.253196][G train loss: 0.563946]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.263894][G eval loss: 0.494340]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.253053][G train loss: 0.567689]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.263718][G eval loss: 0.500600]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.252848][G train loss: 0.573890]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.263697][G eval loss: 0.511079]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.252455][G train loss: 0.583619]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.263792][G eval loss: 0.522250]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.252330][G train loss: 0.593366]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.263811][G eval loss: 0.529987]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.252358][G train loss: 0.599884]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.263920][G eval loss: 0.533785]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.252497][G train loss: 0.602797]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.264104][G eval loss: 0.536412]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.252733][G train loss: 0.604475]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.264546][G eval loss: 0.540638]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.253046][G train loss: 0.606525]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.264909][G eval loss: 0.546347]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.253404][G train loss: 0.608279]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.265047][G eval loss: 0.551624]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.253917][G train loss: 0.608882]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.264916][G eval loss: 0.552944]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.253353][G train loss: 0.611236]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.264630][G eval loss: 0.550315]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.253247][G train loss: 0.610969]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.264200][G eval loss: 0.548874]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.253027][G train loss: 0.611219]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.263665][G eval loss: 0.548193]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.252841][G train loss: 0.611721]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.263036][G eval loss: 0.547959]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.253635][G train loss: 0.609080]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.262375][G eval loss: 0.547273]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.253527][G train loss: 0.604421]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.261654][G eval loss: 0.548683]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.253561][G train loss: 0.597897]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.261036][G eval loss: 0.558938]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.001 -> score=0.819974\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 1.114991]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250029][G train loss: 1.266665]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.107474]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.259055]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 1.109319]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.260855]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 1.109840]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.261440]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249935][G eval loss: 1.108373]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249916][G train loss: 1.260041]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249878][G eval loss: 1.107101]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249849][G train loss: 1.258805]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249797][G eval loss: 1.106237]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249754][G train loss: 1.257949]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249675][G eval loss: 1.106217]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249609][G train loss: 1.257920]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249496][G eval loss: 1.105857]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249393][G train loss: 1.257533]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249225][G eval loss: 1.104958]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249070][G train loss: 1.256600]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248774][G eval loss: 1.103803]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248571][G train loss: 1.255399]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248152][G eval loss: 1.102811]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247893][G train loss: 1.254362]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247297][G eval loss: 1.102483]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246960][G train loss: 1.253994]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246269][G eval loss: 1.102867]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245815][G train loss: 1.254346]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244762][G eval loss: 1.103499]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.244171][G train loss: 1.254942]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242699][G eval loss: 1.104732]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241964][G train loss: 1.256123]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240008][G eval loss: 1.106510]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.239119][G train loss: 1.257855]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236636][G eval loss: 1.108501]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.235566][G train loss: 1.259768]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232527][G eval loss: 1.109794]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.231288][G train loss: 1.260957]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227304][G eval loss: 1.111045]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.225966][G train loss: 1.262037]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221008][G eval loss: 1.111966]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.219701][G train loss: 1.262708]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.213564][G eval loss: 1.112175]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.212284][G train loss: 1.262552]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.204801][G eval loss: 1.111251]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.203625][G train loss: 1.261080]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.194655][G eval loss: 1.108435]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.193556][G train loss: 1.257527]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.183386][G eval loss: 1.102129]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.182324][G train loss: 1.250216]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.171197][G eval loss: 1.091333]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.170229][G train loss: 1.238105]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.158302][G eval loss: 1.077215]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.157603][G train loss: 1.221949]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.145405][G eval loss: 1.059211]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.145171][G train loss: 1.201029]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.133993][G eval loss: 1.033157]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.135117][G train loss: 1.169824]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.129182][G eval loss: 0.989625]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.145666][G train loss: 1.098715]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.170433][G eval loss: 0.871871]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.178854][G train loss: 1.000695]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.275845][G eval loss: 0.698090]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.276035][G train loss: 0.826793]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.332638][G eval loss: 0.624575]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.332821][G train loss: 0.754581]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.351869][G eval loss: 0.580622]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.352589][G train loss: 0.711017]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.361958][G eval loss: 0.551581]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.363663][G train loss: 0.679890]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.368363][G eval loss: 0.543397]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.370762][G train loss: 0.663407]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.372761][G eval loss: 0.536414]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.375353][G train loss: 0.646931]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.376165][G eval loss: 0.522107]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.378108][G train loss: 0.624148]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.379864][G eval loss: 0.506770]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.379442][G train loss: 0.599793]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.386073][G eval loss: 0.498858]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.379634][G train loss: 0.581807]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.401793][G eval loss: 0.495126]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.378883][G train loss: 0.571123]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.408968][G eval loss: 0.490859]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.0012 -> score=0.899828\n",
      "\n",
      "----- 0050: lr_g=0.004, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250054][G eval loss: 1.113217]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250053][G train loss: 1.264891]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 1.107029]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.258610]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 1.110167]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.261703]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.111211]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.262812]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249963][G eval loss: 1.109636]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249947][G train loss: 1.261304]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 1.108336]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249872][G train loss: 1.260039]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249808][G eval loss: 1.107621]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249763][G train loss: 1.259333]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249665][G eval loss: 1.106924]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249594][G train loss: 1.258627]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249422][G eval loss: 1.105941]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249315][G train loss: 1.257618]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249033][G eval loss: 1.104727]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248860][G train loss: 1.256370]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248368][G eval loss: 1.103388]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248055][G train loss: 1.254986]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247142][G eval loss: 1.102237]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246718][G train loss: 1.253790]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.245595][G eval loss: 1.101206]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245038][G train loss: 1.252722]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.243424][G eval loss: 1.101515]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242681][G train loss: 1.253005]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.240444][G eval loss: 1.103434]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239431][G train loss: 1.254897]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.236690][G eval loss: 1.106631]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235310][G train loss: 1.258050]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.231729][G eval loss: 1.111530]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.229965][G train loss: 1.262901]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.225296][G eval loss: 1.117790]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223194][G train loss: 1.269083]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.217657][G eval loss: 1.123288]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.215350][G train loss: 1.274465]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.207882][G eval loss: 1.128696]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.205754][G train loss: 1.279687]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.195873][G eval loss: 1.133526]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.194127][G train loss: 1.284265]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.181508][G eval loss: 1.138912]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.180162][G train loss: 1.289283]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.165275][G eval loss: 1.144581]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.164174][G train loss: 1.294391]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.147368][G eval loss: 1.150960]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.146567][G train loss: 1.299954]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.129551][G eval loss: 1.155985]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.129201][G train loss: 1.303856]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.113171][G eval loss: 1.157951]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.113257][G train loss: 1.304318]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.098906][G eval loss: 1.156969]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.099378][G train loss: 1.300939]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.087202][G eval loss: 1.150542]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.088375][G train loss: 1.289992]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.080925][G eval loss: 1.126542]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.108618][G train loss: 1.214186]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.207755][G eval loss: 0.900253]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.189389][G train loss: 1.065079]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.338125][G eval loss: 0.733485]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.351582][G train loss: 0.840911]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.384060][G eval loss: 0.680492]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.386134][G train loss: 0.804374]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.393316][G eval loss: 0.636666]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.396813][G train loss: 0.762393]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.398859][G eval loss: 0.586896]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.403454][G train loss: 0.718370]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.402271][G eval loss: 0.554728]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.407521][G train loss: 0.687568]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.404116][G eval loss: 0.551520]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.409728][G train loss: 0.677411]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.404718][G eval loss: 0.555532]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.410562][G train loss: 0.671395]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.404275][G eval loss: 0.547668]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.410295][G train loss: 0.654396]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.402922][G eval loss: 0.526905]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.409080][G train loss: 0.626463]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.400842][G eval loss: 0.502423]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.407007][G train loss: 0.595407]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.417123][G eval loss: 0.486622]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.404229][G train loss: 0.570505]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.418365][G eval loss: 0.488019]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.401758][G train loss: 0.557492]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.415567][G eval loss: 0.492055]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.004 lr_d=0.0015 -> score=0.907621\n",
      "\n",
      "----- 0050: lr_g=0.0045, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.117444]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.269119]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.107194]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.258755]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.106875]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.258419]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.105370]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.256998]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.104736]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.256428]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.105475]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249960][G train loss: 1.257193]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 1.106403]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249946][G train loss: 1.258119]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 1.106492]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249927][G train loss: 1.258180]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 1.105583]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249904][G train loss: 1.257228]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249898][G eval loss: 1.104096]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249876][G train loss: 1.255684]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249868][G eval loss: 1.102530]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249840][G train loss: 1.254061]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249831][G eval loss: 1.101248]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249797][G train loss: 1.252745]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249786][G eval loss: 1.100382]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249744][G train loss: 1.251860]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249728][G eval loss: 1.099935]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249678][G train loss: 1.251374]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249656][G eval loss: 1.099761]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249595][G train loss: 1.251131]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249573][G eval loss: 1.099675]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249500][G train loss: 1.250961]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249472][G eval loss: 1.099144]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249386][G train loss: 1.250318]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249350][G eval loss: 1.097839]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249250][G train loss: 1.248860]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249203][G eval loss: 1.095504]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249087][G train loss: 1.246308]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249038][G eval loss: 1.091721]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248904][G train loss: 1.242253]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248847][G eval loss: 1.085855]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248694][G train loss: 1.236011]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248623][G eval loss: 1.076830]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248453][G train loss: 1.226404]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248352][G eval loss: 1.063087]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248167][G train loss: 1.211805]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248051][G eval loss: 1.042005]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247857][G train loss: 1.189413]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247768][G eval loss: 1.012387]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247555][G train loss: 1.158051]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247537][G eval loss: 0.976059]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.247294][G train loss: 1.119506]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.247359][G eval loss: 0.936189]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.247077][G train loss: 1.076809]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.247189][G eval loss: 0.893906]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.246883][G train loss: 1.031512]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.246935][G eval loss: 0.847055]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.246625][G train loss: 0.983417]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.246526][G eval loss: 0.798746]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.246232][G train loss: 0.935360]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.246011][G eval loss: 0.760630]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.245714][G train loss: 0.894670]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.245538][G eval loss: 0.738977]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.245212][G train loss: 0.863261]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.245241][G eval loss: 0.729012]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.244816][G train loss: 0.838432]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.245150][G eval loss: 0.727104]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.244596][G train loss: 0.821048]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.245407][G eval loss: 0.734019]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.244556][G train loss: 0.812566]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.245851][G eval loss: 0.740931]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.0045 lr_d=0.0005 -> score=0.986782\n",
      "\n",
      "----- 0050: lr_g=0.0045, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.115516]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.267191]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.106914]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.258475]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.107912]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.259456]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 1.106964]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.258592]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 1.106084]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249959][G train loss: 1.257776]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249951][G eval loss: 1.106227]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249937][G train loss: 1.257946]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249924][G eval loss: 1.106486]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249904][G train loss: 1.258202]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249886][G eval loss: 1.105974]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249860][G train loss: 1.257663]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 1.104767]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249795][G train loss: 1.256412]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249759][G eval loss: 1.103184]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249710][G train loss: 1.254774]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249660][G eval loss: 1.101691]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249594][G train loss: 1.253224]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249526][G eval loss: 1.100639]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249437][G train loss: 1.252138]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249359][G eval loss: 1.100052]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249244][G train loss: 1.251532]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249136][G eval loss: 1.099941]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248988][G train loss: 1.251382]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248835][G eval loss: 1.100204]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248647][G train loss: 1.251576]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248442][G eval loss: 1.100695]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248208][G train loss: 1.251982]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247938][G eval loss: 1.100956]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247649][G train loss: 1.252130]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247404][G eval loss: 1.100196]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247050][G train loss: 1.251216]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246572][G eval loss: 1.097913]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246158][G train loss: 1.248701]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245538][G eval loss: 1.094048]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.245077][G train loss: 1.244516]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244393][G eval loss: 1.088236]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243893][G train loss: 1.238267]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243074][G eval loss: 1.079585]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242542][G train loss: 1.228960]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241571][G eval loss: 1.066462]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.241017][G train loss: 1.214856]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239922][G eval loss: 1.045863]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.239373][G train loss: 1.192702]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.238224][G eval loss: 1.016835]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.237726][G train loss: 1.161562]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.236817][G eval loss: 0.981044]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.236333][G train loss: 1.123176]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.236020][G eval loss: 0.941715]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.235403][G train loss: 1.080687]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.235486][G eval loss: 0.900940]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.234738][G train loss: 1.035981]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.234355][G eval loss: 0.856417]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.233758][G train loss: 0.989010]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.232151][G eval loss: 0.809478]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.232006][G train loss: 0.941748]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.229289][G eval loss: 0.771200]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.229703][G train loss: 0.901068]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.227394][G eval loss: 0.748136]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.228254][G train loss: 0.869109]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.228794][G eval loss: 0.730310]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.229226][G train loss: 0.838918]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.236430][G eval loss: 0.708710]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.234751][G train loss: 0.806492]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.251842][G eval loss: 0.682342]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.246284][G train loss: 0.774143]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.266815][G eval loss: 0.665387]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.260893][G train loss: 0.747148]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.283041][G eval loss: 0.646423]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.275249][G train loss: 0.725059]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.299445][G eval loss: 0.627484]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.288284][G train loss: 0.701870]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.312559][G eval loss: 0.603835]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.299106][G train loss: 0.675137]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.322289][G eval loss: 0.577856]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.308413][G train loss: 0.647571]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.328900][G eval loss: 0.558579]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.315462][G train loss: 0.625050]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.333209][G eval loss: 0.541907]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.320356][G train loss: 0.606742]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.335669][G eval loss: 0.523094]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.323273][G train loss: 0.588543]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.335746][G eval loss: 0.505206]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.324276][G train loss: 0.571471]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.333339][G eval loss: 0.492498]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.323429][G train loss: 0.559722]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.328883][G eval loss: 0.485286]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.321490][G train loss: 0.553636]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.323344][G eval loss: 0.481829]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.318565][G train loss: 0.548597]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.317148][G eval loss: 0.480754]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.314932][G train loss: 0.545461]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.310966][G eval loss: 0.483053]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.311305][G train loss: 0.546876]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.305528][G eval loss: 0.482450]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.307909][G train loss: 0.548692]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.301002][G eval loss: 0.477718]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.304487][G train loss: 0.548116]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.297142][G eval loss: 0.475355]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.300805][G train loss: 0.547992]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.292818][G eval loss: 0.479753]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.296801][G train loss: 0.551006]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.287969][G eval loss: 0.484312]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.292523][G train loss: 0.553573]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.282864][G eval loss: 0.483803]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.288284][G train loss: 0.551722]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.278229][G eval loss: 0.481414]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.284061][G train loss: 0.548293]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.274115][G eval loss: 0.482024]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.279734][G train loss: 0.546876]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.270895][G eval loss: 0.483535]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.275785][G train loss: 0.545669]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.268454][G eval loss: 0.482757]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.272305][G train loss: 0.542282]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.266521][G eval loss: 0.482023]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.269352][G train loss: 0.539448]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.264670][G eval loss: 0.484578]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.266712][G train loss: 0.540004]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.262740][G eval loss: 0.487815]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.0045 lr_d=0.0008 -> score=0.750555\n",
      "\n",
      "----- 0050: lr_g=0.0045, lr_d=0.001 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 1.114339]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 1.266014]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.106739]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.258300]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.108624]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.260169]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 1.108023]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.259651]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 1.107015]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249942][G train loss: 1.258707]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 1.106836]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249901][G train loss: 1.258555]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249871][G eval loss: 1.106726]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249841][G train loss: 1.258443]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249796][G eval loss: 1.106056]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249752][G train loss: 1.257745]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249688][G eval loss: 1.104856]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249625][G train loss: 1.256503]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249534][G eval loss: 1.103307]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249446][G train loss: 1.254899]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249324][G eval loss: 1.101838]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249201][G train loss: 1.253371]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249027][G eval loss: 1.100836]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248861][G train loss: 1.252335]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248611][G eval loss: 1.100373]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248394][G train loss: 1.251854]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248037][G eval loss: 1.100603]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247753][G train loss: 1.252047]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247240][G eval loss: 1.101279]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246876][G train loss: 1.252653]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246334][G eval loss: 1.102006]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245876][G train loss: 1.253296]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.244987][G eval loss: 1.102241]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244434][G train loss: 1.253414]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243229][G eval loss: 1.101914]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242593][G train loss: 1.252908]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241071][G eval loss: 1.100770]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.240355][G train loss: 1.251511]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238434][G eval loss: 1.098446]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.237648][G train loss: 1.248837]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235316][G eval loss: 1.094220]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.234463][G train loss: 1.244154]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.231754][G eval loss: 1.086769]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.230858][G train loss: 1.235977]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.227597][G eval loss: 1.074670]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.226794][G train loss: 1.222565]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.223067][G eval loss: 1.054620]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.222590][G train loss: 1.200374]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.218759][G eval loss: 1.025442]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.219106][G train loss: 1.167706]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.216815][G eval loss: 0.986028]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.218055][G train loss: 1.124478]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.220748][G eval loss: 0.938215]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.219515][G train loss: 1.077828]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.225789][G eval loss: 0.896267]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.222440][G train loss: 1.033075]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.228538][G eval loss: 0.857618]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.225538][G train loss: 0.986667]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.230248][G eval loss: 0.811410]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.228595][G train loss: 0.934081]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.232580][G eval loss: 0.759829]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.233130][G train loss: 0.881213]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.237847][G eval loss: 0.716472]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.243670][G train loss: 0.829552]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.250780][G eval loss: 0.682304]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.261565][G train loss: 0.783619]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.272821][G eval loss: 0.649906]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.283544][G train loss: 0.747838]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.300226][G eval loss: 0.624431]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.304312][G train loss: 0.725136]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.324161][G eval loss: 0.608116]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.321454][G train loss: 0.708364]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.348213][G eval loss: 0.587074]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.333539][G train loss: 0.694657]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.358448][G eval loss: 0.576897]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.341403][G train loss: 0.678524]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.363060][G eval loss: 0.565488]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.345773][G train loss: 0.659645]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.363596][G eval loss: 0.551551]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.349129][G train loss: 0.637952]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.362341][G eval loss: 0.532475]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.353162][G train loss: 0.610428]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.360553][G eval loss: 0.509343]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.354771][G train loss: 0.583216]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.358471][G eval loss: 0.488089]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.354377][G train loss: 0.559230]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.355846][G eval loss: 0.472143]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.353179][G train loss: 0.539966]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.352676][G eval loss: 0.455302]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.351953][G train loss: 0.521178]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.349238][G eval loss: 0.442895]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.350369][G train loss: 0.507088]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.345463][G eval loss: 0.433172]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.347804][G train loss: 0.498020]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.341247][G eval loss: 0.421044]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.343531][G train loss: 0.488265]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.336303][G eval loss: 0.415053]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.336926][G train loss: 0.484942]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.330247][G eval loss: 0.412560]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.329339][G train loss: 0.483052]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.321953][G eval loss: 0.410075]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.322042][G train loss: 0.479699]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.315065][G eval loss: 0.411251]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.315713][G train loss: 0.478523]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.309729][G eval loss: 0.411172]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.310187][G train loss: 0.475363]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.304863][G eval loss: 0.416534]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.304627][G train loss: 0.476133]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.300230][G eval loss: 0.423008]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.0045 lr_d=0.001 -> score=0.723238\n",
      "\n",
      "----- 0050: lr_g=0.0045, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 1.113159]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250029][G train loss: 1.264834]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.106540]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 1.258101]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 1.109300]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.260844]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.109055]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249958][G train loss: 1.260683]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 1.107972]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249922][G train loss: 1.259664]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249885][G eval loss: 1.107498]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249857][G train loss: 1.259217]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249803][G eval loss: 1.107069]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249762][G train loss: 1.258785]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249681][G eval loss: 1.106734]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249618][G train loss: 1.258424]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249503][G eval loss: 1.105660]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249405][G train loss: 1.257307]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249238][G eval loss: 1.104158]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249090][G train loss: 1.255751]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248794][G eval loss: 1.102656]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248597][G train loss: 1.254191]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248167][G eval loss: 1.101612]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247915][G train loss: 1.253112]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247329][G eval loss: 1.101462]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246998][G train loss: 1.252945]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246314][G eval loss: 1.102115]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245869][G train loss: 1.253564]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244795][G eval loss: 1.102998]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.244219][G train loss: 1.254378]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242744][G eval loss: 1.104198]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.242034][G train loss: 1.255492]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240056][G eval loss: 1.105386]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.239214][G train loss: 1.256549]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236739][G eval loss: 1.106141]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.235733][G train loss: 1.257131]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232650][G eval loss: 1.105970]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.231519][G train loss: 1.256659]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227534][G eval loss: 1.104986]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.226363][G train loss: 1.255259]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221406][G eval loss: 1.102216]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.220343][G train loss: 1.251957]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.214264][G eval loss: 1.096602]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.213338][G train loss: 1.245522]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.205990][G eval loss: 1.086618]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.205418][G train loss: 1.234067]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.197008][G eval loss: 1.068902]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.197504][G train loss: 1.212841]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.190815][G eval loss: 1.037048]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.194815][G train loss: 1.173037]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.200970][G eval loss: 0.975107]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.202103][G train loss: 1.116343]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.217707][G eval loss: 0.921305]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.212868][G train loss: 1.065211]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.225425][G eval loss: 0.901669]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.225486][G train loss: 1.027665]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.230504][G eval loss: 0.886340]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.240818][G train loss: 0.986229]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.235124][G eval loss: 0.862675]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.258377][G train loss: 0.935541]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.240465][G eval loss: 0.822354]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.272512][G train loss: 0.877805]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.248377][G eval loss: 0.764736]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.283686][G train loss: 0.820498]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.259398][G eval loss: 0.704733]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.292123][G train loss: 0.780067]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.272234][G eval loss: 0.682542]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.299727][G train loss: 0.762213]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.290629][G eval loss: 0.668861]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.312635][G train loss: 0.750054]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.306344][G eval loss: 0.659491]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.321784][G train loss: 0.740092]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.327760][G eval loss: 0.623245]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.328569][G train loss: 0.724446]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.355326][G eval loss: 0.578122]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.335872][G train loss: 0.707777]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.361363][G eval loss: 0.564614]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.341793][G train loss: 0.692847]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.363940][G eval loss: 0.553259]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.348570][G train loss: 0.677893]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.364190][G eval loss: 0.538464]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.351983][G train loss: 0.656742]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.362882][G eval loss: 0.522019]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.353056][G train loss: 0.631682]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.360614][G eval loss: 0.506975]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.350721][G train loss: 0.608074]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.357303][G eval loss: 0.498862]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.345199][G train loss: 0.593094]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.353295][G eval loss: 0.495972]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.338700][G train loss: 0.582083]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.349089][G eval loss: 0.492185]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.332187][G train loss: 0.567965]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.345005][G eval loss: 0.481401]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.322824][G train loss: 0.555842]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.340823][G eval loss: 0.467364]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.315186][G train loss: 0.539843]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.336415][G eval loss: 0.461981]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.313981][G train loss: 0.520290]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.331679][G eval loss: 0.462074]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.308439][G train loss: 0.521192]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.326693][G eval loss: 0.461731]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.303376][G train loss: 0.523765]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.321536][G eval loss: 0.459167]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.298367][G train loss: 0.525083]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.316241][G eval loss: 0.456851]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.293448][G train loss: 0.527858]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.310953][G eval loss: 0.460938]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.288642][G train loss: 0.535861]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.305694][G eval loss: 0.468967]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.283834][G train loss: 0.545258]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.300138][G eval loss: 0.475186]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.278957][G train loss: 0.550851]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.290771][G eval loss: 0.480226]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.274431][G train loss: 0.553724]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.274576][G eval loss: 0.489236]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.270633][G train loss: 0.558710]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.270873][G eval loss: 0.504661]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.267384][G train loss: 0.567708]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.266810][G eval loss: 0.518092]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.264093][G train loss: 0.577203]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.262109][G eval loss: 0.526474]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.259120][G train loss: 0.583848]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.257739][G eval loss: 0.531429]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.257280][G train loss: 0.589376]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.253492][G eval loss: 0.534990]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.256001][G train loss: 0.595350]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.250067][G eval loss: 0.538944]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.254952][G train loss: 0.602918]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.249229][G eval loss: 0.544068]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.254007][G train loss: 0.611320]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.249526][G eval loss: 0.550909]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.253316][G train loss: 0.619638]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.250254][G eval loss: 0.558545]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.252855][G train loss: 0.626910]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.251081][G eval loss: 0.565810]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.252759][G train loss: 0.632248]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.251950][G eval loss: 0.572678]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.252910][G train loss: 0.636957]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.252872][G eval loss: 0.581971]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.253129][G train loss: 0.644833]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.253714][G eval loss: 0.589705]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.254881][G train loss: 0.647207]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.254435][G eval loss: 0.596309]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.255035][G train loss: 0.652708]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.255141][G eval loss: 0.599272]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.254403][G train loss: 0.658660]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.255689][G eval loss: 0.596940]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.255175][G train loss: 0.656519]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.256132][G eval loss: 0.592571]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.252726][G train loss: 0.663011]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.256237][G eval loss: 0.586000]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.253256][G train loss: 0.657699]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.256265][G eval loss: 0.578952]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.252568][G train loss: 0.655040]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.256552][G eval loss: 0.569630]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.251354][G train loss: 0.651524]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.252972][G eval loss: 0.570066]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250765][G train loss: 0.644244]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.251708][G eval loss: 0.563739]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250546][G train loss: 0.634727]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.250852][G eval loss: 0.556152]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.249671][G train loss: 0.625725]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.250504][G eval loss: 0.547338]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.246947][G train loss: 0.622378]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.249346][G eval loss: 0.541849]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.245962][G train loss: 0.613731]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.248778][G eval loss: 0.534225]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.245791][G train loss: 0.603184]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.248886][G eval loss: 0.528403]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.243865][G train loss: 0.602756]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.247058][G eval loss: 0.530400]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.241951][G train loss: 0.601022]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.246288][G eval loss: 0.527495]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.241605][G train loss: 0.592549]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.245741][G eval loss: 0.527165]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.241492][G train loss: 0.586923]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.245373][G eval loss: 0.525885]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.241383][G train loss: 0.582716]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.245190][G eval loss: 0.524795]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.242060][G train loss: 0.577906]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.245014][G eval loss: 0.519925]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.244248][G train loss: 0.566747]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.244955][G eval loss: 0.515791]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.244877][G train loss: 0.562249]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.250555][G eval loss: 0.499239]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.246474][G train loss: 0.560405]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.250881][G eval loss: 0.501854]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.249945][G train loss: 0.557466]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.251077][G eval loss: 0.507926]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.0045 lr_d=0.0012 -> score=0.759003\n",
      "\n",
      "----- 0050: lr_g=0.0045, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250054][G eval loss: 1.111385]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250053][G train loss: 1.263060]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 1.106092]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.257653]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 1.110141]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.261685]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.110420]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249979][G train loss: 1.262048]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 1.109236]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249951][G train loss: 1.260928]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249906][G eval loss: 1.108742]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249879][G train loss: 1.260460]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249816][G eval loss: 1.108468]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249773][G train loss: 1.260185]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249673][G eval loss: 1.107483]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249606][G train loss: 1.259173]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249432][G eval loss: 1.105788]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249330][G train loss: 1.257436]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249050][G eval loss: 1.103944]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248886][G train loss: 1.255537]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248399][G eval loss: 1.102230]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248101][G train loss: 1.253767]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247191][G eval loss: 1.101002]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246785][G train loss: 1.252503]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.245655][G eval loss: 1.100189]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245122][G train loss: 1.251673]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.243506][G eval loss: 1.100807]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242796][G train loss: 1.252259]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.240547][G eval loss: 1.103019]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239581][G train loss: 1.254404]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.236832][G eval loss: 1.106209]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235528][G train loss: 1.257495]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.232009][G eval loss: 1.110427]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.230325][G train loss: 1.261582]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.225696][G eval loss: 1.115567]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223693][G train loss: 1.266526]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.218181][G eval loss: 1.119601]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.216007][G train loss: 1.270246]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.208673][G eval loss: 1.122887]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.206638][G train loss: 1.273206]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.196938][G eval loss: 1.124196]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.195390][G train loss: 1.273899]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.182916][G eval loss: 1.123767]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.181970][G train loss: 1.272474]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.167319][G eval loss: 1.119752]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.166929][G train loss: 1.266724]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.151169][G eval loss: 1.108847]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.152679][G train loss: 1.250508]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.151981][G eval loss: 1.060843]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.165501][G train loss: 1.183380]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.198606][G eval loss: 0.959269]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.194739][G train loss: 1.107455]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.223827][G eval loss: 0.925710]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.223964][G train loss: 1.055226]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.229682][G eval loss: 0.943775]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.251621][G train loss: 1.028628]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.233715][G eval loss: 1.004132]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.0045 lr_d=0.0015 -> score=1.237847\n",
      "\n",
      "----- 0050: lr_g=0.005, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.115805]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.267481]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.106955]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.258495]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 1.106259]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 1.257826]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.104640]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.256299]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.104974]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.256692]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 1.106197]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249960][G train loss: 1.257936]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249957][G eval loss: 1.106663]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249946][G train loss: 1.258388]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 1.106144]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249927][G train loss: 1.257828]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249921][G eval loss: 1.104792]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249904][G train loss: 1.256410]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249898][G eval loss: 1.103150]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249876][G train loss: 1.254706]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249868][G eval loss: 1.101682]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249841][G train loss: 1.253182]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 1.100631]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249799][G train loss: 1.252086]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249785][G eval loss: 1.100009]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249744][G train loss: 1.251434]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249723][G eval loss: 1.099672]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249675][G train loss: 1.251066]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249645][G eval loss: 1.099494]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249587][G train loss: 1.250830]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249560][G eval loss: 1.099350]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249491][G train loss: 1.250582]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249447][G eval loss: 1.098595]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249368][G train loss: 1.249710]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249325][G eval loss: 1.096653]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249235][G train loss: 1.247653]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249190][G eval loss: 1.093080]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249092][G train loss: 1.243986]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249035][G eval loss: 1.086301]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248935][G train loss: 1.236947]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248869][G eval loss: 1.074322]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248778][G train loss: 1.224371]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248689][G eval loss: 1.056984]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248623][G train loss: 1.206454]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248566][G eval loss: 1.033288]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248534][G train loss: 1.182591]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248603][G eval loss: 1.006348]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.248578][G train loss: 1.156334]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.248767][G eval loss: 0.983610]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.248725][G train loss: 1.134441]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.248871][G eval loss: 0.965219]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.248780][G train loss: 1.116383]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.248750][G eval loss: 0.940096]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.248628][G train loss: 1.092832]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.248369][G eval loss: 0.906702]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.248297][G train loss: 1.060282]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.247552][G eval loss: 0.869206]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.247752][G train loss: 1.021887]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.246672][G eval loss: 0.832687]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.247447][G train loss: 0.981980]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.246485][G eval loss: 0.786665]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.247297][G train loss: 0.935728]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.246387][G eval loss: 0.741605]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.247400][G train loss: 0.884604]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.246275][G eval loss: 0.702802]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.247496][G train loss: 0.835069]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.246273][G eval loss: 0.672558]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.247486][G train loss: 0.792939]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.246080][G eval loss: 0.651728]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.247322][G train loss: 0.755946]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.245829][G eval loss: 0.648066]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.246934][G train loss: 0.731360]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.245432][G eval loss: 0.664639]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.246591][G train loss: 0.725616]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.246704][G eval loss: 0.688705]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.005 lr_d=0.0005 -> score=0.935409\n",
      "\n",
      "----- 0050: lr_g=0.005, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 1.113877]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250009][G train loss: 1.265553]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 1.106676]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249996][G train loss: 1.258215]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 1.107299]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.258866]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 1.106237]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.257896]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 1.106321]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249962][G train loss: 1.258039]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 1.106948]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249940][G train loss: 1.258687]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249924][G eval loss: 1.106742]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249906][G train loss: 1.258468]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249884][G eval loss: 1.105621]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249859][G train loss: 1.257305]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 1.103963]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249798][G train loss: 1.255582]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249764][G eval loss: 1.102221]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249717][G train loss: 1.253778]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249669][G eval loss: 1.100827]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249605][G train loss: 1.252328]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249540][G eval loss: 1.100009]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249455][G train loss: 1.251463]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249371][G eval loss: 1.099672]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249261][G train loss: 1.251095]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249143][G eval loss: 1.099681]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249002][G train loss: 1.251072]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248834][G eval loss: 1.099947]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248657][G train loss: 1.251276]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248434][G eval loss: 1.100375]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248217][G train loss: 1.251589]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247927][G eval loss: 1.100389]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247667][G train loss: 1.251470]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247403][G eval loss: 1.098961]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.247095][G train loss: 1.249910]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246619][G eval loss: 1.095423]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.246286][G train loss: 1.246208]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245733][G eval loss: 1.088351]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.245399][G train loss: 1.238745]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244772][G eval loss: 1.075880]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.244514][G train loss: 1.225428]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243937][G eval loss: 1.057434]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.243803][G train loss: 1.206105]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.243596][G eval loss: 1.031775]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.243662][G train loss: 1.179810]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.244053][G eval loss: 1.003136]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.244012][G train loss: 1.151745]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.244421][G eval loss: 0.981719]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.244310][G train loss: 1.130534]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.244435][G eval loss: 0.965370]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.244061][G train loss: 1.114332]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.243866][G eval loss: 0.940192]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.243190][G train loss: 1.090957]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.242443][G eval loss: 0.905966]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.241497][G train loss: 1.060179]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.239897][G eval loss: 0.870185]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.239097][G train loss: 1.023313]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.235628][G eval loss: 0.833423]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.236740][G train loss: 0.982146]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.230641][G eval loss: 0.795613]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.236888][G train loss: 0.931684]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.229191][G eval loss: 0.750668]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.238325][G train loss: 0.875758]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.229689][G eval loss: 0.710423]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.240396][G train loss: 0.824065]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.233037][G eval loss: 0.670319]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.244390][G train loss: 0.774967]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.235328][G eval loss: 0.639607]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248199][G train loss: 0.730635]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.243433][G eval loss: 0.604702]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.251560][G train loss: 0.690230]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.246706][G eval loss: 0.588842]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.253886][G train loss: 0.658435]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.256922][G eval loss: 0.577610]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.257786][G train loss: 0.641576]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.263957][G eval loss: 0.590192]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.261756][G train loss: 0.638462]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.272188][G eval loss: 0.606549]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.005 lr_d=0.0008 -> score=0.878737\n",
      "\n",
      "----- 0050: lr_g=0.005, lr_d=0.001 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250019][G eval loss: 1.112700]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250018][G train loss: 1.264376]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 1.106501]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.258041]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 1.108013]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.259580]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 1.107297]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.258956]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249959][G eval loss: 1.107249]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249946][G train loss: 1.258967]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249926][G eval loss: 1.107549]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249906][G train loss: 1.259288]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249873][G eval loss: 1.106968]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249845][G train loss: 1.258694]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249796][G eval loss: 1.105684]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249754][G train loss: 1.257369]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249691][G eval loss: 1.104007]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249630][G train loss: 1.255627]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249543][G eval loss: 1.102280]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249458][G train loss: 1.253836]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249342][G eval loss: 1.100904]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249224][G train loss: 1.252404]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249054][G eval loss: 1.100143]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248895][G train loss: 1.251596]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248637][G eval loss: 1.099954]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248431][G train loss: 1.251373]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248055][G eval loss: 1.100335]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247788][G train loss: 1.251717]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247253][G eval loss: 1.101030]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246911][G train loss: 1.252341]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246323][G eval loss: 1.101690]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245896][G train loss: 1.252871]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.244975][G eval loss: 1.101631]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244476][G train loss: 1.252642]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243279][G eval loss: 1.100525]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242737][G train loss: 1.251327]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241323][G eval loss: 1.097803]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.240786][G train loss: 1.248332]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.239207][G eval loss: 1.091447]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.238703][G train loss: 1.241413]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.237172][G eval loss: 1.078868]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.236874][G train loss: 1.227596]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.236162][G eval loss: 1.058105]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.236106][G train loss: 1.205518]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.237261][G eval loss: 1.027958]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.236895][G train loss: 1.175247]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238345][G eval loss: 0.999975]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238511][G train loss: 1.145638]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.239502][G eval loss: 0.983431]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.240059][G train loss: 1.127417]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.240677][G eval loss: 0.969360]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.240269][G train loss: 1.114083]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.239802][G eval loss: 0.946264]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.238922][G train loss: 1.092773]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.237281][G eval loss: 0.912374]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.235474][G train loss: 1.065061]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.232985][G eval loss: 0.878448]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.231363][G train loss: 1.034481]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.229210][G eval loss: 0.845387]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.226547][G train loss: 1.002141]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.227504][G eval loss: 0.805700]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.224958][G train loss: 0.956638]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.227063][G eval loss: 0.764400]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.231840][G train loss: 0.895249]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.226572][G eval loss: 0.734487]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.242232][G train loss: 0.837041]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.230946][G eval loss: 0.708532]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.254091][G train loss: 0.790973]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.235258][G eval loss: 0.682248]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.260872][G train loss: 0.757459]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.242245][G eval loss: 0.649629]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.268408][G train loss: 0.718733]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.252223][G eval loss: 0.603823]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.271440][G train loss: 0.679231]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.261120][G eval loss: 0.556500]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.274550][G train loss: 0.639375]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.269143][G eval loss: 0.527945]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.276678][G train loss: 0.610043]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.275823][G eval loss: 0.530517]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.279087][G train loss: 0.599748]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.282035][G eval loss: 0.543438]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.005 lr_d=0.001 -> score=0.825473\n",
      "\n",
      "----- 0050: lr_g=0.005, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250031][G eval loss: 1.111520]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250029][G train loss: 1.263196]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.106302]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 1.257842]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 1.108690]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.260256]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 1.108330]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249959][G train loss: 1.259988]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249946][G eval loss: 1.108203]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249928][G train loss: 1.259921]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249891][G eval loss: 1.108204]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249863][G train loss: 1.259943]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249808][G eval loss: 1.107301]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249768][G train loss: 1.259027]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249683][G eval loss: 1.106358]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249623][G train loss: 1.258043]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249511][G eval loss: 1.104834]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249418][G train loss: 1.256455]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249257][G eval loss: 1.103185]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249117][G train loss: 1.254741]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248841][G eval loss: 1.101785]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248651][G train loss: 1.253283]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248203][G eval loss: 1.101134]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247958][G train loss: 1.252581]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247364][G eval loss: 1.101345]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.247044][G train loss: 1.252753]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246386][G eval loss: 1.102011]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.005 lr_d=0.0012 -> score=1.348397\n",
      "\n",
      "----- 0050: lr_g=0.005, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.294996]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250055][G eval loss: 1.109746]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250053][G train loss: 1.261422]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 1.105853]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.257392]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 1.109525]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.261091]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 1.109689]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249980][G train loss: 1.261347]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 1.109469]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.261187]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249911][G eval loss: 1.109457]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249885][G train loss: 1.261197]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249821][G eval loss: 1.108666]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249780][G train loss: 1.260391]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249676][G eval loss: 1.107026]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249612][G train loss: 1.258711]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249439][G eval loss: 1.104844]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249342][G train loss: 1.256464]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249068][G eval loss: 1.102823]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248910][G train loss: 1.254379]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248409][G eval loss: 1.101198]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248124][G train loss: 1.252696]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247269][G eval loss: 1.100064]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246886][G train loss: 1.251503]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.245792][G eval loss: 1.100030]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245282][G train loss: 1.251421]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.243592][G eval loss: 1.101481]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242911][G train loss: 1.252815]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.240659][G eval loss: 1.104209]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239708][G train loss: 1.255465]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.237100][G eval loss: 1.107139]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235823][G train loss: 1.258221]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.232190][G eval loss: 1.110747]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.230639][G train loss: 1.261524]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.226039][G eval loss: 1.114299]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.224240][G train loss: 1.264876]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.218522][G eval loss: 1.116535]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.216694][G train loss: 1.266720]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.209550][G eval loss: 1.114233]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.208230][G train loss: 1.263394]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.202219][G eval loss: 1.099133]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.203250][G train loss: 1.243837]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.208474][G eval loss: 1.054413]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.208412][G train loss: 1.200946]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.216538][G eval loss: 1.010204]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.218228][G train loss: 1.151561]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.219248][G eval loss: 0.989047]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.229657][G train loss: 1.114440]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.228578][G eval loss: 0.982075]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.238806][G train loss: 1.104768]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.228677][G eval loss: 1.000702]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.243225][G train loss: 1.109316]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.226447][G eval loss: 1.010504]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.244556][G train loss: 1.107852]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.223459][G eval loss: 0.999985]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.244930][G train loss: 1.091337]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.220385][G eval loss: 0.979653]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.242069][G train loss: 1.070507]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.217384][G eval loss: 0.957734]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.242934][G train loss: 1.041625]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.214275][G eval loss: 0.930541]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.248987][G train loss: 0.994376]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.211219][G eval loss: 0.907502]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.252563][G train loss: 0.964754]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.208284][G eval loss: 0.888481]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.251068][G train loss: 0.945126]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.207491][G eval loss: 0.860019]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.253375][G train loss: 0.914337]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.213882][G eval loss: 0.817764]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.258151][G train loss: 0.875369]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.225129][G eval loss: 0.764571]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.269977][G train loss: 0.821747]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.234322][G eval loss: 0.724797]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.267517][G train loss: 0.792137]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.245600][G eval loss: 0.653777]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.264234][G train loss: 0.760398]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.255154][G eval loss: 0.610920]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.260863][G train loss: 0.723721]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.283350][G eval loss: 0.531887]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.260091][G train loss: 0.690549]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.286255][G eval loss: 0.528009]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.263098][G train loss: 0.662235]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.284525][G eval loss: 0.543668]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.265731][G train loss: 0.647885]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.282023][G eval loss: 0.569438]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0050 lr_g=0.005 lr_d=0.0015 -> score=0.851461\n",
      "\n",
      ">>> Best config for 0050: lr_g=0.004, lr_d=0.0008, score=0.700129\n",
      "\n",
      "========== Grid search for 0056 ==========\n",
      "\n",
      "----- 0056: lr_g=0.003, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.991085]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.093548]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.979720]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.082323]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.975344]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.078163]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.974444]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.077521]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.973673]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.076946]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.972703]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249953][G train loss: 1.076066]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 0.971923]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249936][G train loss: 1.075316]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249938][G eval loss: 0.971522]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249916][G train loss: 1.074898]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 0.971424]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249891][G train loss: 1.074751]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 0.971504]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249861][G train loss: 1.074764]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249866][G eval loss: 0.971627]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249823][G train loss: 1.074811]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.971670]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249776][G train loss: 1.074776]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249781][G eval loss: 0.971568]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249717][G train loss: 1.074604]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249722][G eval loss: 0.971356]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249645][G train loss: 1.074328]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249651][G eval loss: 0.971088]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249559][G train loss: 1.074006]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249569][G eval loss: 0.970849]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249458][G train loss: 1.073727]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249471][G eval loss: 0.970581]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249341][G train loss: 1.073433]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249359][G eval loss: 0.970507]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249205][G train loss: 1.073350]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249225][G eval loss: 0.970606]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249042][G train loss: 1.073451]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249064][G eval loss: 0.970888]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248848][G train loss: 1.073744]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248868][G eval loss: 0.971272]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248615][G train loss: 1.074136]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248631][G eval loss: 0.971720]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248334][G train loss: 1.074592]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248350][G eval loss: 0.972245]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248000][G train loss: 1.075117]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.247994][G eval loss: 0.972843]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247585][G train loss: 1.075704]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247607][G eval loss: 0.973424]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247133][G train loss: 1.076269]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247133][G eval loss: 0.973637]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246598][G train loss: 1.076453]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246591][G eval loss: 0.973649]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.245996][G train loss: 1.076421]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246015][G eval loss: 0.973602]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245352][G train loss: 1.076318]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245373][G eval loss: 0.973625]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244630][G train loss: 1.076274]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.244663][G eval loss: 0.973686]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243829][G train loss: 1.076258]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.243873][G eval loss: 0.973713]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.242944][G train loss: 1.076172]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.242987][G eval loss: 0.973658]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.241960][G train loss: 1.075948]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.241996][G eval loss: 0.973413]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.240872][G train loss: 1.075464]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.240924][G eval loss: 0.972707]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.239717][G train loss: 1.074457]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.239760][G eval loss: 0.971248]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.238503][G train loss: 1.072622]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.238481][G eval loss: 0.968696]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.237243][G train loss: 1.069507]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.237245][G eval loss: 0.964374]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.236136][G train loss: 1.064322]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.236232][G eval loss: 0.957092]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.235478][G train loss: 1.055516]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.235700][G eval loss: 0.945832]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.235612][G train loss: 1.041946]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.235636][G eval loss: 0.931268]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.236978][G train loss: 1.023447]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.236056][G eval loss: 0.915386]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.238725][G train loss: 1.003674]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.236332][G eval loss: 0.902965]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.239824][G train loss: 0.988103]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.236919][G eval loss: 0.896753]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.241193][G train loss: 0.978695]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.237624][G eval loss: 0.898347]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.242456][G train loss: 0.977443]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.237369][G eval loss: 0.903396]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.003 lr_d=0.0005 -> score=1.140764\n",
      "\n",
      "----- 0056: lr_g=0.003, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 0.989159]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250007][G train loss: 1.091622]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.979430]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.082033]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.976365]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249980][G train loss: 1.079184]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.976029]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.079105]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.975024]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249950][G train loss: 1.078297]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249942][G eval loss: 0.973483]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076846]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249909][G eval loss: 0.972058]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249878][G train loss: 1.075450]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249869][G eval loss: 0.971066]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249828][G train loss: 1.074442]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249816][G eval loss: 0.970679]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249757][G train loss: 1.074007]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249741][G eval loss: 0.970663]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249662][G train loss: 1.073922]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249633][G eval loss: 0.970875]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249527][G train loss: 1.074058]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249488][G eval loss: 0.971125]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249346][G train loss: 1.074230]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249303][G eval loss: 0.971280]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249117][G train loss: 1.074313]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249064][G eval loss: 0.971401]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248823][G train loss: 1.074371]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248750][G eval loss: 0.971583]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248445][G train loss: 1.074498]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248342][G eval loss: 0.971971]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247954][G train loss: 1.074844]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247827][G eval loss: 0.972665]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247333][G train loss: 1.075510]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247315][G eval loss: 0.973361]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246692][G train loss: 1.076197]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246492][G eval loss: 0.973552]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245731][G train loss: 1.076387]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245403][G eval loss: 0.973793]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244503][G train loss: 1.076630]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244157][G eval loss: 0.974192]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243126][G train loss: 1.077023]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242626][G eval loss: 0.975155]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241449][G train loss: 1.077980]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.240801][G eval loss: 0.976550]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239441][G train loss: 1.079354]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238678][G eval loss: 0.978315]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237102][G train loss: 1.081095]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236176][G eval loss: 0.980427]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.234369][G train loss: 1.083140]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.233406][G eval loss: 0.982578]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.231295][G train loss: 1.085229]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.230258][G eval loss: 0.984782]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.227783][G train loss: 1.087343]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.226538][G eval loss: 0.987295]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.223696][G train loss: 1.089674]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.222399][G eval loss: 0.989784]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.219096][G train loss: 1.092000]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.217643][G eval loss: 0.992542]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.213928][G train loss: 1.094397]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.212305][G eval loss: 0.995589]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.208086][G train loss: 1.096978]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.206444][G eval loss: 0.998866]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.201705][G train loss: 1.099542]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.200499][G eval loss: 1.001482]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.195117][G train loss: 1.101419]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.195135][G eval loss: 1.002076]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.189785][G train loss: 1.099365]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.190988][G eval loss: 0.999349]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.188863][G train loss: 1.087415]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.190334][G eval loss: 0.988989]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.196839][G train loss: 1.059966]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.193421][G eval loss: 0.970664]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.208031][G train loss: 1.028820]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.203409][G eval loss: 0.940008]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.221938][G train loss: 0.993554]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.215442][G eval loss: 0.910117]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.237717][G train loss: 0.958035]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.234164][G eval loss: 0.876536]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.254583][G train loss: 0.926727]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.247896][G eval loss: 0.860073]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.269263][G train loss: 0.907690]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.265606][G eval loss: 0.845625]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.282322][G train loss: 0.903606]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.283030][G eval loss: 0.846907]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.289544][G train loss: 0.913296]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.290198][G eval loss: 0.862149]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.298135][G train loss: 0.918789]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.294941][G eval loss: 0.876098]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.303889][G train loss: 0.926746]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.297959][G eval loss: 0.882684]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.306577][G train loss: 0.930937]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.300494][G eval loss: 0.879042]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.311520][G train loss: 0.923589]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.292095][G eval loss: 0.876225]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.311834][G train loss: 0.915541]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.279537][G eval loss: 0.884229]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.310389][G train loss: 0.901464]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.274047][G eval loss: 0.871800]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.305055][G train loss: 0.886459]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.267527][G eval loss: 0.859854]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.299733][G train loss: 0.871966]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.260131][G eval loss: 0.850913]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.290624][G train loss: 0.861888]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.252770][G eval loss: 0.847539]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.278755][G train loss: 0.861957]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.247442][G eval loss: 0.847742]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.270143][G train loss: 0.863847]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.244894][G eval loss: 0.846225]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.264596][G train loss: 0.865280]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.244071][G eval loss: 0.842831]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.263840][G train loss: 0.862294]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.245619][G eval loss: 0.834823]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.263107][G train loss: 0.859634]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.250362][G eval loss: 0.822805]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.261470][G train loss: 0.855794]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.264788][G eval loss: 0.798413]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.262919][G train loss: 0.844689]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.263012][G eval loss: 0.797616]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.266429][G train loss: 0.833927]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.262229][G eval loss: 0.795400]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.265196][G train loss: 0.831627]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.262851][G eval loss: 0.792178]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.265240][G train loss: 0.827101]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.263020][G eval loss: 0.788767]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.263733][G train loss: 0.827168]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.261324][G eval loss: 0.788720]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.262613][G train loss: 0.826372]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.259718][G eval loss: 0.790643]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.262793][G train loss: 0.823170]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.258187][G eval loss: 0.795293]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.003 lr_d=0.0008 -> score=1.053480\n",
      "\n",
      "----- 0056: lr_g=0.003, lr_d=0.001 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 0.987982]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250016][G train loss: 1.090445]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.979253]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.081856]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.977072]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.079891]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 0.977088]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249959][G train loss: 1.080164]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 0.975969]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249927][G train loss: 1.079242]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249904][G eval loss: 0.974129]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249871][G train loss: 1.077492]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249845][G eval loss: 0.972354]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249798][G train loss: 1.075746]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249768][G eval loss: 0.971213]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249698][G train loss: 1.074589]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249659][G eval loss: 0.971020]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249559][G train loss: 1.074347]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249503][G eval loss: 0.971178]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249363][G train loss: 1.074436]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249277][G eval loss: 0.971490]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249084][G train loss: 1.074672]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248960][G eval loss: 0.971845]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248695][G train loss: 1.074949]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248536][G eval loss: 0.972196]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248175][G train loss: 1.075227]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247945][G eval loss: 0.972470]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247465][G train loss: 1.075436]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247149][G eval loss: 0.972948]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246516][G train loss: 1.075858]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246263][G eval loss: 0.973393]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245441][G train loss: 1.076258]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.244925][G eval loss: 0.973822]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.243907][G train loss: 1.076654]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243167][G eval loss: 0.974644]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.241943][G train loss: 1.077458]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241010][G eval loss: 0.975953]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239528][G train loss: 1.078758]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238331][G eval loss: 0.977837]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236532][G train loss: 1.080637]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235099][G eval loss: 0.980242]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.232910][G train loss: 1.083046]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.231359][G eval loss: 0.982765]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.228724][G train loss: 1.085549]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.226830][G eval loss: 0.985921]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.223715][G train loss: 1.088647]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.221514][G eval loss: 0.989733]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.217831][G train loss: 1.092397]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.215352][G eval loss: 0.994265]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.210951][G train loss: 1.096849]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.208428][G eval loss: 0.999293]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.203184][G train loss: 1.101715]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.200439][G eval loss: 1.005380]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.194302][G train loss: 1.107567]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.191691][G eval loss: 1.011954]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.184547][G train loss: 1.113931]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.181982][G eval loss: 1.019278]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.173994][G train loss: 1.120739]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.171225][G eval loss: 1.028113]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.162675][G train loss: 1.128584]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.159849][G eval loss: 1.038727]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.151149][G train loss: 1.137336]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.149512][G eval loss: 1.048778]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.141218][G train loss: 1.144020]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.146757][G eval loss: 1.047834]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.144233][G train loss: 1.126400]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.155321][G eval loss: 1.030281]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.180978][G train loss: 1.065420]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.185242][G eval loss: 0.984704]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.220297][G train loss: 1.012818]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.218766][G eval loss: 0.939200]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.255748][G train loss: 0.969962]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.246660][G eval loss: 0.908788]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.283412][G train loss: 0.937419]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.287492][G eval loss: 0.850209]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.300300][G train loss: 0.916584]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.307848][G eval loss: 0.832852]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.317511][G train loss: 0.895329]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.330172][G eval loss: 0.820699]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.338009][G train loss: 0.875725]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.355199][G eval loss: 0.823585]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.352608][G train loss: 0.882435]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.382519][G eval loss: 0.833778]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.367337][G train loss: 0.888078]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.392534][G eval loss: 0.860368]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.373926][G train loss: 0.911993]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.397679][G eval loss: 0.879785]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.375000][G train loss: 0.931962]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.399839][G eval loss: 0.889064]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.375209][G train loss: 0.941642]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.400702][G eval loss: 0.887819]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.374817][G train loss: 0.941122]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.400591][G eval loss: 0.877890]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.373587][G train loss: 0.932100]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.399711][G eval loss: 0.862049]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.371253][G train loss: 0.917094]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.398198][G eval loss: 0.843845]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.367914][G train loss: 0.899675]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.396122][G eval loss: 0.825905]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.363928][G train loss: 0.882303]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.393280][G eval loss: 0.809659]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.358057][G train loss: 0.867157]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.370601][G eval loss: 0.821739]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.349260][G train loss: 0.861609]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.363179][G eval loss: 0.828357]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.003 lr_d=0.001 -> score=1.191536\n",
      "\n",
      "----- 0056: lr_g=0.003, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 0.986802]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250027][G train loss: 1.089265]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.979053]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.081656]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.977745]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.080564]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.978120]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249948][G train loss: 1.081197]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249926][G eval loss: 0.976948]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249897][G train loss: 1.080221]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 0.974874]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249812][G train loss: 1.078237]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249759][G eval loss: 0.972802]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249692][G train loss: 1.076195]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249628][G eval loss: 0.972054]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249522][G train loss: 1.075429]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249446][G eval loss: 0.971769]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249294][G train loss: 1.075095]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249161][G eval loss: 0.971826]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248936][G train loss: 1.075084]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248722][G eval loss: 0.972009]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248393][G train loss: 1.075190]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248070][G eval loss: 0.972360]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.003 lr_d=0.0012 -> score=1.220429\n",
      "\n",
      "----- 0056: lr_g=0.003, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250054][G eval loss: 0.985029]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250051][G train loss: 1.087492]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.978625]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249986][G train loss: 1.081228]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.978626]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249963][G train loss: 1.081444]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.979522]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.082598]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249952][G eval loss: 0.978220]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249927][G train loss: 1.081493]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249865][G eval loss: 0.976333]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249820][G train loss: 1.079696]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249756][G eval loss: 0.974554]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249689][G train loss: 1.077946]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249589][G eval loss: 0.973728]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249478][G train loss: 1.077104]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249324][G eval loss: 0.973433]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249151][G train loss: 1.076759]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248904][G eval loss: 0.973651]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248634][G train loss: 1.076909]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248269][G eval loss: 0.974305]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.247858][G train loss: 1.077485]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247329][G eval loss: 0.975428]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246705][G train loss: 1.078529]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.245886][G eval loss: 0.977089]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.244957][G train loss: 1.080115]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.243803][G eval loss: 0.979537]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242423][G train loss: 1.082492]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.241184][G eval loss: 0.982275]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239174][G train loss: 1.085165]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.237731][G eval loss: 0.985658]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.234832][G train loss: 1.088495]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.233109][G eval loss: 0.989480]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.229157][G train loss: 1.092278]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.227093][G eval loss: 0.993840]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.221995][G train loss: 1.096616]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.218835][G eval loss: 0.999966]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.212900][G train loss: 1.102714]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.208497][G eval loss: 1.007738]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.202014][G train loss: 1.110477]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.196509][G eval loss: 1.017051]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.189379][G train loss: 1.119763]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.182818][G eval loss: 1.029242]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.174764][G train loss: 1.131885]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.167322][G eval loss: 1.045403]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.158321][G train loss: 1.147889]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.150776][G eval loss: 1.065930]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.140762][G train loss: 1.168218]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.134394][G eval loss: 1.090009]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.123430][G train loss: 1.192000]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.119491][G eval loss: 1.116277]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.107460][G train loss: 1.217839]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.106451][G eval loss: 1.143723]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.093046][G train loss: 1.244632]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.095568][G eval loss: 1.170799]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.080702][G train loss: 1.269997]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.089558][G eval loss: 1.187703]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.073649][G train loss: 1.282450]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.132246][G eval loss: 1.122252]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.184510][G train loss: 1.107798]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.239397][G eval loss: 0.985303]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.270141][G train loss: 1.011541]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.285966][G eval loss: 0.937477]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.314777][G train loss: 0.967328]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.335774][G eval loss: 0.874050]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.334885][G train loss: 0.953673]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.361392][G eval loss: 0.857973]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.368663][G train loss: 0.902062]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.387767][G eval loss: 0.816646]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.393657][G train loss: 0.862226]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.421483][G eval loss: 0.793675]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.403645][G train loss: 0.854445]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.456432][G eval loss: 0.753675]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.408029][G train loss: 0.854696]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.470938][G eval loss: 0.758137]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.409439][G train loss: 0.859129]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.486661][G eval loss: 0.765232]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.408926][G train loss: 0.866278]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.486818][G eval loss: 0.772280]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.407213][G train loss: 0.873644]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.485470][G eval loss: 0.777015]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.404444][G train loss: 0.878839]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.482982][G eval loss: 0.778345]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.400483][G train loss: 0.880658]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.479434][G eval loss: 0.776352]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.395418][G train loss: 0.879118]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.474872][G eval loss: 0.771857]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.389615][G train loss: 0.875007]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.469340][G eval loss: 0.766006]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.389840][G train loss: 0.869436]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.462983][G eval loss: 0.760129]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.383028][G train loss: 0.863733]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.455670][G eval loss: 0.755811]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.374923][G train loss: 0.859500]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.447186][G eval loss: 0.754327]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.363622][G train loss: 0.858018]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.438382][G eval loss: 0.756151]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.350076][G train loss: 0.859811]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.429001][G eval loss: 0.761569]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.340072][G train loss: 0.865144]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.408929][G eval loss: 0.779232]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.323246][G train loss: 0.873309]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.389809][G eval loss: 0.812565]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.313603][G train loss: 0.882941]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.378454][G eval loss: 0.835866]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.304132][G train loss: 0.893526]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.368970][G eval loss: 0.851831]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.294981][G train loss: 0.904691]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.359979][G eval loss: 0.867659]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.286415][G train loss: 0.916026]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.352677][G eval loss: 0.879731]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.279328][G train loss: 0.925914]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.346843][G eval loss: 0.888328]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.274032][G train loss: 0.933364]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.341416][G eval loss: 0.895632]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.264812][G train loss: 0.955237]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.336536][G eval loss: 0.901892]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.260698][G train loss: 0.961355]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.332295][G eval loss: 0.907168]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.257137][G train loss: 0.967053]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.328373][G eval loss: 0.912248]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.251180][G train loss: 0.981758]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.324372][G eval loss: 0.919187]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.248755][G train loss: 0.990055]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.313986][G eval loss: 0.944766]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.247110][G train loss: 0.996670]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.293423][G eval loss: 0.969518]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.246026][G train loss: 1.004144]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.277643][G eval loss: 0.984092]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.245673][G train loss: 1.012525]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.277476][G eval loss: 0.995029]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.245755][G train loss: 1.022100]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.279011][G eval loss: 1.002912]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.246594][G train loss: 1.031631]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.279903][G eval loss: 1.012066]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.247792][G train loss: 1.040853]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.280721][G eval loss: 1.022892]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.240506][G train loss: 1.088303]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.281545][G eval loss: 1.033492]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.240495][G train loss: 1.103320]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.282290][G eval loss: 1.041491]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.240927][G train loss: 1.118109]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.282977][G eval loss: 1.047802]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.242047][G train loss: 1.125296]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.284783][G eval loss: 1.052418]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.243023][G train loss: 1.128329]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.285241][G eval loss: 1.054879]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.244018][G train loss: 1.127588]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.285568][G eval loss: 1.055535]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.248722][G train loss: 1.105522]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.285842][G eval loss: 1.053590]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.099948]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.286072][G eval loss: 1.051090]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.249662][G train loss: 1.097247]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.286337][G eval loss: 1.048493]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.249546][G train loss: 1.092551]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.286376][G eval loss: 1.044192]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.249275][G train loss: 1.087195]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.286090][G eval loss: 1.039451]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250596][G train loss: 1.073956]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.285721][G eval loss: 1.036720]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.250205][G train loss: 1.068400]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.285397][G eval loss: 1.033163]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.249636][G train loss: 1.062907]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.285096][G eval loss: 1.026689]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.248939][G train loss: 1.056407]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.284930][G eval loss: 1.022262]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.246395][G train loss: 1.060968]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.284669][G eval loss: 1.016621]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.245640][G train loss: 1.055169]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.283700][G eval loss: 1.010474]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.244867][G train loss: 1.047538]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.283163][G eval loss: 1.002118]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.244073][G train loss: 1.038653]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.279129][G eval loss: 0.993222]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.243234][G train loss: 1.028396]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.277555][G eval loss: 0.983708]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.242366][G train loss: 1.016215]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.267642][G eval loss: 0.974997]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.242010][G train loss: 0.998232]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.264784][G eval loss: 0.964362]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.243429][G train loss: 0.978197]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.268534][G eval loss: 0.899510]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.242625][G train loss: 0.968498]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.268309][G eval loss: 0.887411]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.241721][G train loss: 0.957321]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.267850][G eval loss: 0.873066]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.239214][G train loss: 0.952912]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.266023][G eval loss: 0.858416]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.238272][G train loss: 0.946331]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.259859][G eval loss: 0.864148]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.237787][G train loss: 0.934576]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.256702][G eval loss: 0.875689]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.003 lr_d=0.0015 -> score=1.132391\n",
      "\n",
      "----- 0056: lr_g=0.0035, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.988836]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.091307]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.977466]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.080124]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.974375]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.077330]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.973579]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.076810]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.972866]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.076246]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 0.972367]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249954][G train loss: 1.075790]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249954][G eval loss: 0.972201]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249938][G train loss: 1.075598]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 0.972243]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249918][G train loss: 1.075573]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.972309]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.0035 lr_d=0.0005 -> score=1.222228\n",
      "\n",
      "----- 0056: lr_g=0.0035, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.986909]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.089380]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.977177]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.079836]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.975399]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249980][G train loss: 1.078353]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.975168]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.078398]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 0.974222]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249952][G train loss: 1.077602]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249944][G eval loss: 0.973151]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249922][G train loss: 1.076574]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249914][G eval loss: 0.972334]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249883][G train loss: 1.075731]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249874][G eval loss: 0.971789]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249832][G train loss: 1.075118]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249818][G eval loss: 0.971561]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249761][G train loss: 1.074801]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249740][G eval loss: 0.971368]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249662][G train loss: 1.074513]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249632][G eval loss: 0.971082]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249528][G train loss: 1.074138]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249496][G eval loss: 0.970780]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249357][G train loss: 1.073757]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249318][G eval loss: 0.970363]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249137][G train loss: 1.073272]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249088][G eval loss: 0.970017]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248855][G train loss: 1.072879]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248786][G eval loss: 0.969985]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248488][G train loss: 1.072816]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248391][G eval loss: 0.970395]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248013][G train loss: 1.073214]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247903][G eval loss: 0.971222]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247424][G train loss: 1.074040]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247330][G eval loss: 0.972108]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246729][G train loss: 1.074933]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246506][G eval loss: 0.972501]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245777][G train loss: 1.075324]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245424][G eval loss: 0.972903]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244572][G train loss: 1.075702]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244135][G eval loss: 0.973648]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243161][G train loss: 1.076407]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242596][G eval loss: 0.974824]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241488][G train loss: 1.077538]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.240826][G eval loss: 0.976126]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239545][G train loss: 1.078782]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238759][G eval loss: 0.977500]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237278][G train loss: 1.080071]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236342][G eval loss: 0.978998]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.234649][G train loss: 1.081402]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.233679][G eval loss: 0.980309]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.231725][G train loss: 1.082522]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.230700][G eval loss: 0.981415]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.228477][G train loss: 1.083294]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.227385][G eval loss: 0.982272]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.224963][G train loss: 1.083493]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.223853][G eval loss: 0.982517]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.221372][G train loss: 1.082582]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.220432][G eval loss: 0.980772]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.218350][G train loss: 1.078531]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.217938][G eval loss: 0.975195]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.217112][G train loss: 1.068421]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.217015][G eval loss: 0.964092]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.219505][G train loss: 1.048713]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.218022][G eval loss: 0.946238]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.223822][G train loss: 1.022851]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.220704][G eval loss: 0.921919]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.228521][G train loss: 0.993726]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.227246][G eval loss: 0.893502]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.235345][G train loss: 0.963556]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.237430][G eval loss: 0.870380]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.242497][G train loss: 0.943587]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.243108][G eval loss: 0.867963]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.248679][G train loss: 0.938055]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.246755][G eval loss: 0.874719]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.256068][G train loss: 0.936713]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.248773][G eval loss: 0.882564]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.259794][G train loss: 0.941528]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.248967][G eval loss: 0.886162]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.261114][G train loss: 0.943369]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.247426][G eval loss: 0.884939]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.261098][G train loss: 0.939641]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.233298][G eval loss: 0.896985]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.260339][G train loss: 0.930173]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.229468][G eval loss: 0.889277]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.258536][G train loss: 0.917304]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.227202][G eval loss: 0.875400]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.251343][G train loss: 0.909574]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.226253][G eval loss: 0.860363]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.250658][G train loss: 0.890304]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.229709][G eval loss: 0.835125]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.250287][G train loss: 0.871333]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.235969][G eval loss: 0.810637]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.248139][G train loss: 0.855427]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.240214][G eval loss: 0.788129]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.247255][G train loss: 0.837876]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.246465][G eval loss: 0.769060]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.247532][G train loss: 0.821400]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.246923][G eval loss: 0.760564]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.245134][G train loss: 0.813434]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.248724][G eval loss: 0.750197]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.251349][G train loss: 0.792920]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.258584][G eval loss: 0.736015]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.255332][G train loss: 0.783645]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.261107][G eval loss: 0.736048]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.255546][G train loss: 0.784104]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.261690][G eval loss: 0.736775]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.256655][G train loss: 0.781090]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.261920][G eval loss: 0.737032]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.257709][G train loss: 0.777161]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.262214][G eval loss: 0.734550]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.257272][G train loss: 0.773231]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.262347][G eval loss: 0.730293]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.256499][G train loss: 0.767478]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.262172][G eval loss: 0.723192]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.256048][G train loss: 0.757684]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.262103][G eval loss: 0.714574]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.255606][G train loss: 0.745102]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.262935][G eval loss: 0.701392]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.254202][G train loss: 0.730948]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.264413][G eval loss: 0.683874]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.252468][G train loss: 0.714113]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.265777][G eval loss: 0.666309]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.250591][G train loss: 0.695800]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.267207][G eval loss: 0.645542]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.248613][G train loss: 0.676749]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.268649][G eval loss: 0.625915]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.246464][G train loss: 0.659082]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.269871][G eval loss: 0.610888]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.247455][G train loss: 0.640862]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.270639][G eval loss: 0.601569]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.247772][G train loss: 0.630334]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.271389][G eval loss: 0.596994]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.247355][G train loss: 0.625542]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.272073][G eval loss: 0.593678]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.249690][G train loss: 0.620235]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.272588][G eval loss: 0.591549]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.251156][G train loss: 0.619851]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.272882][G eval loss: 0.589125]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.252698][G train loss: 0.617498]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.273113][G eval loss: 0.591935]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.254755][G train loss: 0.614407]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.285168][G eval loss: 0.550748]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.254442][G train loss: 0.614619]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.285995][G eval loss: 0.545079]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.254380][G train loss: 0.612341]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.288291][G eval loss: 0.529908]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.254293][G train loss: 0.604350]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.295282][G eval loss: 0.511640]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.256545][G train loss: 0.587723]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.294882][G eval loss: 0.507253]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.255978][G train loss: 0.578759]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.294415][G eval loss: 0.496543]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.255021][G train loss: 0.566601]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.293797][G eval loss: 0.485997]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.253661][G train loss: 0.556085]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.292914][G eval loss: 0.476573]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.252637][G train loss: 0.547373]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.291749][G eval loss: 0.465558]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.252228][G train loss: 0.539912]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.290232][G eval loss: 0.457522]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.251753][G train loss: 0.532533]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.288301][G eval loss: 0.453125]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.251238][G train loss: 0.527190]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.285962][G eval loss: 0.447684]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.250785][G train loss: 0.521901]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.283608][G eval loss: 0.444994]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.250484][G train loss: 0.517356]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.281341][G eval loss: 0.442262]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.250309][G train loss: 0.513826]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.279077][G eval loss: 0.439004]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.250216][G train loss: 0.511029]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.277132][G eval loss: 0.436957]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.250244][G train loss: 0.508443]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.275671][G eval loss: 0.434126]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.250212][G train loss: 0.506411]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.274632][G eval loss: 0.432231]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.249935][G train loss: 0.506595]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.273877][G eval loss: 0.433841]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.250148][G train loss: 0.507558]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.273470][G eval loss: 0.437354]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.250265][G train loss: 0.511301]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.271943][G eval loss: 0.440999]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.250249][G train loss: 0.513973]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.267326][G eval loss: 0.443380]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.250253][G train loss: 0.515541]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.259755][G eval loss: 0.444324]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.250396][G train loss: 0.515086]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.251080][G eval loss: 0.444275]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.250567][G train loss: 0.512915]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.250587][G eval loss: 0.444097]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.250622][G train loss: 0.512015]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.250538][G eval loss: 0.444362]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.250535][G train loss: 0.512428]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.250389][G eval loss: 0.446000]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.250366][G train loss: 0.513792]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.250308][G eval loss: 0.448063]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.250247][G train loss: 0.515138]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.250292][G eval loss: 0.449776]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.250228][G train loss: 0.515987]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.250291][G eval loss: 0.450844]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.250229][G train loss: 0.516152]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.250271][G eval loss: 0.451256]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.250218][G train loss: 0.515404]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.250247][G eval loss: 0.451451]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.250203][G train loss: 0.513910]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.250195][G eval loss: 0.451591]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.0035 lr_d=0.0008 -> score=0.701786\n",
      "\n",
      "----- 0056: lr_g=0.0035, lr_d=0.001 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250019][G eval loss: 0.985732]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250016][G train loss: 1.088203]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.977000]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 1.079659]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.976107]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.079062]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.976228]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249960][G train loss: 1.079458]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249951][G eval loss: 0.975164]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249929][G train loss: 1.078544]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249910][G eval loss: 0.973783]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249878][G train loss: 1.077206]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249854][G eval loss: 0.972613]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249807][G train loss: 1.076010]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249775][G eval loss: 0.971911]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249707][G train loss: 1.075241]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249661][G eval loss: 0.971839]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249563][G train loss: 1.075079]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249503][G eval loss: 0.971784]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249366][G train loss: 1.074927]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249287][G eval loss: 0.971567]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249098][G train loss: 1.074622]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248980][G eval loss: 0.971283]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248722][G train loss: 1.074258]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248561][G eval loss: 0.971045]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248210][G train loss: 1.073952]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247983][G eval loss: 0.971020]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247517][G train loss: 1.073877]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247201][G eval loss: 0.971379]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246587][G train loss: 1.074201]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246343][G eval loss: 0.971987]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245547][G train loss: 1.074794]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245023][G eval loss: 0.972775]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244043][G train loss: 1.075572]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243267][G eval loss: 0.973957]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242095][G train loss: 1.076742]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241071][G eval loss: 0.975546]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239658][G train loss: 1.078321]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238325][G eval loss: 0.977594]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236619][G train loss: 1.080332]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235074][G eval loss: 0.979995]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.233018][G train loss: 1.082707]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.231302][G eval loss: 0.982552]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.228864][G train loss: 1.085178]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.226788][G eval loss: 0.985563]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.223942][G train loss: 1.088031]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.221553][G eval loss: 0.988875]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.218227][G train loss: 1.091147]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.215589][G eval loss: 0.992563]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.211689][G train loss: 1.094521]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.208893][G eval loss: 0.996682]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.204301][G train loss: 1.098173]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.201434][G eval loss: 1.001330]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.196211][G train loss: 1.101916]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.193732][G eval loss: 1.005576]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.187942][G train loss: 1.104715]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.187100][G eval loss: 1.006178]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.182442][G train loss: 1.100035]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.183827][G eval loss: 0.999932]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.188264][G train loss: 1.074054]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.186873][G eval loss: 0.981942]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.202903][G train loss: 1.037689]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.199939][G eval loss: 0.947638]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.221141][G train loss: 0.997467]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.218691][G eval loss: 0.907980]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.240724][G train loss: 0.958030]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.238248][G eval loss: 0.876698]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.260784][G train loss: 0.922453]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.251907][G eval loss: 0.857488]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.277066][G train loss: 0.900613]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.279465][G eval loss: 0.833052]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.288100][G train loss: 0.898665]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.291336][G eval loss: 0.843073]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.297544][G train loss: 0.900959]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.300989][G eval loss: 0.863380]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.306895][G train loss: 0.913699]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.310821][G eval loss: 0.874779]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.313894][G train loss: 0.921325]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.318077][G eval loss: 0.876668]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.314920][G train loss: 0.927803]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.319232][G eval loss: 0.871845]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.313936][G train loss: 0.924144]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.301542][G eval loss: 0.888229]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.311971][G train loss: 0.910953]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.299092][G eval loss: 0.870709]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.305961][G train loss: 0.894773]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.292819][G eval loss: 0.853839]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.300703][G train loss: 0.877821]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.282264][G eval loss: 0.841670]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.294279][G train loss: 0.861643]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.270568][G eval loss: 0.833946]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.285777][G train loss: 0.851265]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.259610][G eval loss: 0.832682]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.271930][G train loss: 0.851913]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.252660][G eval loss: 0.832801]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.259052][G train loss: 0.861905]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.248784][G eval loss: 0.830377]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.254124][G train loss: 0.864096]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.249591][G eval loss: 0.819566]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.254128][G train loss: 0.856817]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.248181][G eval loss: 0.814560]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.252804][G train loss: 0.850390]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.260624][G eval loss: 0.786132]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.254562][G train loss: 0.839562]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.257927][G eval loss: 0.783967]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.257835][G train loss: 0.826914]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.256061][G eval loss: 0.783586]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.255945][G train loss: 0.823062]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.255255][G eval loss: 0.783076]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.254354][G train loss: 0.820913]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.255050][G eval loss: 0.784827]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.254278][G train loss: 0.818148]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.254620][G eval loss: 0.789615]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.0035 lr_d=0.001 -> score=1.044235\n",
      "\n",
      "----- 0056: lr_g=0.0035, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250031][G eval loss: 0.984553]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250028][G train loss: 1.087024]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.976800]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.079459]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.976781]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.079735]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 0.977260]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249949][G train loss: 1.080490]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249930][G eval loss: 0.976139]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249901][G train loss: 1.079520]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249868][G eval loss: 0.974476]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249823][G train loss: 1.077899]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249780][G eval loss: 0.972990]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249716][G train loss: 1.076387]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249654][G eval loss: 0.972700]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249558][G train loss: 1.076029]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249474][G eval loss: 0.972678]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249329][G train loss: 1.075917]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249195][G eval loss: 0.972646]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248976][G train loss: 1.075789]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248744][G eval loss: 0.972410]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248437][G train loss: 1.075464]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248090][G eval loss: 0.972183]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247676][G train loss: 1.075156]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247229][G eval loss: 0.972257]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246666][G train loss: 1.075159]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246275][G eval loss: 0.972594]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245509][G train loss: 1.075443]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244810][G eval loss: 0.973221]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243798][G train loss: 1.076031]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242843][G eval loss: 0.974424]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241540][G train loss: 1.077210]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240278][G eval loss: 0.976236]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238623][G train loss: 1.079005]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.237022][G eval loss: 0.978578]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.234924][G train loss: 1.081345]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232971][G eval loss: 0.981174]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230361][G train loss: 1.083903]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227709][G eval loss: 0.984910]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.224564][G train loss: 1.087581]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221625][G eval loss: 0.989065]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.217883][G train loss: 1.091686]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.214578][G eval loss: 0.993834]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.210112][G train loss: 1.096345]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.206294][G eval loss: 0.999729]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.200965][G train loss: 1.102037]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.196718][G eval loss: 1.006826]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.190515][G train loss: 1.108844]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.186050][G eval loss: 1.014850]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.178963][G train loss: 1.116408]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.174148][G eval loss: 1.024288]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.166370][G train loss: 1.124852]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.161301][G eval loss: 1.035919]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.153261][G train loss: 1.134489]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.149675][G eval loss: 1.046836]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.141829][G train loss: 1.141923]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.147974][G eval loss: 1.042859]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.153192][G train loss: 1.107720]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.165823][G eval loss: 1.007535]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.197594][G train loss: 1.039615]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.197599][G eval loss: 0.966039]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.239849][G train loss: 0.979917]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.223609][G eval loss: 0.925939]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.275003][G train loss: 0.938599]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.274110][G eval loss: 0.853352]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.294680][G train loss: 0.911987]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.301406][G eval loss: 0.827397]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.314853][G train loss: 0.884448]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.322443][G eval loss: 0.816638]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.333496][G train loss: 0.868594]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.345398][G eval loss: 0.821951]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.347989][G train loss: 0.877106]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.377096][G eval loss: 0.833074]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.358604][G train loss: 0.889020]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.388621][G eval loss: 0.855449]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.369757][G train loss: 0.900966]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.393730][G eval loss: 0.866465]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.370274][G train loss: 0.913434]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.396176][G eval loss: 0.864630]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.369667][G train loss: 0.914256]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.386547][G eval loss: 0.859820]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.368065][G train loss: 0.903939]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.371131][G eval loss: 0.871033]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.360254][G train loss: 0.891868]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.367855][G eval loss: 0.852918]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.356190][G train loss: 0.870564]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.364323][G eval loss: 0.830746]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.351589][G train loss: 0.845971]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.359749][G eval loss: 0.809768]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.346031][G train loss: 0.823053]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.353149][G eval loss: 0.794297]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.337724][G train loss: 0.806141]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.344227][G eval loss: 0.787960]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.322621][G train loss: 0.804322]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.334853][G eval loss: 0.791236]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.310759][G train loss: 0.809397]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.322513][G eval loss: 0.799995]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.297329][G train loss: 0.818195]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.300616][G eval loss: 0.806148]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.284009][G train loss: 0.833729]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.291412][G eval loss: 0.813063]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.268614][G train loss: 0.858935]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.285974][G eval loss: 0.819472]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.264685][G train loss: 0.852683]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.280830][G eval loss: 0.826213]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.265327][G train loss: 0.849008]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.275115][G eval loss: 0.830208]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.0035 lr_d=0.0012 -> score=1.105322\n",
      "\n",
      "----- 0056: lr_g=0.0035, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250054][G eval loss: 0.982779]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250051][G train loss: 1.085250]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.976369]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249987][G train loss: 1.079028]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.977652]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249963][G train loss: 1.080606]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.978653]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.081883]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.977410]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249931][G train loss: 1.080790]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249875][G eval loss: 0.975991]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249832][G train loss: 1.079414]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249768][G eval loss: 0.974825]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249703][G train loss: 1.078222]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249601][G eval loss: 0.974415]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249494][G train loss: 1.077744]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249338][G eval loss: 0.974247]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249169][G train loss: 1.077486]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248927][G eval loss: 0.974216]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248666][G train loss: 1.077358]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248290][G eval loss: 0.974334]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.247895][G train loss: 1.077388]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247344][G eval loss: 0.974811]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246748][G train loss: 1.077782]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.245911][G eval loss: 0.975807]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245027][G train loss: 1.078702]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.243840][G eval loss: 0.977742]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242530][G train loss: 1.080577]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.241251][G eval loss: 0.980270]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239344][G train loss: 1.083060]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.237786][G eval loss: 0.983854]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235051][G train loss: 1.086613]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.233168][G eval loss: 0.988171]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.229415][G train loss: 1.090915]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.227186][G eval loss: 0.992998]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.222322][G train loss: 1.095732]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.219008][G eval loss: 0.999394]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.213300][G train loss: 1.102083]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.208799][G eval loss: 1.007327]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.202468][G train loss: 1.109969]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.196849][G eval loss: 1.016805]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.189892][G train loss: 1.119344]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.183202][G eval loss: 1.029122]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.175349][G train loss: 1.131437]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.167789][G eval loss: 1.045207]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.159043][G train loss: 1.147091]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.151443][G eval loss: 1.065116]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.141762][G train loss: 1.166414]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.135429][G eval loss: 1.088003]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.124938][G train loss: 1.188365]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.121246][G eval loss: 1.112219]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.110120][G train loss: 1.210703]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.111592][G eval loss: 1.130239]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.101062][G train loss: 1.221482]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.137114][G eval loss: 1.087139]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.176739][G train loss: 1.095405]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.206428][G eval loss: 0.988930]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.249588][G train loss: 1.004220]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.249495][G eval loss: 0.950576]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.290107][G train loss: 0.960772]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.302324][G eval loss: 0.874356]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.309232][G train loss: 0.940363]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.323434][G eval loss: 0.844722]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.330687][G train loss: 0.906007]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.345647][G eval loss: 0.832353]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.354003][G train loss: 0.872619]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.366011][G eval loss: 0.817134]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.359205][G train loss: 0.871567]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.391565][G eval loss: 0.813755]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.377149][G train loss: 0.860848]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.398030][G eval loss: 0.837405]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.381142][G train loss: 0.881747]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.410043][G eval loss: 0.856913]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.380246][G train loss: 0.900620]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.419302][G eval loss: 0.867714]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.378062][G train loss: 0.910781]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.418705][G eval loss: 0.868190]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.374006][G train loss: 0.911148]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.416029][G eval loss: 0.859691]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.367829][G train loss: 0.903267]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.411649][G eval loss: 0.845074]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.359861][G train loss: 0.889413]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.392463][G eval loss: 0.827682]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.350713][G train loss: 0.872545]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.375589][G eval loss: 0.810803]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.342660][G train loss: 0.855931]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.349118][G eval loss: 0.821175]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.329032][G train loss: 0.851656]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.337114][G eval loss: 0.836471]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.320416][G train loss: 0.844571]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.329314][G eval loss: 0.836861]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.311917][G train loss: 0.838973]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.322002][G eval loss: 0.836725]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.303288][G train loss: 0.836861]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.314763][G eval loss: 0.838672]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.294690][G train loss: 0.837518]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.307298][G eval loss: 0.842609]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.281732][G train loss: 0.854114]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.299038][G eval loss: 0.850036]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.273595][G train loss: 0.860230]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.291296][G eval loss: 0.861779]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.263684][G train loss: 0.872325]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.285056][G eval loss: 0.874243]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.251804][G train loss: 0.896466]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.282191][G eval loss: 0.883208]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.244719][G train loss: 0.917795]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.282652][G eval loss: 0.889219]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.234532][G train loss: 0.952652]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.281635][G eval loss: 0.900100]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.232716][G train loss: 0.965767]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.280546][G eval loss: 0.912227]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.232856][G train loss: 0.972883]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.279901][G eval loss: 0.924742]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.234083][G train loss: 0.980205]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.279525][G eval loss: 0.937033]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.235730][G train loss: 0.987159]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.278994][G eval loss: 0.949093]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.236205][G train loss: 0.994677]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.279551][G eval loss: 0.959637]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.239781][G train loss: 0.991773]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.280442][G eval loss: 0.969696]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.242928][G train loss: 0.994245]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.281162][G eval loss: 0.978187]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.250122][G train loss: 1.003083]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.281972][G eval loss: 0.983122]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.251819][G train loss: 1.010329]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.282834][G eval loss: 0.989683]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.253760][G train loss: 1.015312]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.283654][G eval loss: 0.995403]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.254146][G train loss: 1.022045]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.284427][G eval loss: 0.997553]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.254631][G train loss: 1.027181]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.284958][G eval loss: 0.999356]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.261325][G train loss: 1.025912]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.285889][G eval loss: 1.000534]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.264254][G train loss: 1.019989]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.286216][G eval loss: 0.997412]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.266066][G train loss: 1.020487]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.286574][G eval loss: 0.988512]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.266250][G train loss: 1.019623]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.286895][G eval loss: 0.984594]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.265301][G train loss: 1.016304]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.286823][G eval loss: 0.983301]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.264541][G train loss: 1.013577]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.286718][G eval loss: 0.979564]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.263095][G train loss: 1.009141]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.286730][G eval loss: 0.978179]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.262255][G train loss: 1.005199]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.286762][G eval loss: 0.966693]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.261833][G train loss: 0.999111]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.286237][G eval loss: 0.964936]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.248710][G train loss: 0.995628]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.285562][G eval loss: 0.956612]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.247465][G train loss: 0.989052]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.285196][G eval loss: 0.947838]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.244648][G train loss: 0.990540]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.293303][G eval loss: 0.890628]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.242444][G train loss: 0.987130]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.293162][G eval loss: 0.883315]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.241129][G train loss: 0.985106]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.293019][G eval loss: 0.877071]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.236598][G train loss: 0.989918]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.292858][G eval loss: 0.867340]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.236537][G train loss: 0.977798]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.292912][G eval loss: 0.857373]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.234768][G train loss: 0.975394]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.292999][G eval loss: 0.847358]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.231541][G train loss: 0.980168]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.293027][G eval loss: 0.841011]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.229523][G train loss: 0.977777]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.293502][G eval loss: 0.828374]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.228532][G train loss: 0.971273]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.293854][G eval loss: 0.815526]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.227538][G train loss: 0.965880]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.294499][G eval loss: 0.805200]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.226851][G train loss: 0.955362]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.295261][G eval loss: 0.796220]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.226162][G train loss: 0.948988]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.295915][G eval loss: 0.779642]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.225730][G train loss: 0.939400]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.296726][G eval loss: 0.768440]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.225515][G train loss: 0.926772]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.297598][G eval loss: 0.753848]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.225366][G train loss: 0.914816]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.298268][G eval loss: 0.740686]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.225375][G train loss: 0.903572]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.299157][G eval loss: 0.727886]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.225414][G train loss: 0.892536]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.299874][G eval loss: 0.717193]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.225468][G train loss: 0.877540]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.300510][G eval loss: 0.710328]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.225454][G train loss: 0.866614]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.300534][G eval loss: 0.697202]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.222328][G train loss: 0.864977]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.297127][G eval loss: 0.690422]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.222152][G train loss: 0.855515]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.292227][G eval loss: 0.695526]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.218329][G train loss: 0.858423]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.288970][G eval loss: 0.700729]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.217347][G train loss: 0.851319]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.266935][G eval loss: 0.784527]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.216173][G train loss: 0.846107]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.277945][G eval loss: 0.751551]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.217400][G train loss: 0.852293]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.269845][G eval loss: 0.792118]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.218554][G train loss: 0.841903]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.258142][G eval loss: 0.816690]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.216766][G train loss: 0.835964]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.266176][G eval loss: 0.821679]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.0035 lr_d=0.0015 -> score=1.087855\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.986775]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 1.089255]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.975964]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.078692]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.973747]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.076833]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.972862]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.076212]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.972447]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249966][G train loss: 1.075883]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.972522]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.075942]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.972790]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249939][G train loss: 1.076137]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 0.972991]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076238]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.972881]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.076022]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249895][G eval loss: 0.972301]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249863][G train loss: 1.075346]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249865][G eval loss: 0.971321]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249825][G train loss: 1.074276]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.970161]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249779][G train loss: 1.073047]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249782][G eval loss: 0.969241]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249723][G train loss: 1.072081]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249727][G eval loss: 0.968739]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249655][G train loss: 1.071552]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249658][G eval loss: 0.968655]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249571][G train loss: 1.071466]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249579][G eval loss: 0.968837]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249474][G train loss: 1.071655]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249471][G eval loss: 0.969054]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249347][G train loss: 1.071879]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249344][G eval loss: 0.969378]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249197][G train loss: 1.072205]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249192][G eval loss: 0.969828]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249019][G train loss: 1.072653]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249014][G eval loss: 0.970294]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248810][G train loss: 1.073109]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248805][G eval loss: 0.970551]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248569][G train loss: 1.073356]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248562][G eval loss: 0.970452]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248293][G train loss: 1.073235]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248266][G eval loss: 0.969860]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.247964][G train loss: 1.072597]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.247929][G eval loss: 0.968859]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247594][G train loss: 1.071515]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247562][G eval loss: 0.967220]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247201][G train loss: 1.069715]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247152][G eval loss: 0.964711]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246785][G train loss: 1.066935]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246697][G eval loss: 0.960592]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246347][G train loss: 1.062362]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246193][G eval loss: 0.954414]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245880][G train loss: 1.055551]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245693][G eval loss: 0.945369]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.245474][G train loss: 1.045663]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245287][G eval loss: 0.931847]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.245271][G train loss: 1.030962]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.245104][G eval loss: 0.914033]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.245451][G train loss: 1.011408]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.245203][G eval loss: 0.895361]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.246055][G train loss: 0.990343]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.245440][G eval loss: 0.881844]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.246715][G train loss: 0.974119]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.245645][G eval loss: 0.877419]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.247181][G train loss: 0.966411]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.245656][G eval loss: 0.873742]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.247307][G train loss: 0.958256]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.245266][G eval loss: 0.862030]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.247062][G train loss: 0.942195]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.244450][G eval loss: 0.840141]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.246444][G train loss: 0.918488]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.243213][G eval loss: 0.814473]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.245380][G train loss: 0.894440]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.241843][G eval loss: 0.793898]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.244298][G train loss: 0.874266]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.240947][G eval loss: 0.769380]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.243627][G train loss: 0.853222]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.241040][G eval loss: 0.740603]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.243822][G train loss: 0.827518]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.241701][G eval loss: 0.711399]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.244008][G train loss: 0.798783]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.243544][G eval loss: 0.683486]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.244937][G train loss: 0.768174]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.245486][G eval loss: 0.659814]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.245884][G train loss: 0.739838]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.245892][G eval loss: 0.637654]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.246295][G train loss: 0.714855]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.245523][G eval loss: 0.611536]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.246104][G train loss: 0.687564]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.244737][G eval loss: 0.579775]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.245640][G train loss: 0.656237]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.242981][G eval loss: 0.547579]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.244731][G train loss: 0.622426]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.240408][G eval loss: 0.517886]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.243111][G train loss: 0.590352]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.237019][G eval loss: 0.498071]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.241237][G train loss: 0.570101]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.234386][G eval loss: 0.493093]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.239712][G train loss: 0.562843]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.234311][G eval loss: 0.493576]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.238707][G train loss: 0.563096]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.237833][G eval loss: 0.488874]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.239409][G train loss: 0.564645]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.242105][G eval loss: 0.480953]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.242725][G train loss: 0.558255]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.246890][G eval loss: 0.470073]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.248338][G train loss: 0.540034]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.254335][G eval loss: 0.454821]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.252720][G train loss: 0.525156]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.264269][G eval loss: 0.435458]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.256675][G train loss: 0.509873]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.266481][G eval loss: 0.435348]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.260045][G train loss: 0.504823]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.267556][G eval loss: 0.440662]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.0005 -> score=0.708218\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.984848]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.087328]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.975677]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.078404]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.974773]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.077859]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.974453]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249970][G train loss: 1.077803]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.973803]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249953][G train loss: 1.077240]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249947][G eval loss: 0.973300]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249926][G train loss: 1.076720]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249917][G eval loss: 0.972916]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249887][G train loss: 1.076263]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249875][G eval loss: 0.972530]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249835][G train loss: 1.075776]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249818][G eval loss: 0.972123]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249762][G train loss: 1.075264]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249741][G eval loss: 0.971434]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249665][G train loss: 1.074477]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249636][G eval loss: 0.970513]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249535][G train loss: 1.073466]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249500][G eval loss: 0.969577]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249365][G train loss: 1.072461]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249325][G eval loss: 0.968915]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249148][G train loss: 1.071752]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249094][G eval loss: 0.968727]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248867][G train loss: 1.071537]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248790][G eval loss: 0.969056]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248500][G train loss: 1.071860]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248386][G eval loss: 0.969812]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248019][G train loss: 1.072619]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247868][G eval loss: 0.970860]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247404][G train loss: 1.073669]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247288][G eval loss: 0.971835]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246706][G train loss: 1.074647]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246423][G eval loss: 0.972387]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245722][G train loss: 1.075181]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245291][G eval loss: 0.972911]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244482][G train loss: 1.075652]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.243985][G eval loss: 0.973293]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243072][G train loss: 1.075963]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242485][G eval loss: 0.973550]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241470][G train loss: 1.076116]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.240782][G eval loss: 0.973705]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239669][G train loss: 1.076073]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.238884][G eval loss: 0.973830]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237706][G train loss: 1.075810]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236863][G eval loss: 0.973796]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235675][G train loss: 1.075098]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234806][G eval loss: 0.972839]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.233712][G train loss: 1.072989]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.232761][G eval loss: 0.969082]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.231886][G train loss: 1.067488]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.231113][G eval loss: 0.961081]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.230819][G train loss: 1.056575]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.230403][G eval loss: 0.947883]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.231742][G train loss: 1.038087]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.230465][G eval loss: 0.929777]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.233492][G train loss: 1.014470]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.231200][G eval loss: 0.909043]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.235480][G train loss: 0.989017]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.235745][G eval loss: 0.885975]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.238279][G train loss: 0.967363]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.240784][G eval loss: 0.875062]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.242252][G train loss: 0.955450]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.242788][G eval loss: 0.877915]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.245024][G train loss: 0.954848]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.243219][G eval loss: 0.881607]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.246681][G train loss: 0.955087]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.241874][G eval loss: 0.881142]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.248794][G train loss: 0.947589]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.233198][G eval loss: 0.883169]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.249183][G train loss: 0.935996]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.229005][G eval loss: 0.870633]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.247419][G train loss: 0.918323]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.225043][G eval loss: 0.853904]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.241932][G train loss: 0.900990]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.220607][G eval loss: 0.837426]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.238162][G train loss: 0.878793]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.215969][G eval loss: 0.821863]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.234581][G train loss: 0.858582]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.212448][G eval loss: 0.808102]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.231260][G train loss: 0.843136]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.212599][G eval loss: 0.787448]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.230720][G train loss: 0.825080]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.220784][G eval loss: 0.755982]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.235822][G train loss: 0.798802]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.231044][G eval loss: 0.723325]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.239106][G train loss: 0.777401]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.243640][G eval loss: 0.697724]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.245269][G train loss: 0.750687]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.247907][G eval loss: 0.690029]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.254809][G train loss: 0.726992]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.257222][G eval loss: 0.678162]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.258913][G train loss: 0.716931]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.258372][G eval loss: 0.674579]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.258473][G train loss: 0.713458]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.258907][G eval loss: 0.668751]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.257740][G train loss: 0.708102]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.259414][G eval loss: 0.657853]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.257837][G train loss: 0.696369]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.259113][G eval loss: 0.642307]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.258631][G train loss: 0.676877]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.259626][G eval loss: 0.617449]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.257860][G train loss: 0.655402]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.267260][G eval loss: 0.570233]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.256481][G train loss: 0.630167]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.270270][G eval loss: 0.539071]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.254977][G train loss: 0.603941]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.271633][G eval loss: 0.513165]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.253623][G train loss: 0.580870]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.273057][G eval loss: 0.496234]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.253284][G train loss: 0.562729]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.274336][G eval loss: 0.488495]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.254345][G train loss: 0.551770]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.275702][G eval loss: 0.489834]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.256656][G train loss: 0.552406]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.278095][G eval loss: 0.487520]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.258848][G train loss: 0.560423]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.283496][G eval loss: 0.481337]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.259869][G train loss: 0.565866]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.288989][G eval loss: 0.480085]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.261594][G train loss: 0.568335]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.289092][G eval loss: 0.481458]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.261544][G train loss: 0.568594]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.288968][G eval loss: 0.477374]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.260974][G train loss: 0.561682]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.288666][G eval loss: 0.470182]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.260039][G train loss: 0.551658]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.288192][G eval loss: 0.461762]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.258756][G train loss: 0.537878]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.287600][G eval loss: 0.450810]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.257200][G train loss: 0.523786]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.286771][G eval loss: 0.442139]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.255546][G train loss: 0.515316]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.285618][G eval loss: 0.438561]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.254105][G train loss: 0.514201]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.284115][G eval loss: 0.442519]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.252982][G train loss: 0.518914]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.281880][G eval loss: 0.451205]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.251658][G train loss: 0.526520]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.279233][G eval loss: 0.459441]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.250601][G train loss: 0.533559]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.276302][G eval loss: 0.464226]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.249966][G train loss: 0.537037]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.273094][G eval loss: 0.467690]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.249482][G train loss: 0.539174]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.270016][G eval loss: 0.473023]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.248930][G train loss: 0.542244]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.267961][G eval loss: 0.481602]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.248952][G train loss: 0.548020]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.266210][G eval loss: 0.496008]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.248894][G train loss: 0.559980]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.265691][G eval loss: 0.511525]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.249539][G train loss: 0.573870]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.265316][G eval loss: 0.521812]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250150][G train loss: 0.583338]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.264377][G eval loss: 0.526412]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250368][G train loss: 0.587082]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.262835][G eval loss: 0.524544]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.250341][G train loss: 0.584237]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.260814][G eval loss: 0.517876]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.249950][G train loss: 0.577385]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.258824][G eval loss: 0.508390]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.249259][G train loss: 0.569745]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.256529][G eval loss: 0.500352]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.248375][G train loss: 0.563678]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.253837][G eval loss: 0.493902]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.247554][G train loss: 0.558582]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.251299][G eval loss: 0.486435]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.246983][G train loss: 0.552496]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.249195][G eval loss: 0.479598]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.246724][G train loss: 0.546013]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.247819][G eval loss: 0.473443]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.246553][G train loss: 0.539878]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.247107][G eval loss: 0.468986]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.246263][G train loss: 0.534760]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.246424][G eval loss: 0.467152]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.245745][G train loss: 0.531318]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.245480][G eval loss: 0.468033]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.244893][G train loss: 0.530594]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.244092][G eval loss: 0.471415]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.0008 -> score=0.715508\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.001 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250019][G eval loss: 0.983671]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 1.086151]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.975500]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 1.078228]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.975483]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.078570]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.975515]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249961][G train loss: 1.078865]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249953][G eval loss: 0.974746]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249932][G train loss: 1.078182]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249916][G eval loss: 0.973927]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249884][G train loss: 1.077347]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 0.973183]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249815][G train loss: 1.076530]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249780][G eval loss: 0.972630]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249713][G train loss: 1.075876]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249663][G eval loss: 0.972321]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249567][G train loss: 1.075460]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249505][G eval loss: 0.971728]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249371][G train loss: 1.074770]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249290][G eval loss: 0.970861]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249107][G train loss: 1.073813]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248988][G eval loss: 0.969977]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248737][G train loss: 1.072858]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248568][G eval loss: 0.969446]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248229][G train loss: 1.072279]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248002][G eval loss: 0.969575]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247551][G train loss: 1.072376]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247234][G eval loss: 0.970323]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246638][G train loss: 1.073113]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246348][G eval loss: 0.971379]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245572][G train loss: 1.074170]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245012][G eval loss: 0.972408]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244060][G train loss: 1.075191]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243198][G eval loss: 0.973818]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242062][G train loss: 1.076572]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.240950][G eval loss: 0.975577]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239588][G train loss: 1.078292]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238189][G eval loss: 0.977667]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236556][G train loss: 1.080314]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235031][G eval loss: 0.979633]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.233078][G train loss: 1.082215]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.231336][G eval loss: 0.981535]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.229063][G train loss: 1.083945]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.227046][G eval loss: 0.983414]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.224454][G train loss: 1.085498]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.222085][G eval loss: 0.985291]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.219219][G train loss: 1.086773]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.216788][G eval loss: 0.986633]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.213764][G train loss: 1.086910]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.211721][G eval loss: 0.986273]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.209496][G train loss: 1.082752]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.208338][G eval loss: 0.980257]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.208778][G train loss: 1.068933]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.207655][G eval loss: 0.965686]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.214243][G train loss: 1.041240]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.210259][G eval loss: 0.940868]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.220966][G train loss: 1.009028]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.223193][G eval loss: 0.897447]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.229051][G train loss: 0.973346]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.232912][G eval loss: 0.868837]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.239074][G train loss: 0.941063]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.241091][G eval loss: 0.857216]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.247985][G train loss: 0.924648]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.247161][G eval loss: 0.862603]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.256794][G train loss: 0.922957]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.250931][G eval loss: 0.873654]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.265013][G train loss: 0.926819]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.253570][G eval loss: 0.879112]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.269410][G train loss: 0.931766]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.259530][G eval loss: 0.872709]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.272588][G train loss: 0.931375]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.266910][G eval loss: 0.858560]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.272260][G train loss: 0.924792]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.252598][G eval loss: 0.866149]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.273816][G train loss: 0.903535]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.251274][G eval loss: 0.855032]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.270695][G train loss: 0.888863]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.251025][G eval loss: 0.831885]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.264206][G train loss: 0.875760]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.250770][G eval loss: 0.809874]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.259977][G train loss: 0.860428]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.253960][G eval loss: 0.787171]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.259890][G train loss: 0.840272]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.254001][G eval loss: 0.776792]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.258442][G train loss: 0.826653]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.252418][G eval loss: 0.767043]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.256509][G train loss: 0.813374]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.262562][G eval loss: 0.742723]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.266181][G train loss: 0.781678]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.263409][G eval loss: 0.737210]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.264132][G train loss: 0.776476]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.263420][G eval loss: 0.734782]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.259189][G train loss: 0.778808]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.264185][G eval loss: 0.731386]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.256972][G train loss: 0.775007]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.265934][G eval loss: 0.732559]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.260970][G train loss: 0.768639]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.266873][G eval loss: 0.736927]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.001 -> score=1.003800\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250031][G eval loss: 0.982492]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250028][G train loss: 1.084972]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.975300]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.078028]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.976157]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.079244]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.976549]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249951][G train loss: 1.079899]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249934][G eval loss: 0.975718]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249906][G train loss: 1.079154]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249875][G eval loss: 0.974618]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249831][G train loss: 1.078038]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249785][G eval loss: 0.973566]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249721][G train loss: 1.076913]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249654][G eval loss: 0.973439]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249559][G train loss: 1.076685]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249476][G eval loss: 0.973224]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249336][G train loss: 1.076362]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249210][G eval loss: 0.972683]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248996][G train loss: 1.075724]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248769][G eval loss: 0.971812]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248468][G train loss: 1.074763]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248109][G eval loss: 0.971014]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247703][G train loss: 1.073891]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247265][G eval loss: 0.970855]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246711][G train loss: 1.073680]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246334][G eval loss: 0.971327]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245582][G train loss: 1.074118]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244886][G eval loss: 0.972279]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243897][G train loss: 1.075052]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242916][G eval loss: 0.973792]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241649][G train loss: 1.076553]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240309][G eval loss: 0.975788]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238705][G train loss: 1.078535]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.237012][G eval loss: 0.978193]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.234987][G train loss: 1.080923]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232866][G eval loss: 0.980983]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230375][G train loss: 1.083610]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227654][G eval loss: 0.984635]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.224661][G train loss: 1.087171]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221643][G eval loss: 0.988394]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.218113][G train loss: 1.090822]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.214780][G eval loss: 0.992449]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.210570][G train loss: 1.094641]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.206813][G eval loss: 0.997282]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.201887][G train loss: 1.098915]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.197768][G eval loss: 1.002838]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.192232][G train loss: 1.103476]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.188391][G eval loss: 1.007678]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.182548][G train loss: 1.106460]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.181355][G eval loss: 1.006413]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.179561][G train loss: 1.093710]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.180811][G eval loss: 0.993322]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.194282][G train loss: 1.052959]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.189000][G eval loss: 0.961957]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.211850][G train loss: 1.012021]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.218286][G eval loss: 0.904550]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.230460][G train loss: 0.971210]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.234528][G eval loss: 0.876100]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.246055][G train loss: 0.938073]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.244629][G eval loss: 0.863359]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.260997][G train loss: 0.913223]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.253729][G eval loss: 0.860654]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.277117][G train loss: 0.902031]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.283320][G eval loss: 0.845027]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.288681][G train loss: 0.910226]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.297678][G eval loss: 0.865268]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.292932][G train loss: 0.927107]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.307809][G eval loss: 0.883651]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.301353][G train loss: 0.927920]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.313523][G eval loss: 0.891711]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.305036][G train loss: 0.928327]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.315643][G eval loss: 0.884165]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.303987][G train loss: 0.921614]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.314020][G eval loss: 0.866900]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.301515][G train loss: 0.903662]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.308698][G eval loss: 0.845078]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.300933][G train loss: 0.879652]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.278290][G eval loss: 0.859993]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.291354][G train loss: 0.869546]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.267840][G eval loss: 0.849809]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.283778][G train loss: 0.857591]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.255064][G eval loss: 0.848794]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.271115][G train loss: 0.854308]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.246463][G eval loss: 0.846828]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.259940][G train loss: 0.860623]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.258635][G eval loss: 0.812317]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.255182][G train loss: 0.859122]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.260642][G eval loss: 0.801171]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.255606][G train loss: 0.849249]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.257193][G eval loss: 0.800489]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.254213][G train loss: 0.845366]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.254490][G eval loss: 0.799858]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.256982][G train loss: 0.834624]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.254187][G eval loss: 0.799883]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.256980][G train loss: 0.832787]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.253386][G eval loss: 0.802135]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.253592][G train loss: 0.837647]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.252402][G eval loss: 0.806615]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.254773][G train loss: 0.835467]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.253124][G eval loss: 0.812401]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.255446][G train loss: 0.843933]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.253440][G eval loss: 0.823536]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.252348][G train loss: 0.859112]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.253176][G eval loss: 0.834612]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.249598][G train loss: 0.872380]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.252351][G eval loss: 0.843728]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.247677][G train loss: 0.881551]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.251882][G eval loss: 0.852513]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.248769][G train loss: 0.884585]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.251769][G eval loss: 0.860969]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.247009][G train loss: 0.892415]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.251600][G eval loss: 0.863507]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.245758][G train loss: 0.895395]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.251688][G eval loss: 0.862955]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.244435][G train loss: 0.896192]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.252176][G eval loss: 0.860666]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.242677][G train loss: 0.898178]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.253581][G eval loss: 0.856699]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.241257][G train loss: 0.899638]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.255878][G eval loss: 0.854005]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.240859][G train loss: 0.896526]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.259754][G eval loss: 0.850537]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.240424][G train loss: 0.891708]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.263598][G eval loss: 0.843930]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.239966][G train loss: 0.885352]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.268164][G eval loss: 0.836090]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.237211][G train loss: 0.889773]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.272189][G eval loss: 0.825278]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.237470][G train loss: 0.879691]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.275305][G eval loss: 0.810700]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.238168][G train loss: 0.867056]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.278525][G eval loss: 0.795248]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.232230][G train loss: 0.880980]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.280447][G eval loss: 0.779052]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.231906][G train loss: 0.875028]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.281782][G eval loss: 0.762362]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.230106][G train loss: 0.873473]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.282414][G eval loss: 0.746919]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.229856][G train loss: 0.864236]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.282191][G eval loss: 0.734509]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.232606][G train loss: 0.856225]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.278449][G eval loss: 0.740470]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.231112][G train loss: 0.858129]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.276968][G eval loss: 0.745824]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.234007][G train loss: 0.864435]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.275403][G eval loss: 0.749682]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.233933][G train loss: 0.867134]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.274165][G eval loss: 0.759505]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.234663][G train loss: 0.855679]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.273508][G eval loss: 0.755186]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.233191][G train loss: 0.859714]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.272473][G eval loss: 0.758004]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.233554][G train loss: 0.846075]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.271489][G eval loss: 0.756407]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.227886][G train loss: 0.838047]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.279820][G eval loss: 0.717740]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.224029][G train loss: 0.833865]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.278922][G eval loss: 0.714870]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.227115][G train loss: 0.813887]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.277758][G eval loss: 0.716681]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.227198][G train loss: 0.799802]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.276387][G eval loss: 0.708922]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.229092][G train loss: 0.769142]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.275247][G eval loss: 0.695058]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.228971][G train loss: 0.749268]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.279484][G eval loss: 0.666184]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.232405][G train loss: 0.714475]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.280464][G eval loss: 0.643362]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.232560][G train loss: 0.695427]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.283572][G eval loss: 0.588146]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.233170][G train loss: 0.680039]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.293262][G eval loss: 0.538539]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.237112][G train loss: 0.652892]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.295487][G eval loss: 0.523818]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.244091][G train loss: 0.609582]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.301038][G eval loss: 0.510725]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.246108][G train loss: 0.603465]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.302873][G eval loss: 0.505277]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.246107][G train loss: 0.601820]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.305346][G eval loss: 0.503138]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.248390][G train loss: 0.586810]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.306190][G eval loss: 0.498938]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.248336][G train loss: 0.580089]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.306894][G eval loss: 0.493418]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.248424][G train loss: 0.572059]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.306515][G eval loss: 0.486622]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.248290][G train loss: 0.560078]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.303738][G eval loss: 0.472102]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.248132][G train loss: 0.544912]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.278411][G eval loss: 0.457630]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.250669][G train loss: 0.516518]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.279076][G eval loss: 0.447496]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.250614][G train loss: 0.505881]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.279768][G eval loss: 0.443794]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.250628][G train loss: 0.498110]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.280407][G eval loss: 0.442896]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.247885][G train loss: 0.511607]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.280995][G eval loss: 0.444460]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.247874][G train loss: 0.512926]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.281367][G eval loss: 0.447158]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.247844][G train loss: 0.514336]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.281668][G eval loss: 0.446497]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.247886][G train loss: 0.515132]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.281888][G eval loss: 0.447004]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.250621][G train loss: 0.501216]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.282067][G eval loss: 0.449108]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.250624][G train loss: 0.499805]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.282306][G eval loss: 0.446893]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.250628][G train loss: 0.496821]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.282500][G eval loss: 0.442192]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.250624][G train loss: 0.492221]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.282639][G eval loss: 0.439860]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.250614][G train loss: 0.489124]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.282704][G eval loss: 0.438349]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.250603][G train loss: 0.486505]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.282701][G eval loss: 0.440504]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.250586][G train loss: 0.484961]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.282660][G eval loss: 0.445303]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.0012 -> score=0.727963\n",
      "\n",
      "----- 0056: lr_g=0.004, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250055][G eval loss: 0.980718]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250052][G train loss: 1.083198]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.974865]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.077593]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.977018]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249964][G train loss: 1.080105]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.977931]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.081281]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249961][G eval loss: 0.976979]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249938][G train loss: 1.080415]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249887][G eval loss: 0.976116]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249846][G train loss: 1.079536]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249788][G eval loss: 0.975345]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249727][G train loss: 1.078691]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249625][G eval loss: 0.975064]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249525][G train loss: 1.078309]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249367][G eval loss: 0.974679]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249212][G train loss: 1.077818]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248970][G eval loss: 0.974108]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248728][G train loss: 1.077149]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248360][G eval loss: 0.973533]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.247995][G train loss: 1.076482]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247454][G eval loss: 0.973343]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246901][G train loss: 1.076216]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246095][G eval loss: 0.973969]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245268][G train loss: 1.076784]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244131][G eval loss: 0.975844]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242913][G train loss: 1.078611]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.241609][G eval loss: 0.978813]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239823][G train loss: 1.081556]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.238252][G eval loss: 0.982869]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235659][G train loss: 1.085608]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.233825][G eval loss: 0.987225]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.230242][G train loss: 1.089953]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.227981][G eval loss: 0.991940]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223303][G train loss: 1.094633]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.220066][G eval loss: 0.998120]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.214515][G train loss: 1.100723]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.210096][G eval loss: 1.005861]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.203861][G train loss: 1.108377]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.198405][G eval loss: 1.014715]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.191605][G train loss: 1.117026]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.185092][G eval loss: 1.025843]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.177487][G train loss: 1.127751]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.170220][G eval loss: 1.039841]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.161901][G train loss: 1.140857]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.154556][G eval loss: 1.056859]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.145716][G train loss: 1.156208]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.140435][G eval loss: 1.073669]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.132027][G train loss: 1.168450]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.141275][G eval loss: 1.065348]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.155200][G train loss: 1.112789]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.170638][G eval loss: 1.009308]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.206895][G train loss: 1.036394]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.222572][G eval loss: 0.935474]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.243935][G train loss: 0.984473]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.241330][G eval loss: 0.916158]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.273120][G train loss: 0.941019]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.263632][G eval loss: 0.867461]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.291009][G train loss: 0.914741]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.294513][G eval loss: 0.840455]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.300276][G train loss: 0.900777]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.304785][G eval loss: 0.850982]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.312701][G train loss: 0.897490]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.327211][G eval loss: 0.861805]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.326757][G train loss: 0.902566]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.353717][G eval loss: 0.873431]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.337629][G train loss: 0.928883]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.365594][G eval loss: 0.902584]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.343719][G train loss: 0.946948]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.368774][G eval loss: 0.918557]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.351772][G train loss: 0.951432]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.371713][G eval loss: 0.912387]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.348448][G train loss: 0.950906]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.373596][G eval loss: 0.891152]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.343583][G train loss: 0.935739]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.345845][G eval loss: 0.898645]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.336776][G train loss: 0.910317]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.331948][G eval loss: 0.883077]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.323961][G train loss: 0.893726]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.323183][G eval loss: 0.858876]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.313912][G train loss: 0.868053]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.316026][G eval loss: 0.837356]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.302855][G train loss: 0.845756]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.308802][G eval loss: 0.824063]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.291983][G train loss: 0.832603]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.299887][G eval loss: 0.820391]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.275630][G train loss: 0.846615]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.288546][G eval loss: 0.832158]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.263336][G train loss: 0.858275]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.279765][G eval loss: 0.853153]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.254988][G train loss: 0.872169]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.274788][G eval loss: 0.867442]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.243939][G train loss: 0.893335]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.271568][G eval loss: 0.880618]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.233735][G train loss: 0.921560]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.269908][G eval loss: 0.890162]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.230351][G train loss: 0.935396]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.269833][G eval loss: 0.896179]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.232648][G train loss: 0.936287]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.273596][G eval loss: 0.895001]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.233805][G train loss: 0.940814]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.275276][G eval loss: 0.903542]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.234069][G train loss: 0.947524]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.277030][G eval loss: 0.912414]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.238085][G train loss: 0.943751]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.277534][G eval loss: 0.926855]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.239912][G train loss: 0.954392]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.278066][G eval loss: 0.940128]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.240609][G train loss: 0.969106]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.278984][G eval loss: 0.952236]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.241300][G train loss: 0.982674]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.280150][G eval loss: 0.961567]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.242107][G train loss: 0.992923]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.281277][G eval loss: 0.964135]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.242952][G train loss: 0.999120]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.282387][G eval loss: 0.963762]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.243928][G train loss: 1.002087]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.283359][G eval loss: 0.962133]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.243440][G train loss: 1.011172]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.284235][G eval loss: 0.958715]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.243632][G train loss: 1.009226]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.284985][G eval loss: 0.953621]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.243657][G train loss: 1.005128]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.285585][G eval loss: 0.946565]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.243082][G train loss: 0.999492]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.286056][G eval loss: 0.939010]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.243379][G train loss: 0.989098]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.286387][G eval loss: 0.929981]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.241843][G train loss: 0.981364]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.286510][G eval loss: 0.919279]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.245142][G train loss: 0.979582]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.286731][G eval loss: 0.910147]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.241067][G train loss: 0.975177]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.286935][G eval loss: 0.900252]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.234108][G train loss: 0.980720]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.287077][G eval loss: 0.894335]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.231369][G train loss: 0.983747]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.287148][G eval loss: 0.884040]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.230069][G train loss: 0.975006]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.287172][G eval loss: 0.872828]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.228732][G train loss: 0.963279]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.286995][G eval loss: 0.853682]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.230015][G train loss: 0.938149]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.287157][G eval loss: 0.840319]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.234948][G train loss: 0.916273]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.287193][G eval loss: 0.826186]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.236829][G train loss: 0.900759]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.287355][G eval loss: 0.812455]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.238509][G train loss: 0.875101]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.287374][G eval loss: 0.796333]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.235940][G train loss: 0.864689]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.287280][G eval loss: 0.777795]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.234432][G train loss: 0.853746]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.287346][G eval loss: 0.755821]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.228140][G train loss: 0.836633]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.287166][G eval loss: 0.736049]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.227633][G train loss: 0.816089]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.287788][G eval loss: 0.711235]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.227225][G train loss: 0.797740]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.287948][G eval loss: 0.692519]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.224579][G train loss: 0.786657]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.288109][G eval loss: 0.674143]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.223818][G train loss: 0.764391]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.288662][G eval loss: 0.652450]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.223524][G train loss: 0.745796]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.289153][G eval loss: 0.626922]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.225763][G train loss: 0.712562]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.289519][G eval loss: 0.613712]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.226480][G train loss: 0.692441]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.302026][G eval loss: 0.543480]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.225587][G train loss: 0.682665]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.301780][G eval loss: 0.535717]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.228367][G train loss: 0.655125]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.302037][G eval loss: 0.528561]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.231727][G train loss: 0.630044]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.304360][G eval loss: 0.519124]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.247580][G train loss: 0.552885]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.305555][G eval loss: 0.507392]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.248328][G train loss: 0.541790]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.306788][G eval loss: 0.494878]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.255426][G train loss: 0.501676]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.306972][G eval loss: 0.484106]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.255977][G train loss: 0.492172]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.306781][G eval loss: 0.477487]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.256285][G train loss: 0.482154]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.307459][G eval loss: 0.471748]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.256467][G train loss: 0.473832]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.306071][G eval loss: 0.466509]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.256684][G train loss: 0.465005]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.274252][G eval loss: 0.463276]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.256602][G train loss: 0.463764]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.273097][G eval loss: 0.461857]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.253121][G train loss: 0.474670]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.285925][G eval loss: 0.390602]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.256156][G train loss: 0.457153]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.285828][G eval loss: 0.389875]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.255828][G train loss: 0.455179]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.283778][G eval loss: 0.392757]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.255379][G train loss: 0.454446]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.285394][G eval loss: 0.389310]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.254953][G train loss: 0.455420]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.287930][G eval loss: 0.386239]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.254410][G train loss: 0.456280]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.288483][G eval loss: 0.389181]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.253872][G train loss: 0.455642]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.290332][G eval loss: 0.390738]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.253343][G train loss: 0.459177]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.287939][G eval loss: 0.390870]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.252840][G train loss: 0.462173]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.278464][G eval loss: 0.388261]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.249582][G train loss: 0.477334]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.267409][G eval loss: 0.392773]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.249291][G train loss: 0.482962]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.270446][G eval loss: 0.398762]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.251315][G train loss: 0.476759]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.273489][G eval loss: 0.406332]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.250988][G train loss: 0.479893]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.275824][G eval loss: 0.410241]\n",
      "[Epoch 111/200][Batch 1/1][D train loss: 0.250769][G train loss: 0.481695]\n",
      "[Epoch 111/200][Batch 1/1][D eval loss: 0.277387][G eval loss: 0.416795]\n",
      "[Epoch 112/200][Batch 1/1][D train loss: 0.250460][G train loss: 0.484483]\n",
      "[Epoch 112/200][Batch 1/1][D eval loss: 0.278266][G eval loss: 0.420876]\n",
      "[Epoch 113/200][Batch 1/1][D train loss: 0.250236][G train loss: 0.487493]\n",
      "[Epoch 113/200][Batch 1/1][D eval loss: 0.278785][G eval loss: 0.420189]\n",
      "[Epoch 114/200][Batch 1/1][D train loss: 0.250087][G train loss: 0.487731]\n",
      "[Epoch 114/200][Batch 1/1][D eval loss: 0.279161][G eval loss: 0.421466]\n",
      "[Epoch 115/200][Batch 1/1][D train loss: 0.250045][G train loss: 0.488509]\n",
      "[Epoch 115/200][Batch 1/1][D eval loss: 0.279159][G eval loss: 0.423257]\n",
      "[Epoch 116/200][Batch 1/1][D train loss: 0.250320][G train loss: 0.491991]\n",
      "[Epoch 116/200][Batch 1/1][D eval loss: 0.278944][G eval loss: 0.425997]\n",
      "[Epoch 117/200][Batch 1/1][D train loss: 0.250128][G train loss: 0.491844]\n",
      "[Epoch 117/200][Batch 1/1][D eval loss: 0.278709][G eval loss: 0.427547]\n",
      "[Epoch 118/200][Batch 1/1][D train loss: 0.249755][G train loss: 0.491487]\n",
      "[Epoch 118/200][Batch 1/1][D eval loss: 0.278486][G eval loss: 0.429000]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.004 lr_d=0.0015 -> score=0.707486\n",
      "\n",
      "----- 0056: lr_g=0.0045, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.984913]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.087403]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.975149]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.077962]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.973224]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249984][G train loss: 1.076433]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.972277]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.075698]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.972302]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.075748]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.972886]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.076269]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.973428]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249940][G train loss: 1.076703]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 0.973547]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076703]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.972985]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.076031]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249894][G eval loss: 0.971760]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249863][G train loss: 1.074708]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249864][G eval loss: 0.970271]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249826][G train loss: 1.073149]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 0.969101]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249780][G train loss: 1.071937]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249783][G eval loss: 0.968446]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249725][G train loss: 1.071260]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249726][G eval loss: 0.968242]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249655][G train loss: 1.071054]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249652][G eval loss: 0.968242]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249566][G train loss: 1.071053]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249569][G eval loss: 0.968438]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249465][G train loss: 1.071246]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249454][G eval loss: 0.968772]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249331][G train loss: 1.071575]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249323][G eval loss: 0.969238]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249177][G train loss: 1.072042]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249169][G eval loss: 0.969555]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.248999][G train loss: 1.072361]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249000][G eval loss: 0.969434]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248802][G train loss: 1.072225]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248806][G eval loss: 0.968757]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248582][G train loss: 1.071505]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248578][G eval loss: 0.967520]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248333][G train loss: 1.070182]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248299][G eval loss: 0.965627]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248041][G train loss: 1.068132]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.247976][G eval loss: 0.962486]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247717][G train loss: 1.064740]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247623][G eval loss: 0.956877]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247366][G train loss: 1.058716]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247267][G eval loss: 0.947977]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.247040][G train loss: 1.049217]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.247000][G eval loss: 0.933791]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246885][G train loss: 1.034126]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246889][G eval loss: 0.914802]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.247001][G train loss: 1.013696]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.246956][G eval loss: 0.896211]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.247405][G train loss: 0.992948]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.247089][G eval loss: 0.885946]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.247781][G train loss: 0.980265]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.247089][G eval loss: 0.883466]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.247960][G train loss: 0.975220]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.246821][G eval loss: 0.875852]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.247878][G train loss: 0.964341]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.246316][G eval loss: 0.857509]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.247499][G train loss: 0.943646]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.245516][G eval loss: 0.832472]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.246876][G train loss: 0.916827]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.244511][G eval loss: 0.807551]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.246067][G train loss: 0.891275]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.243563][G eval loss: 0.784439]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.245223][G train loss: 0.867971]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.242969][G eval loss: 0.756652]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.244580][G train loss: 0.841864]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.242918][G eval loss: 0.723503]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.244789][G train loss: 0.809697]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.243653][G eval loss: 0.690182]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.245623][G train loss: 0.776210]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.245108][G eval loss: 0.658314]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.246380][G train loss: 0.744865]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.246043][G eval loss: 0.629406]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.246757][G train loss: 0.715647]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.245728][G eval loss: 0.598259]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.246499][G train loss: 0.683791]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.244976][G eval loss: 0.561444]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.245665][G train loss: 0.648792]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.243589][G eval loss: 0.525369]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.244218][G train loss: 0.613906]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.241149][G eval loss: 0.505946]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.242012][G train loss: 0.590439]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.238520][G eval loss: 0.508830]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.239422][G train loss: 0.585569]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.236989][G eval loss: 0.519911]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.0045 lr_d=0.0005 -> score=0.756900\n",
      "\n",
      "----- 0056: lr_g=0.0045, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.982986]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.085476]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.974863]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 1.077676]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.974255]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.077463]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.973873]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.077293]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.973661]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.077107]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 0.973662]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249929][G train loss: 1.077045]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.973547]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249890][G train loss: 1.076822]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249877][G eval loss: 0.973077]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249838][G train loss: 1.076232]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249820][G eval loss: 0.972211]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249766][G train loss: 1.075256]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249746][G eval loss: 0.970873]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249673][G train loss: 1.073819]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249643][G eval loss: 0.969444]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249546][G train loss: 1.072320]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249512][G eval loss: 0.968489]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249381][G train loss: 1.071322]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249339][G eval loss: 0.968090]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249168][G train loss: 1.070900]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249109][G eval loss: 0.968213]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248888][G train loss: 1.071021]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248797][G eval loss: 0.968653]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248513][G train loss: 1.071456]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248383][G eval loss: 0.969451]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248024][G train loss: 1.072246]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247848][G eval loss: 0.970610]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247398][G train loss: 1.073393]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247286][G eval loss: 0.971650]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246723][G train loss: 1.074432]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246420][G eval loss: 0.972110]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245744][G train loss: 1.074868]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245335][G eval loss: 0.972047]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244566][G train loss: 1.074724]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244150][G eval loss: 0.971413]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243308][G train loss: 1.073933]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242846][G eval loss: 0.970452]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241980][G train loss: 1.072659]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241393][G eval loss: 0.969119]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240566][G train loss: 1.070779]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239765][G eval loss: 0.966420]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.239049][G train loss: 1.067215]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.238162][G eval loss: 0.960427]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.237637][G train loss: 1.059876]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.236988][G eval loss: 0.948851]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.237039][G train loss: 1.045902]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.236701][G eval loss: 0.930729]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.238035][G train loss: 1.023641]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.236731][G eval loss: 0.909579]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.239101][G train loss: 0.998454]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.237689][G eval loss: 0.891396]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.240326][G train loss: 0.977686]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.240804][G eval loss: 0.882499]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.242189][G train loss: 0.968739]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.242053][G eval loss: 0.884556]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.243590][G train loss: 0.967689]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.240501][G eval loss: 0.884251]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.243872][G train loss: 0.962329]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.235753][G eval loss: 0.878774]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.243620][G train loss: 0.946349]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.232654][G eval loss: 0.860050]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.241796][G train loss: 0.922476]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.229237][G eval loss: 0.836871]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.239088][G train loss: 0.895036]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.225584][G eval loss: 0.813248]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.236212][G train loss: 0.869089]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.222488][G eval loss: 0.791499]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.233194][G train loss: 0.847057]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.220890][G eval loss: 0.766442]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.231872][G train loss: 0.822385]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.225809][G eval loss: 0.728236]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.235999][G train loss: 0.788134]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.232087][G eval loss: 0.696781]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.239529][G train loss: 0.758863]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.244554][G eval loss: 0.662693]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.245307][G train loss: 0.731141]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.250439][G eval loss: 0.645938]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.252675][G train loss: 0.707476]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.257454][G eval loss: 0.630455]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.255905][G train loss: 0.691573]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.259699][G eval loss: 0.615286]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.255766][G train loss: 0.678870]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.259961][G eval loss: 0.596041]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.255196][G train loss: 0.660538]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.259512][G eval loss: 0.570294]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.254451][G train loss: 0.636584]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.259380][G eval loss: 0.539583]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.253409][G train loss: 0.610456]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.260907][G eval loss: 0.504001]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.251170][G train loss: 0.585979]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.262656][G eval loss: 0.473830]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.249355][G train loss: 0.566366]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.265120][G eval loss: 0.456278]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.250023][G train loss: 0.548896]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.268987][G eval loss: 0.450919]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.253031][G train loss: 0.537827]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.274122][G eval loss: 0.441956]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.258502][G train loss: 0.525454]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.281976][G eval loss: 0.432858]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.262737][G train loss: 0.516339]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.282881][G eval loss: 0.437852]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.263492][G train loss: 0.510743]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.282998][G eval loss: 0.439851]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.0045 lr_d=0.0008 -> score=0.722848\n",
      "\n",
      "----- 0056: lr_g=0.0045, lr_d=0.001 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250019][G eval loss: 0.981809]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 1.084299]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.974686]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.077499]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.974966]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249974][G train loss: 1.078175]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.974936]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249962][G train loss: 1.078356]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.974602]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249935][G train loss: 1.078048]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.974287]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249889][G train loss: 1.077671]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249862][G eval loss: 0.973813]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249818][G train loss: 1.077088]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249780][G eval loss: 0.973174]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249715][G train loss: 1.076328]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249666][G eval loss: 0.972302]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249573][G train loss: 1.075346]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249510][G eval loss: 0.970980]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249379][G train loss: 1.073926]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249297][G eval loss: 0.969565]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249117][G train loss: 1.072440]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248995][G eval loss: 0.968674]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248748][G train loss: 1.071505]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248574][G eval loss: 0.968494]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248241][G train loss: 1.071298]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247992][G eval loss: 0.969087]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247548][G train loss: 1.071884]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247210][G eval loss: 0.970225]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246614][G train loss: 1.073018]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246169][G eval loss: 0.970846]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245416][G train loss: 1.073623]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.244686][G eval loss: 0.971756]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.243787][G train loss: 1.074486]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.242825][G eval loss: 0.973173]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.241770][G train loss: 1.075855]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.240624][G eval loss: 0.974870]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239360][G train loss: 1.077496]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238070][G eval loss: 0.976471]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236555][G train loss: 1.078973]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.235202][G eval loss: 0.977820]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.233424][G train loss: 1.080076]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.232062][G eval loss: 0.978859]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.230074][G train loss: 1.080520]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.228856][G eval loss: 0.979059]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.226932][G train loss: 1.079234]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.225888][G eval loss: 0.976033]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.224490][G train loss: 1.073344]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.224055][G eval loss: 0.966131]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.224464][G train loss: 1.057604]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.224379][G eval loss: 0.947314]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.227748][G train loss: 1.030536]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.227899][G eval loss: 0.917794]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.231611][G train loss: 0.998372]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.234956][G eval loss: 0.885303]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.236032][G train loss: 0.968168]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.240682][G eval loss: 0.869641]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.241947][G train loss: 0.948930]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.244125][G eval loss: 0.873984]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.246911][G train loss: 0.948099]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.244690][G eval loss: 0.883586]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.249068][G train loss: 0.953441]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.242927][G eval loss: 0.889490]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.249094][G train loss: 0.955504]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.240301][G eval loss: 0.889239]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.249199][G train loss: 0.950493]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.236958][G eval loss: 0.881559]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.246979][G train loss: 0.940747]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.232589][G eval loss: 0.866471]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.243670][G train loss: 0.925584]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.214842][G eval loss: 0.877817]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.240933][G train loss: 0.906146]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.210258][G eval loss: 0.869924]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.233215][G train loss: 0.900222]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.205927][G eval loss: 0.861586]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.228936][G train loss: 0.889013]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.202989][G eval loss: 0.853712]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.225004][G train loss: 0.879846]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.204341][G eval loss: 0.840485]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.222296][G train loss: 0.870058]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.210708][G eval loss: 0.818212]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.227826][G train loss: 0.846228]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.223949][G eval loss: 0.783368]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.236543][G train loss: 0.822906]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.244891][G eval loss: 0.747195]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.245285][G train loss: 0.800728]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 0.747908]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.258577][G train loss: 0.774149]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.261329][G eval loss: 0.735468]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.265672][G train loss: 0.766214]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.260914][G eval loss: 0.742798]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.265068][G train loss: 0.768731]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.259500][G eval loss: 0.743661]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.259988][G train loss: 0.772272]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.258453][G eval loss: 0.741580]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.256137][G train loss: 0.772424]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.257736][G eval loss: 0.734670]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.255884][G train loss: 0.761734]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.257073][G eval loss: 0.721999]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.253667][G train loss: 0.751462]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.256355][G eval loss: 0.703778]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.249619][G train loss: 0.737506]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.256124][G eval loss: 0.682899]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.246574][G train loss: 0.721234]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.258290][G eval loss: 0.660022]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.244270][G train loss: 0.701867]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.261854][G eval loss: 0.637336]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.242245][G train loss: 0.682338]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.265306][G eval loss: 0.612249]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.241082][G train loss: 0.664052]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.268517][G eval loss: 0.594296]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.240959][G train loss: 0.650010]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.270400][G eval loss: 0.584808]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.242962][G train loss: 0.641466]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.262058][G eval loss: 0.607670]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.249431][G train loss: 0.629440]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.264953][G eval loss: 0.587937]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.248394][G train loss: 0.640055]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.283266][G eval loss: 0.523644]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.249903][G train loss: 0.635096]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.295864][G eval loss: 0.500427]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.254709][G train loss: 0.620342]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.295819][G eval loss: 0.504703]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.258698][G train loss: 0.600316]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.295651][G eval loss: 0.496645]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.259268][G train loss: 0.590846]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.295439][G eval loss: 0.484515]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.260660][G train loss: 0.580037]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.295246][G eval loss: 0.474601]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.260893][G train loss: 0.568960]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.295048][G eval loss: 0.469022]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.260575][G train loss: 0.557528]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.294777][G eval loss: 0.465122]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.259667][G train loss: 0.548358]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.294456][G eval loss: 0.463204]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.258335][G train loss: 0.542443]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.294112][G eval loss: 0.461794]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.257215][G train loss: 0.538351]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.293535][G eval loss: 0.461899]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.256301][G train loss: 0.536716]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.292427][G eval loss: 0.463790]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.255256][G train loss: 0.537612]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.290549][G eval loss: 0.466115]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.254003][G train loss: 0.539276]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.287873][G eval loss: 0.468159]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.252051][G train loss: 0.540121]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.284505][G eval loss: 0.468618]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.250902][G train loss: 0.539561]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.281515][G eval loss: 0.467659]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.251125][G train loss: 0.537757]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.276592][G eval loss: 0.468079]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.251105][G train loss: 0.536536]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.267657][G eval loss: 0.471883]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.250885][G train loss: 0.538319]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.251359][G eval loss: 0.478223]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.250723][G train loss: 0.543183]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.250678][G eval loss: 0.484445]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250699][G train loss: 0.548624]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.250663][G eval loss: 0.487792]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250679][G train loss: 0.551711]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.250583][G eval loss: 0.486609]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.250563][G train loss: 0.550594]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.250416][G eval loss: 0.481396]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.250353][G train loss: 0.545850]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.250403][G eval loss: 0.474222]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.250337][G train loss: 0.539659]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.250413][G eval loss: 0.467584]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.250349][G train loss: 0.534598]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.250422][G eval loss: 0.464214]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.250360][G train loss: 0.532478]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.250415][G eval loss: 0.464354]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.250360][G train loss: 0.532890]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.250433][G eval loss: 0.465810]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.250383][G train loss: 0.534249]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.250448][G eval loss: 0.467442]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.250403][G train loss: 0.535021]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.250445][G eval loss: 0.469186]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.250399][G train loss: 0.535095]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.250495][G eval loss: 0.471059]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.250452][G train loss: 0.534448]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.250538][G eval loss: 0.473143]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.250500][G train loss: 0.533951]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.250567][G eval loss: 0.475721]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.0045 lr_d=0.001 -> score=0.726289\n",
      "\n",
      "----- 0056: lr_g=0.0045, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250031][G eval loss: 0.980629]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250029][G train loss: 1.083119]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.974487]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.077300]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.975641]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 1.078850]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.975971]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249952][G train loss: 1.079391]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 0.975570]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249912][G train loss: 1.079016]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249880][G eval loss: 0.974967]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249838][G train loss: 1.078350]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249791][G eval loss: 0.974179]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249729][G train loss: 1.077453]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249658][G eval loss: 0.973932]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249565][G train loss: 1.077086]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249482][G eval loss: 0.973221]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249346][G train loss: 1.076265]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249221][G eval loss: 0.972012]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249017][G train loss: 1.074956]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248802][G eval loss: 0.970630]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248504][G train loss: 1.073502]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248148][G eval loss: 0.969825]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247746][G train loss: 1.072650]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247286][G eval loss: 0.969898]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246741][G train loss: 1.072693]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246320][G eval loss: 0.970689]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245586][G train loss: 1.073474]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244819][G eval loss: 0.971745]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243858][G train loss: 1.074505]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242759][G eval loss: 0.973318]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241528][G train loss: 1.076053]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240073][G eval loss: 0.975444]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238520][G train loss: 1.078137]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236799][G eval loss: 0.977882]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.234835][G train loss: 1.080551]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232686][G eval loss: 0.980609]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230293][G train loss: 1.083126]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227677][G eval loss: 0.983624]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.224847][G train loss: 1.085925]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221969][G eval loss: 0.986376]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.218715][G train loss: 1.088323]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.215618][G eval loss: 0.989146]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.211944][G train loss: 1.090234]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.209221][G eval loss: 0.990560]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.205809][G train loss: 1.088910]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.204725][G eval loss: 0.986187]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.204539][G train loss: 1.075568]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.204652][G eval loss: 0.969138]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.212937][G train loss: 1.041971]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.213169][G eval loss: 0.930599]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.222289][G train loss: 1.003558]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.227989][G eval loss: 0.886247]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.233846][G train loss: 0.961559]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.237781][G eval loss: 0.862488]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.244867][G train loss: 0.932073]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.244805][G eval loss: 0.861849]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.254990][G train loss: 0.922910]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.249268][G eval loss: 0.875284]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.0045 lr_d=0.0012 -> score=1.124552\n",
      "\n",
      "----- 0056: lr_g=0.0045, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250055][G eval loss: 0.978856]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250052][G train loss: 1.081346]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.974049]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 1.076862]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.976494]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249964][G train loss: 1.079702]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 0.977345]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249976][G train loss: 1.080765]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249965][G eval loss: 0.976830]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249942][G train loss: 1.080276]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249893][G eval loss: 0.976472]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249853][G train loss: 1.079855]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249795][G eval loss: 0.975968]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249735][G train loss: 1.079243]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249632][G eval loss: 0.975574]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249535][G train loss: 1.078728]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249379][G eval loss: 0.974692]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249228][G train loss: 1.077735]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248988][G eval loss: 0.973425]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248756][G train loss: 1.076368]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248391][G eval loss: 0.972296]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248044][G train loss: 1.075165]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247500][G eval loss: 0.972057]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246971][G train loss: 1.074879]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246173][G eval loss: 0.972878]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245385][G train loss: 1.075660]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244231][G eval loss: 0.975096]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.243073][G train loss: 1.077851]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.241644][G eval loss: 0.978317]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239936][G train loss: 1.081048]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.238207][G eval loss: 0.982284]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235729][G train loss: 1.084987]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.233709][G eval loss: 0.986582]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.230279][G train loss: 1.089242]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.227853][G eval loss: 0.991288]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223380][G train loss: 1.093900]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.220004][G eval loss: 0.997307]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.214744][G train loss: 1.099757]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.210267][G eval loss: 1.004355]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.204389][G train loss: 1.106578]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.198995][G eval loss: 1.012014]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.192649][G train loss: 1.113689]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.186436][G eval loss: 1.021155]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.179585][G train loss: 1.121551]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.173575][G eval loss: 1.030007]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.166777][G train loss: 1.127376]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.169121][G eval loss: 1.022486]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.175767][G train loss: 1.090175]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.175955][G eval loss: 0.994258]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.201987][G train loss: 1.034354]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.210267][G eval loss: 0.922894]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.227069][G train loss: 0.982274]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231323][G eval loss: 0.889494]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246024][G train loss: 0.944343]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.241740][G eval loss: 0.874536]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.266605][G train loss: 0.911076]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.266131][G eval loss: 0.850913]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.280029][G train loss: 0.910133]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.284152][G eval loss: 0.864486]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.285672][G train loss: 0.925818]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.290367][G eval loss: 0.886585]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.290449][G train loss: 0.934250]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.298864][G eval loss: 0.913938]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.298322][G train loss: 0.948732]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.305217][G eval loss: 0.914093]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.297892][G train loss: 0.955146]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.310878][G eval loss: 0.894725]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.299652][G train loss: 0.944419]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.313143][G eval loss: 0.866197]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.295678][G train loss: 0.925003]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.292343][G eval loss: 0.871793]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.289704][G train loss: 0.899082]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.285648][G eval loss: 0.857480]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.276211][G train loss: 0.885996]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.279497][G eval loss: 0.840697]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.264895][G train loss: 0.873718]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.273722][G eval loss: 0.831757]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.253248][G train loss: 0.876587]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.281358][G eval loss: 0.807337]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.245905][G train loss: 0.881913]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.274109][G eval loss: 0.813651]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.245709][G train loss: 0.874326]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.269645][G eval loss: 0.814746]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.243342][G train loss: 0.874089]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.267946][G eval loss: 0.816726]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.239059][G train loss: 0.880465]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.269857][G eval loss: 0.815155]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.236312][G train loss: 0.881747]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.270137][G eval loss: 0.821147]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.234993][G train loss: 0.883440]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.269799][G eval loss: 0.832769]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.235411][G train loss: 0.886989]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.270101][G eval loss: 0.847243]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.236442][G train loss: 0.893281]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.271373][G eval loss: 0.864205]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.237477][G train loss: 0.905603]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.273370][G eval loss: 0.885155]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.241346][G train loss: 0.910131]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.275564][G eval loss: 0.912371]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.243139][G train loss: 0.933530]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.277329][G eval loss: 0.934960]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.243397][G train loss: 0.956428]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.278968][G eval loss: 0.952394]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.245047][G train loss: 0.972974]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.289307][G eval loss: 0.915312]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.245630][G train loss: 0.981440]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.290539][G eval loss: 0.915271]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.247773][G train loss: 0.980191]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.291719][G eval loss: 0.907730]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.250918][G train loss: 0.969985]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.292641][G eval loss: 0.897577]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.248985][G train loss: 0.971075]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.293426][G eval loss: 0.890925]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.248854][G train loss: 0.963715]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.294270][G eval loss: 0.878576]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.248407][G train loss: 0.952711]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.295087][G eval loss: 0.864632]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.247756][G train loss: 0.940878]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.296110][G eval loss: 0.854388]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.240860][G train loss: 0.961496]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.296673][G eval loss: 0.849154]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.242073][G train loss: 0.954058]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.297281][G eval loss: 0.839450]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.240546][G train loss: 0.952141]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.297711][G eval loss: 0.820636]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.241153][G train loss: 0.945863]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.298067][G eval loss: 0.811533]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.239955][G train loss: 0.939781]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.298321][G eval loss: 0.801167]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.239320][G train loss: 0.929717]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.298463][G eval loss: 0.784058]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.239113][G train loss: 0.918090]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.298557][G eval loss: 0.767178]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.238037][G train loss: 0.904542]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.299120][G eval loss: 0.749984]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.236576][G train loss: 0.890511]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.291690][G eval loss: 0.767744]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.232558][G train loss: 0.886953]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.291102][G eval loss: 0.761393]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.230603][G train loss: 0.883565]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.290755][G eval loss: 0.752594]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.232386][G train loss: 0.864671]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.290532][G eval loss: 0.740910]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.232134][G train loss: 0.857186]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.290105][G eval loss: 0.725331]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.230779][G train loss: 0.846634]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.289379][G eval loss: 0.715162]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.228301][G train loss: 0.833636]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.289127][G eval loss: 0.705800]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.232077][G train loss: 0.820916]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.288855][G eval loss: 0.702849]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.234845][G train loss: 0.799823]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.288530][G eval loss: 0.689412]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.233141][G train loss: 0.790295]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.287948][G eval loss: 0.677263]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.229786][G train loss: 0.775315]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.287694][G eval loss: 0.675767]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.229424][G train loss: 0.760703]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.288582][G eval loss: 0.660369]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.228988][G train loss: 0.748672]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.287294][G eval loss: 0.647726]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.228496][G train loss: 0.736161]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.295850][G eval loss: 0.592363]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.233183][G train loss: 0.693875]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.294377][G eval loss: 0.585884]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.233597][G train loss: 0.680164]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.293327][G eval loss: 0.583779]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.233450][G train loss: 0.672434]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.292595][G eval loss: 0.585629]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.239093][G train loss: 0.637041]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.291410][G eval loss: 0.597688]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.0045 lr_d=0.0015 -> score=0.889098\n",
      "\n",
      "----- 0056: lr_g=0.005, lr_d=0.0005 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.983262]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 1.085763]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.974966]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 1.077878]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.972596]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249983][G train loss: 1.075903]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.971803]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.075256]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.972467]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249967][G train loss: 1.075900]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.973388]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.076722]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.973974]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249940][G train loss: 1.077180]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 0.973794]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249919][G train loss: 1.076877]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.972600]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249894][G train loss: 1.075571]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249894][G eval loss: 0.970789]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249864][G train loss: 1.073671]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249865][G eval loss: 0.969249]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249827][G train loss: 1.072073]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249829][G eval loss: 0.968380]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249782][G train loss: 1.071177]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249782][G eval loss: 0.968018]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249725][G train loss: 1.070812]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249720][G eval loss: 0.967883]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249650][G train loss: 1.070683]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249641][G eval loss: 0.968051]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249556][G train loss: 1.070858]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249558][G eval loss: 0.968512]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249455][G train loss: 1.071324]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249444][G eval loss: 0.968845]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249323][G train loss: 1.071671]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249320][G eval loss: 0.968839]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249177][G train loss: 1.071693]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249176][G eval loss: 0.968364]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249011][G train loss: 1.071224]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249022][G eval loss: 0.967345]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248834][G train loss: 1.070163]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248842][G eval loss: 0.965676]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248638][G train loss: 1.068394]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248619][G eval loss: 0.962746]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248415][G train loss: 1.065264]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248335][G eval loss: 0.957577]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248149][G train loss: 1.059786]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248056][G eval loss: 0.948302]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247909][G train loss: 1.049943]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247864][G eval loss: 0.932287]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247809][G train loss: 1.033020]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247839][G eval loss: 0.912860]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.247940][G train loss: 1.012268]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.248026][G eval loss: 0.899421]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.248268][G train loss: 0.997239]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.248261][G eval loss: 0.899520]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.248500][G train loss: 0.995593]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.248231][G eval loss: 0.899579]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.005 lr_d=0.0005 -> score=1.147810\n",
      "\n",
      "----- 0056: lr_g=0.005, lr_d=0.0008 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250011][G eval loss: 0.981335]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.083836]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.974680]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.077592]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.973629]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.076936]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.973401]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249972][G train loss: 1.076853]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.973827]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249956][G train loss: 1.077259]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249951][G eval loss: 0.974162]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249930][G train loss: 1.077495]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.974089]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249891][G train loss: 1.077295]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249876][G eval loss: 0.973317]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249838][G train loss: 1.076400]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249821][G eval loss: 0.971812]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249769][G train loss: 1.074781]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249749][G eval loss: 0.969886]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249678][G train loss: 1.072767]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249650][G eval loss: 0.968404]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249554][G train loss: 1.071227]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249520][G eval loss: 0.967727]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249391][G train loss: 1.070521]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249341][G eval loss: 0.967623]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249173][G train loss: 1.070414]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249100][G eval loss: 0.967840]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248882][G train loss: 1.070635]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248771][G eval loss: 0.968480]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248494][G train loss: 1.071279]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248350][G eval loss: 0.969580]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248002][G train loss: 1.072376]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247821][G eval loss: 0.970787]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247388][G train loss: 1.073588]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247291][G eval loss: 0.971324]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246753][G train loss: 1.074144]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246467][G eval loss: 0.970965]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245831][G train loss: 1.073755]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245491][G eval loss: 0.969879]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244796][G train loss: 1.072496]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244453][G eval loss: 0.968085]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243761][G train loss: 1.070362]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243303][G eval loss: 0.965070]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242704][G train loss: 1.066733]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241967][G eval loss: 0.959119]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.241571][G train loss: 1.059775]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.240822][G eval loss: 0.947416]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.240849][G train loss: 1.046257]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.240461][G eval loss: 0.928494]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.241323][G train loss: 1.024277]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.240884][G eval loss: 0.907935]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.242056][G train loss: 1.001496]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.241886][G eval loss: 0.895650]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.242656][G train loss: 0.987957]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.242951][G eval loss: 0.896262]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.243486][G train loss: 0.986526]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.243028][G eval loss: 0.895993]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.243983][G train loss: 0.983061]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.241983][G eval loss: 0.885391]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243554][G train loss: 0.970031]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.237249][G eval loss: 0.869512]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.242466][G train loss: 0.947002]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.234017][G eval loss: 0.847412]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.240442][G train loss: 0.918085]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.230934][G eval loss: 0.824926]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.237619][G train loss: 0.892694]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.227891][G eval loss: 0.804856]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.235136][G train loss: 0.872163]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.225943][G eval loss: 0.785346]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.233920][G train loss: 0.850998]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.225819][G eval loss: 0.757344]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.234647][G train loss: 0.822189]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.231236][G eval loss: 0.720661]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.240531][G train loss: 0.784703]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.241436][G eval loss: 0.682875]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.245879][G train loss: 0.748613]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.247290][G eval loss: 0.658830]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.251209][G train loss: 0.719510]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.253205][G eval loss: 0.639946]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.255183][G train loss: 0.694146]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.256553][G eval loss: 0.613839]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.255022][G train loss: 0.674351]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.256214][G eval loss: 0.582160]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.253127][G train loss: 0.648754]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.254292][G eval loss: 0.548194]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.250091][G train loss: 0.618842]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.251636][G eval loss: 0.514499]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.246924][G train loss: 0.590070]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.248717][G eval loss: 0.491059]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.244201][G train loss: 0.571573]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.245483][G eval loss: 0.492778]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.240219][G train loss: 0.575253]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.247564][G eval loss: 0.503656]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.238654][G train loss: 0.591770]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.253504][G eval loss: 0.517778]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.241649][G train loss: 0.599908]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.264104][G eval loss: 0.510616]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.248680][G train loss: 0.593037]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.276490][G eval loss: 0.494613]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.256894][G train loss: 0.568472]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.277825][G eval loss: 0.491822]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.261136][G train loss: 0.555837]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.278225][G eval loss: 0.495803]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.260367][G train loss: 0.561498]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.277945][G eval loss: 0.505853]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.258958][G train loss: 0.573068]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.277295][G eval loss: 0.508025]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.257489][G train loss: 0.577251]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.276419][G eval loss: 0.499627]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.256142][G train loss: 0.569748]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.275350][G eval loss: 0.482878]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.254980][G train loss: 0.553111]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.274087][G eval loss: 0.462597]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.254029][G train loss: 0.533451]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.272072][G eval loss: 0.444828]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.253017][G train loss: 0.517437]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.269121][G eval loss: 0.433392]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.252168][G train loss: 0.508105]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.264708][G eval loss: 0.429821]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.251430][G train loss: 0.506687]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.259648][G eval loss: 0.435323]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.250573][G train loss: 0.513875]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.256538][G eval loss: 0.445938]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.250023][G train loss: 0.525816]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.255387][G eval loss: 0.458445]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.249630][G train loss: 0.539301]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.254485][G eval loss: 0.472462]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.249296][G train loss: 0.553663]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.253846][G eval loss: 0.486910]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.249258][G train loss: 0.568226]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.253528][G eval loss: 0.499735]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.249667][G train loss: 0.580869]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.253666][G eval loss: 0.508394]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.250445][G train loss: 0.588744]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.254001][G eval loss: 0.510903]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.251337][G train loss: 0.589885]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.254021][G eval loss: 0.510626]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.251869][G train loss: 0.587448]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.253656][G eval loss: 0.509939]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.251991][G train loss: 0.583959]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.252990][G eval loss: 0.508376]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.251789][G train loss: 0.580749]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.252326][G eval loss: 0.504683]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.251496][G train loss: 0.576641]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.251682][G eval loss: 0.499753]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.251071][G train loss: 0.570784]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.250958][G eval loss: 0.493880]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.250508][G train loss: 0.563523]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.250544][G eval loss: 0.486110]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.250285][G train loss: 0.555361]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.250550][G eval loss: 0.478365]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.250396][G train loss: 0.547230]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.250670][G eval loss: 0.471407]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.250560][G train loss: 0.540920]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.250741][G eval loss: 0.465520]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.250648][G train loss: 0.535783]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.250735][G eval loss: 0.460729]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.250639][G train loss: 0.531036]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.250601][G eval loss: 0.456553]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.250531][G train loss: 0.527613]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.250397][G eval loss: 0.453513]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.250315][G train loss: 0.524414]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.250014][G eval loss: 0.451333]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.249940][G train loss: 0.520915]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.249457][G eval loss: 0.449753]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.249385][G train loss: 0.517853]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.248781][G eval loss: 0.448543]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.248625][G train loss: 0.515811]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.248119][G eval loss: 0.449316]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.247728][G train loss: 0.515420]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.247679][G eval loss: 0.449656]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.005 lr_d=0.0008 -> score=0.697335\n",
      "\n",
      "----- 0056: lr_g=0.005, lr_d=0.001 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250020][G eval loss: 0.980158]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250017][G train loss: 1.082659]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.974504]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 1.077416]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.974342]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249975][G train loss: 1.077650]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.974466]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249963][G train loss: 1.077918]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249958][G eval loss: 0.974768]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249937][G train loss: 1.078200]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249921][G eval loss: 0.974787]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249891][G train loss: 1.078121]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249862][G eval loss: 0.974355]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249818][G train loss: 1.077560]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249780][G eval loss: 0.973408]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249717][G train loss: 1.076490]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249670][G eval loss: 0.971870]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249579][G train loss: 1.074839]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249522][G eval loss: 0.969939]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249393][G train loss: 1.072818]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249315][G eval loss: 0.968462]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249137][G train loss: 1.071283]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249019][G eval loss: 0.967869]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248777][G train loss: 1.070660]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248596][G eval loss: 0.967946]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248271][G train loss: 1.070730]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248003][G eval loss: 0.968575]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247571][G train loss: 1.071358]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247197][G eval loss: 0.969859]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246626][G train loss: 1.072642]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246175][G eval loss: 0.971017]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245447][G train loss: 1.073785]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.244748][G eval loss: 0.972124]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.243860][G train loss: 1.074871]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.242931][G eval loss: 0.973121]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.241891][G train loss: 1.075828]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.240794][G eval loss: 0.973813]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239602][G train loss: 1.076386]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.238434][G eval loss: 0.974126]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.237143][G train loss: 1.076304]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.236017][G eval loss: 0.973875]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.234765][G train loss: 1.075161]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.233676][G eval loss: 0.971549]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.232754][G train loss: 1.071071]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.231798][G eval loss: 0.963219]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.231651][G train loss: 1.059469]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.231549][G eval loss: 0.945116]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.232979][G train loss: 1.036028]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.233572][G eval loss: 0.918981]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235103][G train loss: 1.007126]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.236436][G eval loss: 0.895858]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.237340][G train loss: 0.982475]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.240619][G eval loss: 0.883804]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.240277][G train loss: 0.969392]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.242769][G eval loss: 0.887873]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.243854][G train loss: 0.967824]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.242864][G eval loss: 0.892303]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.005 lr_d=0.001 -> score=1.135167\n",
      "\n",
      "----- 0056: lr_g=0.005, lr_d=0.0012 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250032][G eval loss: 0.978978]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250029][G train loss: 1.081479]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.974306]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 1.077218]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.975018]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249969][G train loss: 1.078326]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.975501]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249954][G train loss: 1.078953]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249943][G eval loss: 0.975733]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249916][G train loss: 1.079165]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249883][G eval loss: 0.975462]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249841][G train loss: 1.078795]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249790][G eval loss: 0.974714]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249730][G train loss: 1.077919]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249659][G eval loss: 0.974162]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249568][G train loss: 1.077243]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249490][G eval loss: 0.972800]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249357][G train loss: 1.075767]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249237][G eval loss: 0.970997]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249038][G train loss: 1.073875]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248827][G eval loss: 0.969558]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248537][G train loss: 1.072375]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248187][G eval loss: 0.969040]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247795][G train loss: 1.071825]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247310][G eval loss: 0.969444]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246777][G train loss: 1.072217]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246319][G eval loss: 0.970312]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245602][G train loss: 1.073083]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244756][G eval loss: 0.971546]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243823][G train loss: 1.074297]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242701][G eval loss: 0.973297]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241514][G train loss: 1.076021]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240066][G eval loss: 0.975364]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238579][G train loss: 1.078052]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236887][G eval loss: 0.977306]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.235032][G train loss: 1.079949]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232949][G eval loss: 0.979133]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230713][G train loss: 1.081576]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.228228][G eval loss: 0.980938]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.225700][G train loss: 1.082930]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.223109][G eval loss: 0.981783]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.220618][G train loss: 1.082496]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.218593][G eval loss: 0.979581]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.217207][G train loss: 1.076225]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.216205][G eval loss: 0.968542]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.218586][G train loss: 1.055625]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.218570][G eval loss: 0.940972]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.224064][G train loss: 1.021119]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.225905][G eval loss: 0.903088]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.229118][G train loss: 0.985942]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.233708][G eval loss: 0.873651]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.235703][G train loss: 0.955089]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.239645][G eval loss: 0.868954]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.243777][G train loss: 0.942741]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.242575][G eval loss: 0.882383]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.248594][G train loss: 0.949158]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.243029][G eval loss: 0.895247]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.252129][G train loss: 0.954840]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.242250][G eval loss: 0.900580]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.253140][G train loss: 0.957495]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.240126][G eval loss: 0.897329]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.251148][G train loss: 0.952620]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.237329][G eval loss: 0.883671]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.247635][G train loss: 0.939292]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.234580][G eval loss: 0.864039]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.247035][G train loss: 0.916079]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.216315][G eval loss: 0.873452]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.243740][G train loss: 0.899137]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.213295][G eval loss: 0.866724]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.235036][G train loss: 0.896998]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.217227][G eval loss: 0.851456]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.236417][G train loss: 0.880132]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.238762][G eval loss: 0.805169]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.240802][G train loss: 0.866112]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.248453][G eval loss: 0.785933]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.247040][G train loss: 0.846931]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.251901][G eval loss: 0.781211]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.251153][G train loss: 0.832823]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.262163][G eval loss: 0.765973]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.263585][G train loss: 0.806706]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.261505][G eval loss: 0.770174]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.262745][G train loss: 0.807224]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.260099][G eval loss: 0.773360]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.257296][G train loss: 0.813478]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.258779][G eval loss: 0.779614]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.250325][G train loss: 0.829056]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.258520][G eval loss: 0.788050]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.247572][G train loss: 0.840257]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.260424][G eval loss: 0.794294]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.245329][G train loss: 0.845869]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.263312][G eval loss: 0.794193]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.241571][G train loss: 0.850393]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.265905][G eval loss: 0.797817]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.240240][G train loss: 0.851782]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.268429][G eval loss: 0.795903]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.240632][G train loss: 0.848459]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.269846][G eval loss: 0.791743]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.238549][G train loss: 0.844463]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.270490][G eval loss: 0.784512]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.236984][G train loss: 0.833510]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.270689][G eval loss: 0.767930]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.235332][G train loss: 0.818804]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.270884][G eval loss: 0.746080]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.233826][G train loss: 0.800653]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.271227][G eval loss: 0.721335]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.232806][G train loss: 0.782206]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.271630][G eval loss: 0.697186]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.230001][G train loss: 0.771984]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.272358][G eval loss: 0.681853]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.227353][G train loss: 0.771998]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.272621][G eval loss: 0.667593]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.227720][G train loss: 0.766284]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.272223][G eval loss: 0.659353]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.228672][G train loss: 0.763399]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.265820][G eval loss: 0.665875]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.224120][G train loss: 0.778679]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.259458][G eval loss: 0.683195]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.225037][G train loss: 0.774005]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.259415][G eval loss: 0.682012]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.224064][G train loss: 0.771681]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.266559][G eval loss: 0.657918]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.225505][G train loss: 0.758890]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.269751][G eval loss: 0.648520]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.228556][G train loss: 0.736123]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.279426][G eval loss: 0.601520]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.233790][G train loss: 0.715279]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.280212][G eval loss: 0.606566]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.240126][G train loss: 0.690183]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.281625][G eval loss: 0.619329]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.005 lr_d=0.0012 -> score=0.900954\n",
      "\n",
      "----- 0056: lr_g=0.005, lr_d=0.0015 -----\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.113681]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250056][G eval loss: 0.977204]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250053][G train loss: 1.079705]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.973865]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 1.076777]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.975863]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249965][G train loss: 1.079170]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 0.976870]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 1.080322]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.977008]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249948][G train loss: 1.080440]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 0.976981]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249860][G train loss: 1.080315]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249794][G eval loss: 0.976500]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249734][G train loss: 1.079706]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249625][G eval loss: 0.975696]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249530][G train loss: 1.078778]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249376][G eval loss: 0.974053]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249231][G train loss: 1.077020]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.248985][G eval loss: 0.972121]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248766][G train loss: 1.074997]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248387][G eval loss: 0.970935]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248058][G train loss: 1.073749]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247504][G eval loss: 0.971068]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246995][G train loss: 1.073846]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246232][G eval loss: 0.972450]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245462][G train loss: 1.075207]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244258][G eval loss: 0.975127]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.243109][G train loss: 1.077863]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.241666][G eval loss: 0.978578]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239970][G train loss: 1.081292]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.238339][G eval loss: 0.982630]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235872][G train loss: 1.085318]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.233855][G eval loss: 0.987131]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.230462][G train loss: 1.089772]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.228112][G eval loss: 0.991521]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223683][G train loss: 1.094078]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.220414][G eval loss: 0.996784]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.215170][G train loss: 1.099083]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.210871][G eval loss: 1.002614]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.205143][G train loss: 1.104413]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.200005][G eval loss: 1.008398]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.194146][G train loss: 1.108905]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.190302][G eval loss: 1.008750]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.186243][G train loss: 1.102650]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.188179][G eval loss: 0.992676]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.198795][G train loss: 1.057600]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.198179][G eval loss: 0.948075]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.213002][G train loss: 1.010089]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.219609][G eval loss: 0.895148]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.225265][G train loss: 0.970587]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.231426][G eval loss: 0.873591]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.238238][G train loss: 0.939545]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.236945][G eval loss: 0.882657]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.247991][G train loss: 0.935362]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.239850][G eval loss: 0.904329]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1.pth\n",
      "Done training LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 0056 lr_g=0.005 lr_d=0.0015 -> score=1.144179\n",
      "\n",
      ">>> Best config for 0056: lr_g=0.005, lr_d=0.0008, score=0.697335\n",
      "\n",
      "========== Grid search for 2330 ==========\n",
      "\n",
      "----- 2330: lr_g=0.003, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.496703]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.461003]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.485410]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 0.449789]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.480912]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.445371]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.480150]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.444718]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.479787]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 0.444442]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 0.478982]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249956][G train loss: 0.443650]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249955][G eval loss: 0.478180]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249940][G train loss: 0.442811]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.477700]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249921][G train loss: 0.442260]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249924][G eval loss: 0.477545]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249899][G train loss: 0.442030]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249902][G eval loss: 0.477579]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249871][G train loss: 0.441993]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249875][G eval loss: 0.477657]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.003 lr_d=0.0005 -> score=0.727532\n",
      "\n",
      "----- 2330: lr_g=0.003, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250008][G eval loss: 0.494774]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250006][G train loss: 0.459073]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.485117]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 0.449496]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249984][G eval loss: 0.481920]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249979][G train loss: 0.446379]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.481724]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.446292]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249963][G eval loss: 0.481136]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249950][G train loss: 0.445790]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249939][G eval loss: 0.479772]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249919][G train loss: 0.444441]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249908][G eval loss: 0.478342]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249879][G train loss: 0.442972]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249867][G eval loss: 0.477388]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249826][G train loss: 0.441949]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249812][G eval loss: 0.476892]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249756][G train loss: 0.441378]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249734][G eval loss: 0.477039]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249656][G train loss: 0.441454]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249621][G eval loss: 0.477276]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249515][G train loss: 0.441632]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249467][G eval loss: 0.477472]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249320][G train loss: 0.441787]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249257][G eval loss: 0.477586]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249053][G train loss: 0.441877]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249013][G eval loss: 0.477554]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248747][G train loss: 0.441840]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248734][G eval loss: 0.477506]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248388][G train loss: 0.441810]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248377][G eval loss: 0.477537]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247929][G train loss: 0.441879]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247908][G eval loss: 0.477844]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247327][G train loss: 0.442232]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247322][G eval loss: 0.478549]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246576][G train loss: 0.442984]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246673][G eval loss: 0.479536]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245720][G train loss: 0.444016]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245831][G eval loss: 0.480281]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244634][G train loss: 0.444789]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244709][G eval loss: 0.481239]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243220][G train loss: 0.445755]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243366][G eval loss: 0.482347]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241518][G train loss: 0.446852]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241661][G eval loss: 0.483685]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239419][G train loss: 0.448159]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239572][G eval loss: 0.485346]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.236878][G train loss: 0.449780]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.237093][G eval loss: 0.487276]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.233893][G train loss: 0.451669]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234389][G eval loss: 0.489143]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.230625][G train loss: 0.453503]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231262][G eval loss: 0.491047]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.226924][G train loss: 0.455382]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.227717][G eval loss: 0.492972]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.222761][G train loss: 0.457292]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.223623][G eval loss: 0.495108]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.218038][G train loss: 0.459415]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.218889][G eval loss: 0.497624]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.003 lr_d=0.0008 -> score=0.716513\n",
      "\n",
      "----- 2330: lr_g=0.003, lr_d=0.001 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250017][G eval loss: 0.493594]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250014][G train loss: 0.457893]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.484930]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 0.449309]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.482615]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249972][G train loss: 0.447075]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.482775]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249958][G train loss: 0.447343]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249946][G eval loss: 0.482075]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249925][G train loss: 0.446730]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249905][G eval loss: 0.480408]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249872][G train loss: 0.445076]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249850][G eval loss: 0.478654]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249800][G train loss: 0.443284]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249773][G eval loss: 0.477759]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249700][G train loss: 0.442321]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249660][G eval loss: 0.477339]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249553][G train loss: 0.441826]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249487][G eval loss: 0.477289]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249331][G train loss: 0.441706]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249229][G eval loss: 0.477466]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248997][G train loss: 0.441825]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248901][G eval loss: 0.477681]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.003 lr_d=0.001 -> score=0.726583\n",
      "\n",
      "----- 2330: lr_g=0.003, lr_d=0.0012 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250028][G eval loss: 0.492412]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250026][G train loss: 0.456711]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 0.484724]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249987][G train loss: 0.449103]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.483284]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249965][G train loss: 0.447743]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249963][G eval loss: 0.483808]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249946][G train loss: 0.448376]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249924][G eval loss: 0.483054]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249895][G train loss: 0.447709]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 0.481155]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249812][G train loss: 0.445824]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249764][G eval loss: 0.479301]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249686][G train loss: 0.443933]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249619][G eval loss: 0.477872]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249497][G train loss: 0.442435]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249395][G eval loss: 0.477062]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249207][G train loss: 0.441550]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249067][G eval loss: 0.476867]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248773][G train loss: 0.441286]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248603][G eval loss: 0.477068]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248171][G train loss: 0.441430]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247985][G eval loss: 0.477742]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247370][G train loss: 0.442063]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247125][G eval loss: 0.478953]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246254][G train loss: 0.443253]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246097][G eval loss: 0.480356]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.244878][G train loss: 0.444653]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244589][G eval loss: 0.481546]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.242915][G train loss: 0.445867]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242514][G eval loss: 0.483177]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.240250][G train loss: 0.447540]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.239984][G eval loss: 0.485130]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.236957][G train loss: 0.449546]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.236801][G eval loss: 0.487459]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.232855][G train loss: 0.451928]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.232954][G eval loss: 0.489843]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.227982][G train loss: 0.454363]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.227963][G eval loss: 0.492941]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.221950][G train loss: 0.457498]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.221746][G eval loss: 0.496376]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.214736][G train loss: 0.460948]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.213863][G eval loss: 0.500961]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.205968][G train loss: 0.465523]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.204327][G eval loss: 0.506760]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.195606][G train loss: 0.471295]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.193247][G eval loss: 0.513433]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.183898][G train loss: 0.477930]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.181193][G eval loss: 0.520427]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.171083][G train loss: 0.484872]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.167488][G eval loss: 0.528851]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.157348][G train loss: 0.493242]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.152354][G eval loss: 0.538934]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.143707][G train loss: 0.503196]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.137184][G eval loss: 0.550820]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.130795][G train loss: 0.514878]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.124278][G eval loss: 0.561783]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.119834][G train loss: 0.525357]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.117260][G eval loss: 0.563612]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.114698][G train loss: 0.525474]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.157357][G eval loss: 0.477946]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.164103][G train loss: 0.422620]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.295758][G eval loss: 0.315055]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.304484][G train loss: 0.271223]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.357870][G eval loss: 0.288750]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.358255][G train loss: 0.252178]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.378755][G eval loss: 0.286056]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.377711][G train loss: 0.250126]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.388827][G eval loss: 0.282519]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.387667][G train loss: 0.246791]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.394603][G eval loss: 0.277931]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.394135][G train loss: 0.242404]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.398125][G eval loss: 0.273495]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.398422][G train loss: 0.238104]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.400139][G eval loss: 0.269964]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.400759][G train loss: 0.234700]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.400991][G eval loss: 0.267733]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.402980][G train loss: 0.232492]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.401105][G eval loss: 0.266555]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.404912][G train loss: 0.231173]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.400523][G eval loss: 0.265623]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.404652][G train loss: 0.229905]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.399301][G eval loss: 0.264263]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.403684][G train loss: 0.228106]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.397535][G eval loss: 0.262293]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.402091][G train loss: 0.225715]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.395278][G eval loss: 0.259733]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.399559][G train loss: 0.222843]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.392578][G eval loss: 0.256802]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.394862][G train loss: 0.219725]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.389612][G eval loss: 0.253814]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.389385][G train loss: 0.216629]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.386173][G eval loss: 0.251115]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.374743][G train loss: 0.228357]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.382407][G eval loss: 0.250503]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.375199][G train loss: 0.224167]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.378212][G eval loss: 0.250350]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.375001][G train loss: 0.214429]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.373779][G eval loss: 0.250688]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.373691][G train loss: 0.212340]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.369054][G eval loss: 0.250931]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.368994][G train loss: 0.212051]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.363443][G eval loss: 0.251388]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.363390][G train loss: 0.211825]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.030777][G eval loss: 0.894638]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.030749][G train loss: 0.854175]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.030875][G eval loss: 0.903439]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.030859][G train loss: 0.861802]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.031117][G eval loss: 0.910092]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.031110][G train loss: 0.867030]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.031255][G eval loss: 0.916563]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.031257][G train loss: 0.871909]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.031257][G eval loss: 0.922983]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.031264][G train loss: 0.876707]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.031113][G eval loss: 0.929332]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.031124][G train loss: 0.881635]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.030805][G eval loss: 0.935470]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.030816][G train loss: 0.886954]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.030344][G eval loss: 0.941225]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.030354][G train loss: 0.892983]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.029752][G eval loss: 0.947114]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.029760][G train loss: 0.899802]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.029060][G eval loss: 0.953016]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.029074][G train loss: 0.907101]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.028272][G eval loss: 0.958147]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.028303][G train loss: 0.915111]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.027390][G eval loss: 0.964193]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.027469][G train loss: 0.924018]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.026431][G eval loss: 0.971577]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.026639][G train loss: 0.934065]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.025438][G eval loss: 0.979593]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.025809][G train loss: 0.944521]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.024403][G eval loss: 0.988801]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.024883][G train loss: 0.955353]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.023330][G eval loss: 0.997995]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.023843][G train loss: 0.966445]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.022229][G eval loss: 1.006924]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.022699][G train loss: 0.976277]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.021110][G eval loss: 1.015367]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.021474][G train loss: 0.985017]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.019979][G eval loss: 1.023122]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.020211][G train loss: 0.992681]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.018848][G eval loss: 1.029868]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.018951][G train loss: 0.999162]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.017721][G eval loss: 1.037064]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.017763][G train loss: 1.004594]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.016604][G eval loss: 1.042878]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.016622][G train loss: 1.009036]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.015504][G eval loss: 1.048312]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.015513][G train loss: 1.013855]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.014428][G eval loss: 1.053588]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.014433][G train loss: 1.018749]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.013382][G eval loss: 1.058806]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.013385][G train loss: 1.023682]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.012370][G eval loss: 1.064108]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.012374][G train loss: 1.028451]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.011399][G eval loss: 1.069645]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.011403][G train loss: 1.033380]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.010472][G eval loss: 1.075154]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.010476][G train loss: 1.038509]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.009611][G eval loss: 1.080423]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.009614][G train loss: 1.043504]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.008800][G eval loss: 1.085530]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.008803][G train loss: 1.048420]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.008038][G eval loss: 1.090348]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.008041][G train loss: 1.052873]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.007325][G eval loss: 1.094887]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.007327][G train loss: 1.057239]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.006661][G eval loss: 1.099183]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.006663][G train loss: 1.061489]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.006045][G eval loss: 1.103053]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.006047][G train loss: 1.065434]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.005476][G eval loss: 1.106480]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.005478][G train loss: 1.069141]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.004953][G eval loss: 1.109472]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.004954][G train loss: 1.072714]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.313366][G eval loss: 0.400300]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.333926][G train loss: 0.315721]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.315985][G eval loss: 0.400144]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.378081][G train loss: 0.218652]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.353291][G eval loss: 0.320399]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.382083][G train loss: 0.210691]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.355828][G eval loss: 0.320182]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.391519][G train loss: 0.199436]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.358097][G eval loss: 0.320080]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.396412][G train loss: 0.188299]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.360199][G eval loss: 0.320167]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.405450][G train loss: 0.180144]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.391462][G eval loss: 0.241421]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.408331][G train loss: 0.175108]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.398385][G eval loss: 0.239826]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.417973][G train loss: 0.161191]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.400908][G eval loss: 0.238829]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.426928][G train loss: 0.141335]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.408724][G eval loss: 0.188185]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.430253][G train loss: 0.139283]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.433999][G eval loss: 0.156849]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.440679][G train loss: 0.118661]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.440453][G eval loss: 0.153529]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.442013][G train loss: 0.116563]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.443040][G eval loss: 0.151329]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.443043][G train loss: 0.114427]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.443893][G eval loss: 0.149830]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.443896][G train loss: 0.112941]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.444579][G eval loss: 0.148969]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.444583][G train loss: 0.111823]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.445109][G eval loss: 0.148327]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.445114][G train loss: 0.111271]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.445489][G eval loss: 0.147564]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.445494][G train loss: 0.110531]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.445726][G eval loss: 0.146625]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.445734][G train loss: 0.109533]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.445826][G eval loss: 0.145208]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.445835][G train loss: 0.108072]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.445796][G eval loss: 0.143563]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.445809][G train loss: 0.106106]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.445634][G eval loss: 0.142165]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.445664][G train loss: 0.104441]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.445347][G eval loss: 0.141274]\n",
      "[Epoch 111/200][Batch 1/1][D train loss: 0.445421][G train loss: 0.103393]\n",
      "[Epoch 111/200][Batch 1/1][D eval loss: 0.444936][G eval loss: 0.140946]\n",
      "[Epoch 112/200][Batch 1/1][D train loss: 0.446008][G train loss: 0.102728]\n",
      "[Epoch 112/200][Batch 1/1][D eval loss: 0.444510][G eval loss: 0.141044]\n",
      "[Epoch 113/200][Batch 1/1][D train loss: 0.454369][G train loss: 0.102628]\n",
      "[Epoch 113/200][Batch 1/1][D eval loss: 0.443960][G eval loss: 0.141697]\n",
      "[Epoch 114/200][Batch 1/1][D train loss: 0.453827][G train loss: 0.102922]\n",
      "[Epoch 114/200][Batch 1/1][D eval loss: 0.443289][G eval loss: 0.142295]\n",
      "[Epoch 115/200][Batch 1/1][D train loss: 0.453184][G train loss: 0.102799]\n",
      "[Epoch 115/200][Batch 1/1][D eval loss: 0.442498][G eval loss: 0.142570]\n",
      "[Epoch 116/200][Batch 1/1][D train loss: 0.454977][G train loss: 0.102566]\n",
      "[Epoch 116/200][Batch 1/1][D eval loss: 0.441693][G eval loss: 0.142483]\n",
      "[Epoch 117/200][Batch 1/1][D train loss: 0.455064][G train loss: 0.102175]\n",
      "[Epoch 117/200][Batch 1/1][D eval loss: 0.440874][G eval loss: 0.141546]\n",
      "[Epoch 118/200][Batch 1/1][D train loss: 0.450866][G train loss: 0.101312]\n",
      "[Epoch 118/200][Batch 1/1][D eval loss: 0.439943][G eval loss: 0.141014]\n",
      "[Epoch 119/200][Batch 1/1][D train loss: 0.449829][G train loss: 0.100647]\n",
      "[Epoch 119/200][Batch 1/1][D eval loss: 0.438889][G eval loss: 0.140644]\n",
      "[Epoch 120/200][Batch 1/1][D train loss: 0.448774][G train loss: 0.100025]\n",
      "[Epoch 120/200][Batch 1/1][D eval loss: 0.437712][G eval loss: 0.140298]\n",
      "[Epoch 121/200][Batch 1/1][D train loss: 0.447598][G train loss: 0.099569]\n",
      "[Epoch 121/200][Batch 1/1][D eval loss: 0.436409][G eval loss: 0.140033]\n",
      "[Epoch 122/200][Batch 1/1][D train loss: 0.446298][G train loss: 0.099186]\n",
      "[Epoch 122/200][Batch 1/1][D eval loss: 0.434983][G eval loss: 0.139650]\n",
      "[Epoch 123/200][Batch 1/1][D train loss: 0.444872][G train loss: 0.098990]\n",
      "[Epoch 123/200][Batch 1/1][D eval loss: 0.433430][G eval loss: 0.139304]\n",
      "[Epoch 124/200][Batch 1/1][D train loss: 0.443320][G train loss: 0.098990]\n",
      "[Epoch 124/200][Batch 1/1][D eval loss: 0.431750][G eval loss: 0.139783]\n",
      "[Epoch 125/200][Batch 1/1][D train loss: 0.441642][G train loss: 0.099175]\n",
      "[Epoch 125/200][Batch 1/1][D eval loss: 0.429942][G eval loss: 0.139824]\n",
      "[Epoch 126/200][Batch 1/1][D train loss: 0.439835][G train loss: 0.099256]\n",
      "[Epoch 126/200][Batch 1/1][D eval loss: 0.428008][G eval loss: 0.140088]\n",
      "[Epoch 127/200][Batch 1/1][D train loss: 0.437901][G train loss: 0.099258]\n",
      "[Epoch 127/200][Batch 1/1][D eval loss: 0.425943][G eval loss: 0.140084]\n",
      "[Epoch 128/200][Batch 1/1][D train loss: 0.435837][G train loss: 0.099171]\n",
      "[Epoch 128/200][Batch 1/1][D eval loss: 0.423851][G eval loss: 0.139793]\n",
      "[Epoch 129/200][Batch 1/1][D train loss: 0.433745][G train loss: 0.098900]\n",
      "[Epoch 129/200][Batch 1/1][D eval loss: 0.421672][G eval loss: 0.139425]\n",
      "[Epoch 130/200][Batch 1/1][D train loss: 0.431568][G train loss: 0.098465]\n",
      "[Epoch 130/200][Batch 1/1][D eval loss: 0.419367][G eval loss: 0.139304]\n",
      "[Epoch 131/200][Batch 1/1][D train loss: 0.429263][G train loss: 0.098078]\n",
      "[Epoch 131/200][Batch 1/1][D eval loss: 0.416930][G eval loss: 0.139540]\n",
      "[Epoch 132/200][Batch 1/1][D train loss: 0.426828][G train loss: 0.097843]\n",
      "[Epoch 132/200][Batch 1/1][D eval loss: 0.414360][G eval loss: 0.139947]\n",
      "[Epoch 133/200][Batch 1/1][D train loss: 0.424261][G train loss: 0.097837]\n",
      "[Epoch 133/200][Batch 1/1][D eval loss: 0.411655][G eval loss: 0.140277]\n",
      "[Epoch 134/200][Batch 1/1][D train loss: 0.421559][G train loss: 0.098117]\n",
      "[Epoch 134/200][Batch 1/1][D eval loss: 0.408813][G eval loss: 0.140942]\n",
      "[Epoch 135/200][Batch 1/1][D train loss: 0.418722][G train loss: 0.098903]\n",
      "[Epoch 135/200][Batch 1/1][D eval loss: 0.405832][G eval loss: 0.142053]\n",
      "[Epoch 136/200][Batch 1/1][D train loss: 0.415745][G train loss: 0.099996]\n",
      "[Epoch 136/200][Batch 1/1][D eval loss: 0.402713][G eval loss: 0.143197]\n",
      "[Epoch 137/200][Batch 1/1][D train loss: 0.412629][G train loss: 0.100973]\n",
      "[Epoch 137/200][Batch 1/1][D eval loss: 0.399442][G eval loss: 0.144219]\n",
      "[Epoch 138/200][Batch 1/1][D train loss: 0.409371][G train loss: 0.101801]\n",
      "[Epoch 138/200][Batch 1/1][D eval loss: 0.396040][G eval loss: 0.145095]\n",
      "[Epoch 139/200][Batch 1/1][D train loss: 0.405969][G train loss: 0.102409]\n",
      "[Epoch 139/200][Batch 1/1][D eval loss: 0.392489][G eval loss: 0.146043]\n",
      "[Epoch 140/200][Batch 1/1][D train loss: 0.402423][G train loss: 0.103002]\n",
      "[Epoch 140/200][Batch 1/1][D eval loss: 0.388795][G eval loss: 0.146779]\n",
      "[Epoch 141/200][Batch 1/1][D train loss: 0.398737][G train loss: 0.103723]\n",
      "[Epoch 141/200][Batch 1/1][D eval loss: 0.384960][G eval loss: 0.147746]\n",
      "[Epoch 142/200][Batch 1/1][D train loss: 0.394905][G train loss: 0.104620]\n",
      "[Epoch 142/200][Batch 1/1][D eval loss: 0.380984][G eval loss: 0.148960]\n",
      "[Epoch 143/200][Batch 1/1][D train loss: 0.390935][G train loss: 0.105772]\n",
      "[Epoch 143/200][Batch 1/1][D eval loss: 0.376873][G eval loss: 0.150314]\n",
      "[Epoch 144/200][Batch 1/1][D train loss: 0.386759][G train loss: 0.107084]\n",
      "[Epoch 144/200][Batch 1/1][D eval loss: 0.372719][G eval loss: 0.151992]\n",
      "[Epoch 145/200][Batch 1/1][D train loss: 0.373388][G train loss: 0.108643]\n",
      "[Epoch 145/200][Batch 1/1][D eval loss: 0.368470][G eval loss: 0.154020]\n",
      "[Epoch 146/200][Batch 1/1][D train loss: 0.368896][G train loss: 0.110538]\n",
      "[Epoch 146/200][Batch 1/1][D eval loss: 0.364252][G eval loss: 0.156468]\n",
      "[Epoch 147/200][Batch 1/1][D train loss: 0.364668][G train loss: 0.112635]\n",
      "[Epoch 147/200][Batch 1/1][D eval loss: 0.359913][G eval loss: 0.158909]\n",
      "[Epoch 148/200][Batch 1/1][D train loss: 0.360252][G train loss: 0.114905]\n",
      "[Epoch 148/200][Batch 1/1][D eval loss: 0.355462][G eval loss: 0.161488]\n",
      "[Epoch 149/200][Batch 1/1][D train loss: 0.355823][G train loss: 0.117439]\n",
      "[Epoch 149/200][Batch 1/1][D eval loss: 0.350908][G eval loss: 0.164316]\n",
      "[Epoch 150/200][Batch 1/1][D train loss: 0.351304][G train loss: 0.120114]\n",
      "[Epoch 150/200][Batch 1/1][D eval loss: 0.346254][G eval loss: 0.167297]\n",
      "[Epoch 151/200][Batch 1/1][D train loss: 0.346646][G train loss: 0.122973]\n",
      "[Epoch 151/200][Batch 1/1][D eval loss: 0.341582][G eval loss: 0.170299]\n",
      "[Epoch 152/200][Batch 1/1][D train loss: 0.335561][G train loss: 0.145076]\n",
      "[Epoch 152/200][Batch 1/1][D eval loss: 0.178542][G eval loss: 0.651789]\n",
      "[Epoch 153/200][Batch 1/1][D train loss: 0.199260][G train loss: 0.549193]\n",
      "[Epoch 153/200][Batch 1/1][D eval loss: 0.126015][G eval loss: 0.811954]\n",
      "[Epoch 154/200][Batch 1/1][D train loss: 0.160510][G train loss: 0.665028]\n",
      "[Epoch 154/200][Batch 1/1][D eval loss: 0.125648][G eval loss: 0.813036]\n",
      "[Epoch 155/200][Batch 1/1][D train loss: 0.124988][G train loss: 0.766090]\n",
      "[Epoch 155/200][Batch 1/1][D eval loss: 0.125327][G eval loss: 0.814282]\n",
      "[Epoch 156/200][Batch 1/1][D train loss: 0.122666][G train loss: 0.779446]\n",
      "[Epoch 156/200][Batch 1/1][D eval loss: 0.125101][G eval loss: 0.815720]\n",
      "[Epoch 157/200][Batch 1/1][D train loss: 0.135894][G train loss: 0.731772]\n",
      "[Epoch 157/200][Batch 1/1][D eval loss: 0.124927][G eval loss: 0.817459]\n",
      "[Epoch 158/200][Batch 1/1][D train loss: 0.162748][G train loss: 0.652124]\n",
      "[Epoch 158/200][Batch 1/1][D eval loss: 0.148320][G eval loss: 0.740884]\n",
      "[Epoch 159/200][Batch 1/1][D train loss: 0.178806][G train loss: 0.598015]\n",
      "[Epoch 159/200][Batch 1/1][D eval loss: 0.170459][G eval loss: 0.665394]\n",
      "[Epoch 160/200][Batch 1/1][D train loss: 0.182795][G train loss: 0.581931]\n",
      "[Epoch 160/200][Batch 1/1][D eval loss: 0.192012][G eval loss: 0.590500]\n",
      "[Epoch 161/200][Batch 1/1][D train loss: 0.192221][G train loss: 0.547467]\n",
      "[Epoch 161/200][Batch 1/1][D eval loss: 0.190668][G eval loss: 0.593447]\n",
      "[Epoch 162/200][Batch 1/1][D train loss: 0.191028][G train loss: 0.550218]\n",
      "[Epoch 162/200][Batch 1/1][D eval loss: 0.146947][G eval loss: 0.742724]\n",
      "[Epoch 163/200][Batch 1/1][D train loss: 0.174612][G train loss: 0.607913]\n",
      "[Epoch 163/200][Batch 1/1][D eval loss: 0.229512][G eval loss: 0.447444]\n",
      "[Epoch 164/200][Batch 1/1][D train loss: 0.215935][G train loss: 0.449917]\n",
      "[Epoch 164/200][Batch 1/1][D eval loss: 0.288625][G eval loss: 0.225236]\n",
      "[Epoch 165/200][Batch 1/1][D train loss: 0.289248][G train loss: 0.179065]\n",
      "[Epoch 165/200][Batch 1/1][D eval loss: 0.285079][G eval loss: 0.231181]\n",
      "[Epoch 166/200][Batch 1/1][D train loss: 0.285726][G train loss: 0.184980]\n",
      "[Epoch 166/200][Batch 1/1][D eval loss: 0.281597][G eval loss: 0.237354]\n",
      "[Epoch 167/200][Batch 1/1][D train loss: 0.282252][G train loss: 0.191078]\n",
      "[Epoch 167/200][Batch 1/1][D eval loss: 0.278194][G eval loss: 0.243986]\n",
      "[Epoch 168/200][Batch 1/1][D train loss: 0.278861][G train loss: 0.197445]\n",
      "[Epoch 168/200][Batch 1/1][D eval loss: 0.274876][G eval loss: 0.250712]\n",
      "[Epoch 169/200][Batch 1/1][D train loss: 0.275563][G train loss: 0.204068]\n",
      "[Epoch 169/200][Batch 1/1][D eval loss: 0.271655][G eval loss: 0.257465]\n",
      "[Epoch 170/200][Batch 1/1][D train loss: 0.272331][G train loss: 0.210877]\n",
      "[Epoch 170/200][Batch 1/1][D eval loss: 0.268724][G eval loss: 0.264333]\n",
      "[Epoch 171/200][Batch 1/1][D train loss: 0.269396][G train loss: 0.217747]\n",
      "[Epoch 171/200][Batch 1/1][D eval loss: 0.266040][G eval loss: 0.270996]\n",
      "[Epoch 172/200][Batch 1/1][D train loss: 0.266690][G train loss: 0.224510]\n",
      "[Epoch 172/200][Batch 1/1][D eval loss: 0.263378][G eval loss: 0.277987]\n",
      "[Epoch 173/200][Batch 1/1][D train loss: 0.264083][G train loss: 0.231536]\n",
      "[Epoch 173/200][Batch 1/1][D eval loss: 0.260810][G eval loss: 0.285397]\n",
      "[Epoch 174/200][Batch 1/1][D train loss: 0.261523][G train loss: 0.238872]\n",
      "[Epoch 174/200][Batch 1/1][D eval loss: 0.258193][G eval loss: 0.292883]\n",
      "[Epoch 175/200][Batch 1/1][D train loss: 0.259063][G train loss: 0.246353]\n",
      "[Epoch 175/200][Batch 1/1][D eval loss: 0.255973][G eval loss: 0.300125]\n",
      "[Epoch 176/200][Batch 1/1][D train loss: 0.256956][G train loss: 0.253617]\n",
      "[Epoch 176/200][Batch 1/1][D eval loss: 0.254319][G eval loss: 0.306902]\n",
      "[Epoch 177/200][Batch 1/1][D train loss: 0.255303][G train loss: 0.260495]\n",
      "[Epoch 177/200][Batch 1/1][D eval loss: 0.252744][G eval loss: 0.314124]\n",
      "[Epoch 178/200][Batch 1/1][D train loss: 0.253660][G train loss: 0.267533]\n",
      "[Epoch 178/200][Batch 1/1][D eval loss: 0.251284][G eval loss: 0.321233]\n",
      "[Epoch 179/200][Batch 1/1][D train loss: 0.252224][G train loss: 0.274739]\n",
      "[Epoch 179/200][Batch 1/1][D eval loss: 0.249881][G eval loss: 0.328993]\n",
      "[Epoch 180/200][Batch 1/1][D train loss: 0.250793][G train loss: 0.282238]\n",
      "[Epoch 180/200][Batch 1/1][D eval loss: 0.248596][G eval loss: 0.336769]\n",
      "[Epoch 181/200][Batch 1/1][D train loss: 0.249536][G train loss: 0.290090]\n",
      "[Epoch 181/200][Batch 1/1][D eval loss: 0.247372][G eval loss: 0.344681]\n",
      "[Epoch 182/200][Batch 1/1][D train loss: 0.248377][G train loss: 0.297907]\n",
      "[Epoch 182/200][Batch 1/1][D eval loss: 0.246260][G eval loss: 0.352626]\n",
      "[Epoch 183/200][Batch 1/1][D train loss: 0.247359][G train loss: 0.305677]\n",
      "[Epoch 183/200][Batch 1/1][D eval loss: 0.245352][G eval loss: 0.360144]\n",
      "[Epoch 184/200][Batch 1/1][D train loss: 0.246461][G train loss: 0.313209]\n",
      "[Epoch 184/200][Batch 1/1][D eval loss: 0.244525][G eval loss: 0.366911]\n",
      "[Epoch 185/200][Batch 1/1][D train loss: 0.245698][G train loss: 0.320860]\n",
      "[Epoch 185/200][Batch 1/1][D eval loss: 0.243799][G eval loss: 0.374270]\n",
      "[Epoch 186/200][Batch 1/1][D train loss: 0.245029][G train loss: 0.328568]\n",
      "[Epoch 186/200][Batch 1/1][D eval loss: 0.243207][G eval loss: 0.381909]\n",
      "[Epoch 187/200][Batch 1/1][D train loss: 0.244471][G train loss: 0.336313]\n",
      "[Epoch 187/200][Batch 1/1][D eval loss: 0.242684][G eval loss: 0.389600]\n",
      "[Epoch 188/200][Batch 1/1][D train loss: 0.243996][G train loss: 0.343925]\n",
      "[Epoch 188/200][Batch 1/1][D eval loss: 0.169485][G eval loss: 0.849222]\n",
      "[Epoch 189/200][Batch 1/1][D train loss: 0.175063][G train loss: 0.796894]\n",
      "[Epoch 189/200][Batch 1/1][D eval loss: 0.170928][G eval loss: 0.856282]\n",
      "[Epoch 190/200][Batch 1/1][D train loss: 0.174967][G train loss: 0.805598]\n",
      "[Epoch 190/200][Batch 1/1][D eval loss: 0.183146][G eval loss: 0.817985]\n",
      "[Epoch 191/200][Batch 1/1][D train loss: 0.184396][G train loss: 0.763013]\n",
      "[Epoch 191/200][Batch 1/1][D eval loss: 0.184972][G eval loss: 0.819799]\n",
      "[Epoch 192/200][Batch 1/1][D train loss: 0.183730][G train loss: 0.779550]\n",
      "[Epoch 192/200][Batch 1/1][D eval loss: 0.186714][G eval loss: 0.821748]\n",
      "[Epoch 193/200][Batch 1/1][D train loss: 0.182951][G train loss: 0.796579]\n",
      "[Epoch 193/200][Batch 1/1][D eval loss: 0.188504][G eval loss: 0.823474]\n",
      "[Epoch 194/200][Batch 1/1][D train loss: 0.184682][G train loss: 0.797489]\n",
      "[Epoch 194/200][Batch 1/1][D eval loss: 0.190205][G eval loss: 0.825032]\n",
      "[Epoch 195/200][Batch 1/1][D train loss: 0.188290][G train loss: 0.784132]\n",
      "[Epoch 195/200][Batch 1/1][D eval loss: 0.191788][G eval loss: 0.826767]\n",
      "[Epoch 196/200][Batch 1/1][D train loss: 0.190221][G train loss: 0.784098]\n",
      "[Epoch 196/200][Batch 1/1][D eval loss: 0.217033][G eval loss: 0.833150]\n",
      "[Epoch 197/200][Batch 1/1][D train loss: 0.205835][G train loss: 0.687953]\n",
      "[Epoch 197/200][Batch 1/1][D eval loss: 0.272704][G eval loss: 0.447443]\n",
      "[Epoch 198/200][Batch 1/1][D train loss: 0.244227][G train loss: 0.402260]\n",
      "[Epoch 198/200][Batch 1/1][D eval loss: 0.272837][G eval loss: 0.451941]\n",
      "[Epoch 199/200][Batch 1/1][D train loss: 0.244401][G train loss: 0.406751]\n",
      "[Epoch 199/200][Batch 1/1][D eval loss: 0.272733][G eval loss: 0.456170]\n",
      "[Epoch 200/200][Batch 1/1][D train loss: 0.244456][G train loss: 0.410739]\n",
      "[Epoch 200/200][Batch 1/1][D eval loss: 0.272512][G eval loss: 0.459878]\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.003 lr_d=0.0012 -> score=0.732390\n",
      "\n",
      "----- 2330: lr_g=0.003, lr_d=0.0015 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.003, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250052][G eval loss: 0.490634]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250050][G train loss: 0.454934]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249991][G eval loss: 0.484312]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249986][G train loss: 0.448691]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.484175]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249964][G train loss: 0.448635]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.485237]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249975][G train loss: 0.449806]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249960][G eval loss: 0.484384]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249936][G train loss: 0.449039]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249891][G eval loss: 0.482680]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249845][G train loss: 0.447349]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249802][G eval loss: 0.481053]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249728][G train loss: 0.445685]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249666][G eval loss: 0.479809]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249546][G train loss: 0.444372]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249435][G eval loss: 0.479106]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249240][G train loss: 0.443594]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249062][G eval loss: 0.478925]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248751][G train loss: 0.443344]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248493][G eval loss: 0.479215]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248000][G train loss: 0.443577]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247670][G eval loss: 0.479968]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246896][G train loss: 0.444290]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246363][G eval loss: 0.481164]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245168][G train loss: 0.445465]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244429][G eval loss: 0.483059]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242620][G train loss: 0.447360]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.242088][G eval loss: 0.485302]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239381][G train loss: 0.449629]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.238958][G eval loss: 0.488370]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235056][G train loss: 0.452743]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.235139][G eval loss: 0.492318]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.229666][G train loss: 0.456746]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.230599][G eval loss: 0.496761]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223254][G train loss: 0.461244]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.224542][G eval loss: 0.501920]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.215332][G train loss: 0.466453]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.216382][G eval loss: 0.508103]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.205758][G train loss: 0.472671]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.205838][G eval loss: 0.515905]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.194349][G train loss: 0.480486]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.193272][G eval loss: 0.525737]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.181082][G train loss: 0.490304]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.178170][G eval loss: 0.538340]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.166251][G train loss: 0.502874]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.160740][G eval loss: 0.555222]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.149320][G train loss: 0.519711]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.142798][G eval loss: 0.574892]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.131851][G train loss: 0.539274]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.125639][G eval loss: 0.594858]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.115703][G train loss: 0.559019]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.108838][G eval loss: 0.609770]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.104139][G train loss: 0.573219]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.106645][G eval loss: 0.593158]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.106318][G train loss: 0.551875]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.275734][G eval loss: 0.333211]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.292237][G train loss: 0.279413]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.355031][G eval loss: 0.287316]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.354894][G train loss: 0.251257]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.373403][G eval loss: 0.286388]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.372475][G train loss: 0.250683]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.382344][G eval loss: 0.284917]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.381390][G train loss: 0.249350]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.387556][G eval loss: 0.281562]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.386969][G train loss: 0.246070]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.390517][G eval loss: 0.277451]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.390332][G train loss: 0.242130]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.391843][G eval loss: 0.272984]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.392778][G train loss: 0.237762]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.392096][G eval loss: 0.268539]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.396266][G train loss: 0.233478]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.391393][G eval loss: 0.264566]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.396596][G train loss: 0.229644]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.389791][G eval loss: 0.261325]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.395161][G train loss: 0.226495]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.387318][G eval loss: 0.259022]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.392057][G train loss: 0.224226]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.384099][G eval loss: 0.257704]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.385191][G train loss: 0.222841]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.380175][G eval loss: 0.257275]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.380073][G train loss: 0.222257]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.375426][G eval loss: 0.257558]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.375344][G train loss: 0.222336]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.369890][G eval loss: 0.258473]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.369828][G train loss: 0.223025]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.363612][G eval loss: 0.259901]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.363568][G train loss: 0.224211]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.356660][G eval loss: 0.262195]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.356632][G train loss: 0.226276]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.349136][G eval loss: 0.265119]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.349120][G train loss: 0.228968]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.340971][G eval loss: 0.268851]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.328702][G train loss: 0.260070]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.032220][G eval loss: 0.970967]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.032230][G train loss: 0.934188]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.034169][G eval loss: 0.979085]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.034178][G train loss: 0.941893]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.035832][G eval loss: 0.987026]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.035843][G train loss: 0.949228]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.037128][G eval loss: 0.994920]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.037139][G train loss: 0.956396]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.038034][G eval loss: 1.002286]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.038043][G train loss: 0.962819]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.038545][G eval loss: 1.008340]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.038552][G train loss: 0.968152]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.038673][G eval loss: 1.012729]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.038679][G train loss: 0.972469]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.038444][G eval loss: 1.015541]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.038449][G train loss: 0.975774]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.037889][G eval loss: 1.017740]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.037892][G train loss: 0.978630]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.037044][G eval loss: 1.021486]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.037046][G train loss: 0.981941]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.035945][G eval loss: 1.028026]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.035947][G train loss: 0.986917]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.034688][G eval loss: 1.038855]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.034689][G train loss: 0.994549]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.033317][G eval loss: 1.050113]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.033317][G train loss: 1.002846]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.031809][G eval loss: 1.059755]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.031810][G train loss: 1.009579]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.078370][G eval loss: 0.929181]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.041751][G train loss: 0.982398]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.083150][G eval loss: 0.890505]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.040251][G train loss: 0.986480]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.101395][G eval loss: 0.862770]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.045510][G train loss: 0.964205]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.101927][G eval loss: 0.852353]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.057314][G train loss: 0.925842]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.108920][G eval loss: 0.819884]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.068765][G train loss: 0.881457]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.120618][G eval loss: 0.807235]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.080228][G train loss: 0.859096]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.123578][G eval loss: 0.809477]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.087153][G train loss: 0.849099]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.123458][G eval loss: 0.811848]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.003 lr_d=0.0015 -> score=0.935305\n",
      "\n",
      "----- 2330: lr_g=0.0035, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.494471]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.458774]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.483142]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 0.447537]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.479937]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.444447]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.479604]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.444242]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.479229]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249968][G train loss: 0.443922]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.478671]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249957][G train loss: 0.443326]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.478344]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249941][G train loss: 0.442915]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.478295]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249923][G train loss: 0.442773]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249923][G eval loss: 0.478320]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249899][G train loss: 0.442712]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 0.478200]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249870][G train loss: 0.442526]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 0.477877]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249834][G train loss: 0.442159]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249836][G eval loss: 0.477337]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249790][G train loss: 0.441605]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249792][G eval loss: 0.476666]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249735][G train loss: 0.440957]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249738][G eval loss: 0.476048]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249668][G train loss: 0.440392]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249672][G eval loss: 0.475658]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249586][G train loss: 0.440060]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249594][G eval loss: 0.475523]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249487][G train loss: 0.439978]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249500][G eval loss: 0.475623]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249367][G train loss: 0.440113]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249384][G eval loss: 0.475897]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249221][G train loss: 0.440395]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249249][G eval loss: 0.476282]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249050][G train loss: 0.440757]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249071][G eval loss: 0.476824]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248829][G train loss: 0.441255]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248875][G eval loss: 0.477491]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248581][G train loss: 0.441869]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248648][G eval loss: 0.478087]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248288][G train loss: 0.442415]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248385][G eval loss: 0.478639]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.247946][G train loss: 0.442930]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248074][G eval loss: 0.479152]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247548][G train loss: 0.443425]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247707][G eval loss: 0.479637]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247080][G train loss: 0.443915]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247303][G eval loss: 0.480148]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246561][G train loss: 0.444445]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246864][G eval loss: 0.480748]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.245994][G train loss: 0.445071]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246350][G eval loss: 0.481513]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245331][G train loss: 0.445859]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245813][G eval loss: 0.482329]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244624][G train loss: 0.446697]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245160][G eval loss: 0.482966]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243785][G train loss: 0.447362]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.244383][G eval loss: 0.483586]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.242806][G train loss: 0.448009]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.243485][G eval loss: 0.484239]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.241680][G train loss: 0.448683]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.242477][G eval loss: 0.484866]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.240427][G train loss: 0.449318]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.241325][G eval loss: 0.485579]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.239010][G train loss: 0.450026]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.240063][G eval loss: 0.486366]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.0035 lr_d=0.0005 -> score=0.726429\n",
      "\n",
      "----- 2330: lr_g=0.0035, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 0.492541]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250006][G train loss: 0.456845]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.482849]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 0.447244]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249985][G eval loss: 0.480948]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249980][G train loss: 0.445458]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249978][G eval loss: 0.481182]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249970][G train loss: 0.445820]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249965][G eval loss: 0.480579]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249952][G train loss: 0.445271]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249943][G eval loss: 0.479453]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249924][G train loss: 0.444108]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249913][G eval loss: 0.478490]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249885][G train loss: 0.443061]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 0.477952]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249833][G train loss: 0.442430]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249817][G eval loss: 0.477626]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249763][G train loss: 0.442019]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249737][G eval loss: 0.477567]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249663][G train loss: 0.441893]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249626][G eval loss: 0.477376]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249526][G train loss: 0.441660]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249481][G eval loss: 0.476998]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249345][G train loss: 0.441268]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249287][G eval loss: 0.476550]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249096][G train loss: 0.440844]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249022][G eval loss: 0.476235]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248763][G train loss: 0.440583]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248731][G eval loss: 0.476102]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248392][G train loss: 0.440507]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248380][G eval loss: 0.476357]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247940][G train loss: 0.440817]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247916][G eval loss: 0.476996]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247345][G train loss: 0.441492]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247326][G eval loss: 0.477994]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246591][G train loss: 0.442504]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246687][G eval loss: 0.479109]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245751][G train loss: 0.443598]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245823][G eval loss: 0.480109]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244649][G train loss: 0.444557]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244647][G eval loss: 0.481440]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243187][G train loss: 0.445836]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243276][G eval loss: 0.482846]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241470][G train loss: 0.447195]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241587][G eval loss: 0.484298]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239397][G train loss: 0.448609]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239548][G eval loss: 0.485810]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.236920][G train loss: 0.450105]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.237141][G eval loss: 0.487337]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.234034][G train loss: 0.451639]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234492][G eval loss: 0.488835]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.230847][G train loss: 0.453154]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231426][G eval loss: 0.490454]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.227225][G train loss: 0.454797]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.227924][G eval loss: 0.492224]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.0035 lr_d=0.0008 -> score=0.720148\n",
      "\n",
      "----- 2330: lr_g=0.0035, lr_d=0.001 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250017][G eval loss: 0.491361]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250015][G train loss: 0.455665]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.482662]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 0.447057]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.481644]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249973][G train loss: 0.446154]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.482233]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249960][G train loss: 0.446872]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 0.481514]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249929][G train loss: 0.446207]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249910][G eval loss: 0.480082]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249880][G train loss: 0.444737]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249855][G eval loss: 0.478797]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249809][G train loss: 0.443368]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249775][G eval loss: 0.478284]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249707][G train loss: 0.442762]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249657][G eval loss: 0.478010]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249558][G train loss: 0.442404]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249483][G eval loss: 0.477785]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249341][G train loss: 0.442112]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249240][G eval loss: 0.477535]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249028][G train loss: 0.441821]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248907][G eval loss: 0.477226]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248604][G train loss: 0.441499]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248517][G eval loss: 0.476953]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248101][G train loss: 0.441252]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247974][G eval loss: 0.477011]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247404][G train loss: 0.441363]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247243][G eval loss: 0.477623]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246470][G train loss: 0.442036]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246466][G eval loss: 0.478613]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245426][G train loss: 0.443082]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245311][G eval loss: 0.479691]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.243938][G train loss: 0.444200]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243821][G eval loss: 0.481082]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242020][G train loss: 0.445609]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241885][G eval loss: 0.482785]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239561][G train loss: 0.447295]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.239404][G eval loss: 0.484971]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236467][G train loss: 0.449446]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.236576][G eval loss: 0.487229]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.232926][G train loss: 0.451656]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.233151][G eval loss: 0.489623]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.228761][G train loss: 0.454000]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.228948][G eval loss: 0.492328]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.223825][G train loss: 0.456654]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.223976][G eval loss: 0.495332]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.218098][G train loss: 0.459635]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.218131][G eval loss: 0.498692]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.211534][G train loss: 0.463003]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.211210][G eval loss: 0.502688]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.203996][G train loss: 0.467013]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.203244][G eval loss: 0.507365]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.195453][G train loss: 0.471707]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.194200][G eval loss: 0.512764]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.185902][G train loss: 0.477133]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.184533][G eval loss: 0.518559]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.175596][G train loss: 0.482958]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.173934][G eval loss: 0.524720]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.164638][G train loss: 0.489109]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.162536][G eval loss: 0.531447]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.153689][G train loss: 0.495795]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.151205][G eval loss: 0.538180]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.143680][G train loss: 0.502443]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.141904][G eval loss: 0.542197]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.136127][G train loss: 0.505937]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.139308][G eval loss: 0.534335]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.135469][G train loss: 0.496214]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.200142][G eval loss: 0.424661]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.208777][G train loss: 0.366938]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.289994][G eval loss: 0.326504]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.293994][G train loss: 0.284857]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.338446][G eval loss: 0.304179]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.337714][G train loss: 0.267167]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.358904][G eval loss: 0.297773]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.357166][G train loss: 0.261534]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.368898][G eval loss: 0.290665]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.367165][G train loss: 0.254411]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.374731][G eval loss: 0.282625]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.372960][G train loss: 0.246628]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.378199][G eval loss: 0.276876]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.376297][G train loss: 0.240976]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.380251][G eval loss: 0.274001]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.378521][G train loss: 0.237550]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.381253][G eval loss: 0.273230]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.379733][G train loss: 0.235710]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.381464][G eval loss: 0.271630]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.380155][G train loss: 0.232795]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.381056][G eval loss: 0.268276]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.380881][G train loss: 0.228332]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.380136][G eval loss: 0.263364]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.382580][G train loss: 0.222645]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.378728][G eval loss: 0.256877]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.382326][G train loss: 0.215895]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.376330][G eval loss: 0.249040]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.368042][G train loss: 0.228880]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.348401][G eval loss: 0.280777]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.365949][G train loss: 0.223340]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.344595][G eval loss: 0.281384]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.363335][G train loss: 0.217226]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.341650][G eval loss: 0.275097]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.360139][G train loss: 0.211389]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.338072][G eval loss: 0.269961]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.355876][G train loss: 0.206657]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.333167][G eval loss: 0.267119]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.349502][G train loss: 0.203690]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.324310][G eval loss: 0.269395]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.345280][G train loss: 0.202515]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.311165][G eval loss: 0.280328]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.341152][G train loss: 0.202465]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.298301][G eval loss: 0.303225]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.336761][G train loss: 0.203612]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.291496][G eval loss: 0.317679]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.0035 lr_d=0.001 -> score=0.609175\n",
      "\n",
      "----- 2330: lr_g=0.0035, lr_d=0.0012 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250029][G eval loss: 0.490179]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250026][G train loss: 0.454482]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.482455]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249987][G train loss: 0.446850]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.482313]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249967][G train loss: 0.446822]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249964][G eval loss: 0.483265]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249949][G train loss: 0.447903]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249930][G eval loss: 0.482488]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249902][G train loss: 0.447180]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249869][G eval loss: 0.480818]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249823][G train loss: 0.445473]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249776][G eval loss: 0.479434]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249702][G train loss: 0.444006]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249633][G eval loss: 0.478441]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249518][G train loss: 0.442920]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249411][G eval loss: 0.477815]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249236][G train loss: 0.442210]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249101][G eval loss: 0.477452]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248835][G train loss: 0.441782]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248637][G eval loss: 0.477323]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248232][G train loss: 0.441612]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247999][G eval loss: 0.477424]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247414][G train loss: 0.441703]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247171][G eval loss: 0.477996]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246347][G train loss: 0.442301]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246172][G eval loss: 0.478958]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245015][G train loss: 0.443320]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244697][G eval loss: 0.480010]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243101][G train loss: 0.444433]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242663][G eval loss: 0.481759]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.240498][G train loss: 0.446242]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240188][G eval loss: 0.483964]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.237283][G train loss: 0.448494]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.237083][G eval loss: 0.486580]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.233298][G train loss: 0.451135]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.233285][G eval loss: 0.489209]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.228521][G train loss: 0.453746]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.228258][G eval loss: 0.492721]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.222526][G train loss: 0.457216]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.222060][G eval loss: 0.496660]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.215386][G train loss: 0.461101]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.214351][G eval loss: 0.501585]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.206839][G train loss: 0.465973]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.205085][G eval loss: 0.507491]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.196782][G train loss: 0.471835]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.194339][G eval loss: 0.514009]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.185404][G train loss: 0.478339]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.182538][G eval loss: 0.520875]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.172800][G train loss: 0.485191]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.169081][G eval loss: 0.529149]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.159117][G train loss: 0.493449]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.154103][G eval loss: 0.539144]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.145422][G train loss: 0.503341]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.139368][G eval loss: 0.550110]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.132930][G train loss: 0.514078]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.128143][G eval loss: 0.557552]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.123727][G train loss: 0.520828]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.127289][G eval loss: 0.545387]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.125164][G train loss: 0.506004]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.232415][G eval loss: 0.376469]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.245791][G train loss: 0.319255]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.330970][G eval loss: 0.300748]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.332248][G train loss: 0.263078]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.363820][G eval loss: 0.292843]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.362282][G train loss: 0.256709]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.376812][G eval loss: 0.288284]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.374901][G train loss: 0.252527]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.383811][G eval loss: 0.282603]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.382068][G train loss: 0.246850]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.388092][G eval loss: 0.277234]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.386756][G train loss: 0.241557]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.390509][G eval loss: 0.273903]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.389549][G train loss: 0.238184]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.391570][G eval loss: 0.272520]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.390856][G train loss: 0.236305]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.391629][G eval loss: 0.271171]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.393201][G train loss: 0.234057]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.391057][G eval loss: 0.268557]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.394087][G train loss: 0.230415]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.389823][G eval loss: 0.264468]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.393482][G train loss: 0.225450]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.387970][G eval loss: 0.259474]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.391887][G train loss: 0.219856]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.385577][G eval loss: 0.254440]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.389213][G train loss: 0.214310]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.382710][G eval loss: 0.249702]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.384337][G train loss: 0.209107]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.379505][G eval loss: 0.245173]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.367158][G train loss: 0.219352]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.375835][G eval loss: 0.243861]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.373959][G train loss: 0.202856]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.371732][G eval loss: 0.243025]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.371492][G train loss: 0.200392]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.367203][G eval loss: 0.242098]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.367005][G train loss: 0.198492]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.362308][G eval loss: 0.241050]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.362143][G train loss: 0.196318]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.357213][G eval loss: 0.239644]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.357076][G train loss: 0.193838]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.351511][G eval loss: 0.238362]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.350814][G train loss: 0.192012]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.345466][G eval loss: 0.237215]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.342612][G train loss: 0.192170]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.339122][G eval loss: 0.236682]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.333945][G train loss: 0.196216]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.332333][G eval loss: 0.237318]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.327252][G train loss: 0.197224]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.325523][G eval loss: 0.239167]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.320970][G train loss: 0.196841]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.318650][G eval loss: 0.242757]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.315972][G train loss: 0.194969]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.311882][G eval loss: 0.247963]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.310597][G train loss: 0.197388]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.305116][G eval loss: 0.254016]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.304351][G train loss: 0.202512]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.298436][G eval loss: 0.260503]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.297928][G train loss: 0.208879]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.291851][G eval loss: 0.267204]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.291526][G train loss: 0.216080]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.285487][G eval loss: 0.272774]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.285304][G train loss: 0.222928]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.279443][G eval loss: 0.279446]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.279354][G train loss: 0.230774]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.273827][G eval loss: 0.287796]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.273816][G train loss: 0.239930]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.268703][G eval loss: 0.297463]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.268762][G train loss: 0.249906]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.264432][G eval loss: 0.308111]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.264560][G train loss: 0.260394]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.260719][G eval loss: 0.320134]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.260814][G train loss: 0.271990]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.257520][G eval loss: 0.333599]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.257561][G train loss: 0.284584]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.254992][G eval loss: 0.347683]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.255063][G train loss: 0.297495]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.253038][G eval loss: 0.361832]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.253182][G train loss: 0.310289]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.251609][G eval loss: 0.375731]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.251723][G train loss: 0.322875]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.250621][G eval loss: 0.389653]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.250642][G train loss: 0.334822]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.250082][G eval loss: 0.403456]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.249962][G train loss: 0.346856]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.249940][G eval loss: 0.416695]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.249705][G train loss: 0.359246]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.250134][G eval loss: 0.428349]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.249818][G train loss: 0.370113]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.250563][G eval loss: 0.439557]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.250253][G train loss: 0.380690]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.251175][G eval loss: 0.449372]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.250855][G train loss: 0.389840]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.251804][G eval loss: 0.457141]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.251461][G train loss: 0.397135]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.252466][G eval loss: 0.463573]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.252029][G train loss: 0.403475]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.253111][G eval loss: 0.468589]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.252559][G train loss: 0.408700]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.253651][G eval loss: 0.471655]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.253019][G train loss: 0.412278]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.254072][G eval loss: 0.473154]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.253396][G train loss: 0.414502]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.254402][G eval loss: 0.473670]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.253813][G train loss: 0.415937]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.254635][G eval loss: 0.473512]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.253933][G train loss: 0.416703]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.254758][G eval loss: 0.473005]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.253957][G train loss: 0.417025]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.254772][G eval loss: 0.472081]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.254009][G train loss: 0.416941]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.254691][G eval loss: 0.470900]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.253956][G train loss: 0.416452]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.254526][G eval loss: 0.469371]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.253801][G train loss: 0.415485]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.254289][G eval loss: 0.467255]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.253545][G train loss: 0.413853]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.253993][G eval loss: 0.464503]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.253198][G train loss: 0.411441]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.253655][G eval loss: 0.461078]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.252792][G train loss: 0.408261]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.253290][G eval loss: 0.457246]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.252457][G train loss: 0.404385]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.252908][G eval loss: 0.453100]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.252120][G train loss: 0.400029]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.252521][G eval loss: 0.448674]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.251700][G train loss: 0.395289]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.252134][G eval loss: 0.443942]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.251305][G train loss: 0.390462]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.251774][G eval loss: 0.438850]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.250980][G train loss: 0.385574]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.251435][G eval loss: 0.434017]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.250713][G train loss: 0.380701]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.251128][G eval loss: 0.429345]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.250455][G train loss: 0.375866]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.250861][G eval loss: 0.424846]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.250199][G train loss: 0.371159]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.250633][G eval loss: 0.420374]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.249953][G train loss: 0.366523]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.250447][G eval loss: 0.415761]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.249772][G train loss: 0.361869]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.250321][G eval loss: 0.411091]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.249649][G train loss: 0.357330]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.250223][G eval loss: 0.406593]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.249632][G train loss: 0.352943]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.250165][G eval loss: 0.402201]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.249595][G train loss: 0.348746]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.250145][G eval loss: 0.397887]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.249538][G train loss: 0.344774]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.250138][G eval loss: 0.393805]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.249540][G train loss: 0.341029]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.250140][G eval loss: 0.389983]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.249601][G train loss: 0.337377]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.250146][G eval loss: 0.386469]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.249679][G train loss: 0.333873]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.250200][G eval loss: 0.383052]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.249803][G train loss: 0.330530]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.250280][G eval loss: 0.379664]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.249913][G train loss: 0.327192]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.250374][G eval loss: 0.376469]\n",
      "[Epoch 111/200][Batch 1/1][D train loss: 0.249992][G train loss: 0.324080]\n",
      "[Epoch 111/200][Batch 1/1][D eval loss: 0.250457][G eval loss: 0.373785]\n",
      "[Epoch 112/200][Batch 1/1][D train loss: 0.250054][G train loss: 0.321299]\n",
      "[Epoch 112/200][Batch 1/1][D eval loss: 0.250541][G eval loss: 0.371583]\n",
      "[Epoch 113/200][Batch 1/1][D train loss: 0.250118][G train loss: 0.319046]\n",
      "[Epoch 113/200][Batch 1/1][D eval loss: 0.250602][G eval loss: 0.369743]\n",
      "[Epoch 114/200][Batch 1/1][D train loss: 0.250168][G train loss: 0.317128]\n",
      "[Epoch 114/200][Batch 1/1][D eval loss: 0.250698][G eval loss: 0.367898]\n",
      "[Epoch 115/200][Batch 1/1][D train loss: 0.250218][G train loss: 0.315233]\n",
      "[Epoch 115/200][Batch 1/1][D eval loss: 0.250782][G eval loss: 0.366362]\n",
      "[Epoch 116/200][Batch 1/1][D train loss: 0.250295][G train loss: 0.313554]\n",
      "[Epoch 116/200][Batch 1/1][D eval loss: 0.250844][G eval loss: 0.364946]\n",
      "[Epoch 117/200][Batch 1/1][D train loss: 0.250332][G train loss: 0.312130]\n",
      "[Epoch 117/200][Batch 1/1][D eval loss: 0.250853][G eval loss: 0.363756]\n",
      "[Epoch 118/200][Batch 1/1][D train loss: 0.250349][G train loss: 0.310953]\n",
      "[Epoch 118/200][Batch 1/1][D eval loss: 0.250798][G eval loss: 0.362708]\n",
      "[Epoch 119/200][Batch 1/1][D train loss: 0.250328][G train loss: 0.310038]\n",
      "[Epoch 119/200][Batch 1/1][D eval loss: 0.250756][G eval loss: 0.361817]\n",
      "[Epoch 120/200][Batch 1/1][D train loss: 0.250297][G train loss: 0.309381]\n",
      "[Epoch 120/200][Batch 1/1][D eval loss: 0.250740][G eval loss: 0.361231]\n",
      "[Epoch 121/200][Batch 1/1][D train loss: 0.250266][G train loss: 0.309029]\n",
      "[Epoch 121/200][Batch 1/1][D eval loss: 0.250717][G eval loss: 0.360798]\n",
      "[Epoch 122/200][Batch 1/1][D train loss: 0.250243][G train loss: 0.308926]\n",
      "[Epoch 122/200][Batch 1/1][D eval loss: 0.250690][G eval loss: 0.360583]\n",
      "[Epoch 123/200][Batch 1/1][D train loss: 0.250207][G train loss: 0.309090]\n",
      "[Epoch 123/200][Batch 1/1][D eval loss: 0.250649][G eval loss: 0.360567]\n",
      "[Epoch 124/200][Batch 1/1][D train loss: 0.250163][G train loss: 0.309529]\n",
      "[Epoch 124/200][Batch 1/1][D eval loss: 0.250579][G eval loss: 0.360848]\n",
      "[Epoch 125/200][Batch 1/1][D train loss: 0.250094][G train loss: 0.310282]\n",
      "[Epoch 125/200][Batch 1/1][D eval loss: 0.250523][G eval loss: 0.361415]\n",
      "[Epoch 126/200][Batch 1/1][D train loss: 0.250008][G train loss: 0.311281]\n",
      "[Epoch 126/200][Batch 1/1][D eval loss: 0.250465][G eval loss: 0.362320]\n",
      "[Epoch 127/200][Batch 1/1][D train loss: 0.249912][G train loss: 0.312472]\n",
      "[Epoch 127/200][Batch 1/1][D eval loss: 0.250364][G eval loss: 0.363691]\n",
      "[Epoch 128/200][Batch 1/1][D train loss: 0.249790][G train loss: 0.313985]\n",
      "[Epoch 128/200][Batch 1/1][D eval loss: 0.250319][G eval loss: 0.365083]\n",
      "[Epoch 129/200][Batch 1/1][D train loss: 0.249677][G train loss: 0.315444]\n",
      "[Epoch 129/200][Batch 1/1][D eval loss: 0.250246][G eval loss: 0.366992]\n",
      "[Epoch 130/200][Batch 1/1][D train loss: 0.249536][G train loss: 0.317315]\n",
      "[Epoch 130/200][Batch 1/1][D eval loss: 0.250177][G eval loss: 0.368804]\n",
      "[Epoch 131/200][Batch 1/1][D train loss: 0.249440][G train loss: 0.319423]\n",
      "[Epoch 131/200][Batch 1/1][D eval loss: 0.250128][G eval loss: 0.370985]\n",
      "[Epoch 132/200][Batch 1/1][D train loss: 0.249355][G train loss: 0.321866]\n",
      "[Epoch 132/200][Batch 1/1][D eval loss: 0.250047][G eval loss: 0.373421]\n",
      "[Epoch 133/200][Batch 1/1][D train loss: 0.249246][G train loss: 0.324318]\n",
      "[Epoch 133/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 0.375959]\n",
      "[Epoch 134/200][Batch 1/1][D train loss: 0.249140][G train loss: 0.327005]\n",
      "[Epoch 134/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 0.378602]\n",
      "[Epoch 135/200][Batch 1/1][D train loss: 0.249049][G train loss: 0.329902]\n",
      "[Epoch 135/200][Batch 1/1][D eval loss: 0.249906][G eval loss: 0.381423]\n",
      "[Epoch 136/200][Batch 1/1][D train loss: 0.248984][G train loss: 0.332868]\n",
      "[Epoch 136/200][Batch 1/1][D eval loss: 0.249919][G eval loss: 0.384366]\n",
      "[Epoch 137/200][Batch 1/1][D train loss: 0.248922][G train loss: 0.335920]\n",
      "[Epoch 137/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.387328]\n",
      "[Epoch 138/200][Batch 1/1][D train loss: 0.248917][G train loss: 0.339003]\n",
      "[Epoch 138/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.390268]\n",
      "[Epoch 139/200][Batch 1/1][D train loss: 0.248922][G train loss: 0.342054]\n",
      "[Epoch 139/200][Batch 1/1][D eval loss: 0.250024][G eval loss: 0.393103]\n",
      "[Epoch 140/200][Batch 1/1][D train loss: 0.248945][G train loss: 0.345088]\n",
      "[Epoch 140/200][Batch 1/1][D eval loss: 0.250094][G eval loss: 0.395512]\n",
      "[Epoch 141/200][Batch 1/1][D train loss: 0.248988][G train loss: 0.348054]\n",
      "[Epoch 141/200][Batch 1/1][D eval loss: 0.250166][G eval loss: 0.397901]\n",
      "[Epoch 142/200][Batch 1/1][D train loss: 0.249015][G train loss: 0.350847]\n",
      "[Epoch 142/200][Batch 1/1][D eval loss: 0.250217][G eval loss: 0.400389]\n",
      "[Epoch 143/200][Batch 1/1][D train loss: 0.249049][G train loss: 0.353447]\n",
      "[Epoch 143/200][Batch 1/1][D eval loss: 0.250302][G eval loss: 0.402187]\n",
      "[Epoch 144/200][Batch 1/1][D train loss: 0.249080][G train loss: 0.355620]\n",
      "[Epoch 144/200][Batch 1/1][D eval loss: 0.250385][G eval loss: 0.403697]\n",
      "[Epoch 145/200][Batch 1/1][D train loss: 0.249094][G train loss: 0.357559]\n",
      "[Epoch 145/200][Batch 1/1][D eval loss: 0.250463][G eval loss: 0.405351]\n",
      "[Epoch 146/200][Batch 1/1][D train loss: 0.249138][G train loss: 0.359469]\n",
      "[Epoch 146/200][Batch 1/1][D eval loss: 0.250587][G eval loss: 0.406714]\n",
      "[Epoch 147/200][Batch 1/1][D train loss: 0.249183][G train loss: 0.361015]\n",
      "[Epoch 147/200][Batch 1/1][D eval loss: 0.250665][G eval loss: 0.407585]\n",
      "[Epoch 148/200][Batch 1/1][D train loss: 0.249242][G train loss: 0.362120]\n",
      "[Epoch 148/200][Batch 1/1][D eval loss: 0.250738][G eval loss: 0.408130]\n",
      "[Epoch 149/200][Batch 1/1][D train loss: 0.249253][G train loss: 0.362854]\n",
      "[Epoch 149/200][Batch 1/1][D eval loss: 0.250766][G eval loss: 0.407735]\n",
      "[Epoch 150/200][Batch 1/1][D train loss: 0.249236][G train loss: 0.363151]\n",
      "[Epoch 150/200][Batch 1/1][D eval loss: 0.250757][G eval loss: 0.407387]\n",
      "[Epoch 151/200][Batch 1/1][D train loss: 0.249234][G train loss: 0.362882]\n",
      "[Epoch 151/200][Batch 1/1][D eval loss: 0.250733][G eval loss: 0.406775]\n",
      "[Epoch 152/200][Batch 1/1][D train loss: 0.249191][G train loss: 0.362282]\n",
      "[Epoch 152/200][Batch 1/1][D eval loss: 0.250664][G eval loss: 0.405943]\n",
      "[Epoch 153/200][Batch 1/1][D train loss: 0.249159][G train loss: 0.361377]\n",
      "[Epoch 153/200][Batch 1/1][D eval loss: 0.250629][G eval loss: 0.404786]\n",
      "[Epoch 154/200][Batch 1/1][D train loss: 0.249094][G train loss: 0.360277]\n",
      "[Epoch 154/200][Batch 1/1][D eval loss: 0.250627][G eval loss: 0.403478]\n",
      "[Epoch 155/200][Batch 1/1][D train loss: 0.249018][G train loss: 0.359042]\n",
      "[Epoch 155/200][Batch 1/1][D eval loss: 0.250578][G eval loss: 0.402211]\n",
      "[Epoch 156/200][Batch 1/1][D train loss: 0.248958][G train loss: 0.357707]\n",
      "[Epoch 156/200][Batch 1/1][D eval loss: 0.250539][G eval loss: 0.400698]\n",
      "[Epoch 157/200][Batch 1/1][D train loss: 0.249242][G train loss: 0.355397]\n",
      "[Epoch 157/200][Batch 1/1][D eval loss: 0.250452][G eval loss: 0.398687]\n",
      "[Epoch 158/200][Batch 1/1][D train loss: 0.248913][G train loss: 0.354460]\n",
      "[Epoch 158/200][Batch 1/1][D eval loss: 0.250359][G eval loss: 0.397191]\n",
      "[Epoch 159/200][Batch 1/1][D train loss: 0.248015][G train loss: 0.354939]\n",
      "[Epoch 159/200][Batch 1/1][D eval loss: 0.250344][G eval loss: 0.395601]\n",
      "[Epoch 160/200][Batch 1/1][D train loss: 0.248200][G train loss: 0.352522]\n",
      "[Epoch 160/200][Batch 1/1][D eval loss: 0.250288][G eval loss: 0.393908]\n",
      "[Epoch 161/200][Batch 1/1][D train loss: 0.248914][G train loss: 0.349060]\n",
      "[Epoch 161/200][Batch 1/1][D eval loss: 0.250192][G eval loss: 0.392248]\n",
      "[Epoch 162/200][Batch 1/1][D train loss: 0.250330][G train loss: 0.345117]\n",
      "[Epoch 162/200][Batch 1/1][D eval loss: 0.250192][G eval loss: 0.390237]\n",
      "[Epoch 163/200][Batch 1/1][D train loss: 0.248879][G train loss: 0.345235]\n",
      "[Epoch 163/200][Batch 1/1][D eval loss: 0.250155][G eval loss: 0.387995]\n",
      "[Epoch 164/200][Batch 1/1][D train loss: 0.250658][G train loss: 0.340794]\n",
      "[Epoch 164/200][Batch 1/1][D eval loss: 0.250050][G eval loss: 0.385928]\n",
      "[Epoch 165/200][Batch 1/1][D train loss: 0.250423][G train loss: 0.339094]\n",
      "[Epoch 165/200][Batch 1/1][D eval loss: 0.250041][G eval loss: 0.383686]\n",
      "[Epoch 166/200][Batch 1/1][D train loss: 0.250474][G train loss: 0.337033]\n",
      "[Epoch 166/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.381645]\n",
      "[Epoch 167/200][Batch 1/1][D train loss: 0.250543][G train loss: 0.334937]\n",
      "[Epoch 167/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.379515]\n",
      "[Epoch 168/200][Batch 1/1][D train loss: 0.250021][G train loss: 0.333513]\n",
      "[Epoch 168/200][Batch 1/1][D eval loss: 0.249946][G eval loss: 0.377621]\n",
      "[Epoch 169/200][Batch 1/1][D train loss: 0.250486][G train loss: 0.331299]\n",
      "[Epoch 169/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 0.375974]\n",
      "[Epoch 170/200][Batch 1/1][D train loss: 0.250346][G train loss: 0.329891]\n",
      "[Epoch 170/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.374037]\n",
      "[Epoch 171/200][Batch 1/1][D train loss: 0.250386][G train loss: 0.328322]\n",
      "[Epoch 171/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 0.372716]\n",
      "[Epoch 172/200][Batch 1/1][D train loss: 0.250522][G train loss: 0.326866]\n",
      "[Epoch 172/200][Batch 1/1][D eval loss: 0.249963][G eval loss: 0.371519]\n",
      "[Epoch 173/200][Batch 1/1][D train loss: 0.250282][G train loss: 0.325868]\n",
      "[Epoch 173/200][Batch 1/1][D eval loss: 0.249961][G eval loss: 0.370444]\n",
      "[Epoch 174/200][Batch 1/1][D train loss: 0.249568][G train loss: 0.325564]\n",
      "[Epoch 174/200][Batch 1/1][D eval loss: 0.249948][G eval loss: 0.369460]\n",
      "[Epoch 175/200][Batch 1/1][D train loss: 0.249246][G train loss: 0.324965]\n",
      "[Epoch 175/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.368439]\n",
      "[Epoch 176/200][Batch 1/1][D train loss: 0.249837][G train loss: 0.323216]\n",
      "[Epoch 176/200][Batch 1/1][D eval loss: 0.249957][G eval loss: 0.367480]\n",
      "[Epoch 177/200][Batch 1/1][D train loss: 0.250540][G train loss: 0.321507]\n",
      "[Epoch 177/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.366432]\n",
      "[Epoch 178/200][Batch 1/1][D train loss: 0.250211][G train loss: 0.320997]\n",
      "[Epoch 178/200][Batch 1/1][D eval loss: 0.249985][G eval loss: 0.365476]\n",
      "[Epoch 179/200][Batch 1/1][D train loss: 0.249891][G train loss: 0.320538]\n",
      "[Epoch 179/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.364218]\n",
      "[Epoch 180/200][Batch 1/1][D train loss: 0.249758][G train loss: 0.319910]\n",
      "[Epoch 180/200][Batch 1/1][D eval loss: 0.250006][G eval loss: 0.363012]\n",
      "[Epoch 181/200][Batch 1/1][D train loss: 0.249412][G train loss: 0.319536]\n",
      "[Epoch 181/200][Batch 1/1][D eval loss: 0.250024][G eval loss: 0.362050]\n",
      "[Epoch 182/200][Batch 1/1][D train loss: 0.249564][G train loss: 0.318547]\n",
      "[Epoch 182/200][Batch 1/1][D eval loss: 0.250014][G eval loss: 0.361432]\n",
      "[Epoch 183/200][Batch 1/1][D train loss: 0.250173][G train loss: 0.317035]\n",
      "[Epoch 183/200][Batch 1/1][D eval loss: 0.250013][G eval loss: 0.361195]\n",
      "[Epoch 184/200][Batch 1/1][D train loss: 0.250424][G train loss: 0.316302]\n",
      "[Epoch 184/200][Batch 1/1][D eval loss: 0.250015][G eval loss: 0.360745]\n",
      "[Epoch 185/200][Batch 1/1][D train loss: 0.250487][G train loss: 0.315707]\n",
      "[Epoch 185/200][Batch 1/1][D eval loss: 0.250015][G eval loss: 0.360116]\n",
      "[Epoch 186/200][Batch 1/1][D train loss: 0.250408][G train loss: 0.315179]\n",
      "[Epoch 186/200][Batch 1/1][D eval loss: 0.250004][G eval loss: 0.359510]\n",
      "[Epoch 187/200][Batch 1/1][D train loss: 0.250642][G train loss: 0.314291]\n",
      "[Epoch 187/200][Batch 1/1][D eval loss: 0.250063][G eval loss: 0.359045]\n",
      "[Epoch 188/200][Batch 1/1][D train loss: 0.250710][G train loss: 0.313607]\n",
      "[Epoch 188/200][Batch 1/1][D eval loss: 0.250049][G eval loss: 0.358888]\n",
      "[Epoch 189/200][Batch 1/1][D train loss: 0.250725][G train loss: 0.313148]\n",
      "[Epoch 189/200][Batch 1/1][D eval loss: 0.250016][G eval loss: 0.358861]\n",
      "[Epoch 190/200][Batch 1/1][D train loss: 0.250713][G train loss: 0.312899]\n",
      "[Epoch 190/200][Batch 1/1][D eval loss: 0.250046][G eval loss: 0.359014]\n",
      "[Epoch 191/200][Batch 1/1][D train loss: 0.250816][G train loss: 0.312716]\n",
      "[Epoch 191/200][Batch 1/1][D eval loss: 0.250044][G eval loss: 0.359244]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.0035 lr_d=0.0012 -> score=0.609289\n",
      "\n",
      "----- 2330: lr_g=0.0035, lr_d=0.0015 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0035, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250053][G eval loss: 0.488401]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250051][G train loss: 0.452705]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.482041]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249987][G train loss: 0.446436]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.483195]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249965][G train loss: 0.447704]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 0.484684]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.449322]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.483836]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249943][G train loss: 0.448528]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249902][G eval loss: 0.482370]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249859][G train loss: 0.447025]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249814][G eval loss: 0.481138]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249743][G train loss: 0.445710]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249674][G eval loss: 0.480224]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249557][G train loss: 0.444704]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249436][G eval loss: 0.479591]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249247][G train loss: 0.443985]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249055][G eval loss: 0.479144]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248754][G train loss: 0.443474]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248485][G eval loss: 0.478921]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248010][G train loss: 0.443211]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247640][G eval loss: 0.479015]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246896][G train loss: 0.443295]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246312][G eval loss: 0.479527]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245166][G train loss: 0.443836]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244393][G eval loss: 0.480874]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242654][G train loss: 0.445241]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.241953][G eval loss: 0.483202]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239350][G train loss: 0.447635]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.238806][G eval loss: 0.486526]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235072][G train loss: 0.451023]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.234881][G eval loss: 0.491103]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.229655][G train loss: 0.455653]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.230330][G eval loss: 0.496112]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223321][G train loss: 0.460687]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.224231][G eval loss: 0.501448]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.215522][G train loss: 0.466003]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.215909][G eval loss: 0.507915]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.205940][G train loss: 0.472423]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.205226][G eval loss: 0.516091]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.194451][G train loss: 0.480540]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.192362][G eval loss: 0.526231]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.181026][G train loss: 0.490621]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.177057][G eval loss: 0.538700]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.165902][G train loss: 0.503035]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.159812][G eval loss: 0.555170]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.148695][G train loss: 0.519477]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.141656][G eval loss: 0.574398]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.131039][G train loss: 0.538614]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.122466][G eval loss: 0.594391]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.115240][G train loss: 0.558460]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.106653][G eval loss: 0.608992]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.103899][G train loss: 0.572198]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.112296][G eval loss: 0.574715]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.112974][G train loss: 0.531096]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.304605][G eval loss: 0.308262]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.312033][G train loss: 0.266266]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.357341][G eval loss: 0.289325]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.357104][G train loss: 0.253188]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.372429][G eval loss: 0.287245]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.371761][G train loss: 0.251385]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.380183][G eval loss: 0.283672]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.379500][G train loss: 0.248074]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.384749][G eval loss: 0.278907]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.384137][G train loss: 0.243433]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.387259][G eval loss: 0.273944]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.386810][G train loss: 0.238583]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.388213][G eval loss: 0.269230]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.391930][G train loss: 0.233850]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.388036][G eval loss: 0.265000]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.394691][G train loss: 0.229459]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.386978][G eval loss: 0.261403]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.394021][G train loss: 0.225581]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.385137][G eval loss: 0.258715]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.391039][G train loss: 0.222589]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.382384][G eval loss: 0.256952]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.387792][G train loss: 0.220539]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.378781][G eval loss: 0.255869]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.381534][G train loss: 0.219145]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.374621][G eval loss: 0.254988]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.360683][G train loss: 0.243121]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.369659][G eval loss: 0.255448]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.355926][G train loss: 0.243100]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.363922][G eval loss: 0.256347]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.356296][G train loss: 0.230725]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.357507][G eval loss: 0.257848]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.351688][G train loss: 0.225847]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.350460][G eval loss: 0.260081]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.350385][G train loss: 0.218479]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.342889][G eval loss: 0.262000]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.342840][G train loss: 0.218880]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.334706][G eval loss: 0.263789]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.333898][G train loss: 0.219570]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.326766][G eval loss: 0.266308]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.320920][G train loss: 0.233769]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.295413][G eval loss: 0.326002]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.312805][G train loss: 0.235016]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.110411][G eval loss: 0.744760]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.0035 lr_d=0.0015 -> score=0.855171\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.492428]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.456734]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.481570]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 0.445985]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249990][G eval loss: 0.479411]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.443979]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.479137]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.443824]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.478821]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.443501]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.478656]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249958][G train loss: 0.443256]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249957][G eval loss: 0.478775]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249943][G train loss: 0.443271]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249942][G eval loss: 0.478930]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249924][G train loss: 0.443326]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249923][G eval loss: 0.478793]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249900][G train loss: 0.443111]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 0.478188]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249871][G train loss: 0.442470]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 0.477212]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249836][G train loss: 0.441500]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249837][G eval loss: 0.476158]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249792][G train loss: 0.440490]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249792][G eval loss: 0.475355]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249737][G train loss: 0.439750]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249738][G eval loss: 0.474886]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249670][G train loss: 0.439343]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249672][G eval loss: 0.474737]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249588][G train loss: 0.439233]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249592][G eval loss: 0.474841]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249487][G train loss: 0.439338]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249492][G eval loss: 0.475208]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249363][G train loss: 0.439665]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249376][G eval loss: 0.475873]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249216][G train loss: 0.440269]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249230][G eval loss: 0.476661]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249037][G train loss: 0.440990]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249068][G eval loss: 0.477423]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248834][G train loss: 0.441707]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248882][G eval loss: 0.477996]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248598][G train loss: 0.442263]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248667][G eval loss: 0.478358]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248323][G train loss: 0.442636]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248417][G eval loss: 0.478646]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.247999][G train loss: 0.442956]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248126][G eval loss: 0.479003]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247618][G train loss: 0.443350]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247779][G eval loss: 0.479422]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247164][G train loss: 0.443784]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247374][G eval loss: 0.479907]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246640][G train loss: 0.444281]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246905][G eval loss: 0.480406]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246038][G train loss: 0.444806]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246372][G eval loss: 0.480631]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245363][G train loss: 0.445056]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245772][G eval loss: 0.480848]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244601][G train loss: 0.445291]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245130][G eval loss: 0.481233]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243781][G train loss: 0.445677]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.244402][G eval loss: 0.481869]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.0005 -> score=0.726271\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 0.490498]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250007][G train loss: 0.454804]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.481278]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249993][G train loss: 0.445692]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 0.480424]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 0.444992]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 0.480719]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249971][G train loss: 0.445405]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.480171]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249954][G train loss: 0.444852]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249945][G eval loss: 0.479437]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249927][G train loss: 0.444036]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.478919]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249889][G train loss: 0.443416]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 0.478578]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249835][G train loss: 0.442975]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249813][G eval loss: 0.478090]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249762][G train loss: 0.442408]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249735][G eval loss: 0.477506]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249666][G train loss: 0.441790]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249630][G eval loss: 0.476628]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249537][G train loss: 0.440917]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249487][G eval loss: 0.475726]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249360][G train loss: 0.440060]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249297][G eval loss: 0.475144]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249116][G train loss: 0.439541]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249037][G eval loss: 0.474986]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248789][G train loss: 0.439445]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248738][G eval loss: 0.475130]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248411][G train loss: 0.439629]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248379][G eval loss: 0.475665]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247950][G train loss: 0.440167]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247891][G eval loss: 0.476588]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247333][G train loss: 0.441050]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247276][G eval loss: 0.477981]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246555][G train loss: 0.442385]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246648][G eval loss: 0.479481]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245728][G train loss: 0.443822]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245765][G eval loss: 0.480725]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244612][G train loss: 0.445020]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244617][G eval loss: 0.481917]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243186][G train loss: 0.446195]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243252][G eval loss: 0.482861]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241494][G train loss: 0.447148]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241578][G eval loss: 0.483849]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239458][G train loss: 0.448163]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239561][G eval loss: 0.485037]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237029][G train loss: 0.449387]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.237145][G eval loss: 0.486421]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.0008 -> score=0.723566\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.001 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 0.489318]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250015][G train loss: 0.453624]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.481091]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 0.445506]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.481121]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249974][G train loss: 0.445689]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 0.481770]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249961][G train loss: 0.446457]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249952][G eval loss: 0.481103]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249933][G train loss: 0.445784]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.480056]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249886][G train loss: 0.444656]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 0.479194]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249816][G train loss: 0.443690]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249779][G eval loss: 0.478874]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249715][G train loss: 0.443271]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249662][G eval loss: 0.478439]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249568][G train loss: 0.442758]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249499][G eval loss: 0.477722]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249367][G train loss: 0.442008]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249274][G eval loss: 0.476830]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249083][G train loss: 0.441122]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248972][G eval loss: 0.475999]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248694][G train loss: 0.440335]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248546][G eval loss: 0.475630]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248147][G train loss: 0.440029]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247991][G eval loss: 0.475688]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247439][G train loss: 0.440148]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247282][G eval loss: 0.476437]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246534][G train loss: 0.440943]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246484][G eval loss: 0.477626]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245479][G train loss: 0.442139]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245317][G eval loss: 0.478658]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.244003][G train loss: 0.443131]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243717][G eval loss: 0.480279]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242016][G train loss: 0.444691]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241734][G eval loss: 0.482319]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239571][G train loss: 0.446672]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.239271][G eval loss: 0.484545]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236563][G train loss: 0.448856]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.236333][G eval loss: 0.486857]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.232999][G train loss: 0.451155]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.232910][G eval loss: 0.489037]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.228902][G train loss: 0.453341]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.228884][G eval loss: 0.491371]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.224154][G train loss: 0.455689]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.224070][G eval loss: 0.494152]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.218588][G train loss: 0.458479]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.218245][G eval loss: 0.497497]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.212070][G train loss: 0.461848]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.211349][G eval loss: 0.501552]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.204575][G train loss: 0.465936]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.203484][G eval loss: 0.506332]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.196092][G train loss: 0.470747]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.194688][G eval loss: 0.511726]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.186719][G train loss: 0.476123]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.185189][G eval loss: 0.517525]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.176556][G train loss: 0.481873]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.174972][G eval loss: 0.523319]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.166008][G train loss: 0.487422]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.164216][G eval loss: 0.529277]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.155796][G train loss: 0.492825]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.154860][G eval loss: 0.532866]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.148098][G train loss: 0.494652]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.160746][G eval loss: 0.508911]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.159047][G train loss: 0.461219]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.201321][G eval loss: 0.448638]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.215481][G train loss: 0.376569]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.236119][G eval loss: 0.419287]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.262159][G train loss: 0.337896]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.272034][G eval loss: 0.386226]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.295668][G train loss: 0.313017]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.296164][G eval loss: 0.362606]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.315196][G train loss: 0.293487]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.348837][G eval loss: 0.291119]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.340552][G train loss: 0.264876]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.370642][G eval loss: 0.273249]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.348858][G train loss: 0.254444]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.377673][G eval loss: 0.271821]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.362208][G train loss: 0.242207]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.381181][G eval loss: 0.280206]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.371646][G train loss: 0.242469]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.382758][G eval loss: 0.291040]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.373480][G train loss: 0.252271]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.382841][G eval loss: 0.298441]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.375202][G train loss: 0.257988]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.382003][G eval loss: 0.299334]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.377422][G train loss: 0.256996]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.380158][G eval loss: 0.293659]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.377831][G train loss: 0.249626]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.377293][G eval loss: 0.283019]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.377160][G train loss: 0.237646]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.373848][G eval loss: 0.269673]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.373718][G train loss: 0.223994]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.369824][G eval loss: 0.259857]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.370476][G train loss: 0.212930]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.365194][G eval loss: 0.250197]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.366172][G train loss: 0.203023]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.360518][G eval loss: 0.241284]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.361905][G train loss: 0.194977]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.355831][G eval loss: 0.235251]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.357568][G train loss: 0.189520]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.350861][G eval loss: 0.231398]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.352868][G train loss: 0.186119]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.345684][G eval loss: 0.229311]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.346718][G train loss: 0.184886]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.340103][G eval loss: 0.228793]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.338089][G train loss: 0.185652]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.334453][G eval loss: 0.229514]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.331555][G train loss: 0.188373]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.328705][G eval loss: 0.230526]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.326454][G train loss: 0.193206]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.322892][G eval loss: 0.232935]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.321105][G train loss: 0.197866]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.317079][G eval loss: 0.236449]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.315571][G train loss: 0.201883]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.311279][G eval loss: 0.240619]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.309902][G train loss: 0.205567]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.305517][G eval loss: 0.243895]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.304190][G train loss: 0.208427]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.299810][G eval loss: 0.246365]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.298585][G train loss: 0.210748]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.294277][G eval loss: 0.249780]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.293248][G train loss: 0.214017]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.288882][G eval loss: 0.254312]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.288187][G train loss: 0.218539]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.283672][G eval loss: 0.260206]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.283350][G train loss: 0.223909]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.279066][G eval loss: 0.266929]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.001 -> score=0.545996\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.0012 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250029][G eval loss: 0.488135]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250027][G train loss: 0.452442]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.480884]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 0.445299]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.481789]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 0.446357]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249966][G eval loss: 0.482801]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249951][G train loss: 0.447487]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249934][G eval loss: 0.482071]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249908][G train loss: 0.446751]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249875][G eval loss: 0.480786]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249831][G train loss: 0.445385]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249781][G eval loss: 0.479800]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249711][G train loss: 0.444297]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249635][G eval loss: 0.478969]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249527][G train loss: 0.443367]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249409][G eval loss: 0.478155]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249245][G train loss: 0.442477]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249113][G eval loss: 0.477260]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248863][G train loss: 0.441548]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248668][G eval loss: 0.476438]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248284][G train loss: 0.440733]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248036][G eval loss: 0.476033]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247475][G train loss: 0.440371]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247225][G eval loss: 0.476447]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246426][G train loss: 0.440850]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246232][G eval loss: 0.477718]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245103][G train loss: 0.442187]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244764][G eval loss: 0.479011]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243203][G train loss: 0.443530]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.242717][G eval loss: 0.481003]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.240595][G train loss: 0.445527]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240149][G eval loss: 0.483543]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.237305][G train loss: 0.448033]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.237060][G eval loss: 0.486337]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.233356][G train loss: 0.450781]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.233171][G eval loss: 0.489349]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.228537][G train loss: 0.453725]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.228156][G eval loss: 0.493107]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.222578][G train loss: 0.457432]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.222078][G eval loss: 0.496804]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.215584][G train loss: 0.461108]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.214442][G eval loss: 0.501257]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.207135][G train loss: 0.465554]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.205286][G eval loss: 0.506636]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.197248][G train loss: 0.470939]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.194578][G eval loss: 0.513022]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.185932][G train loss: 0.477353]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.182770][G eval loss: 0.519771]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.173389][G train loss: 0.484073]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.169409][G eval loss: 0.527805]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.159898][G train loss: 0.492032]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.154896][G eval loss: 0.536974]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.146807][G train loss: 0.500874]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.141264][G eval loss: 0.545967]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.135456][G train loss: 0.509145]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.132065][G eval loss: 0.549414]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.128653][G train loss: 0.510367]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.171534][G eval loss: 0.470105]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.182172][G train loss: 0.406911]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.256844][G eval loss: 0.372172]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.279178][G train loss: 0.304466]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.338666][G eval loss: 0.300978]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.339144][G train loss: 0.264402]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.365665][G eval loss: 0.294163]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.364058][G train loss: 0.258025]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.376424][G eval loss: 0.285275]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.374429][G train loss: 0.249241]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.382327][G eval loss: 0.275208]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.380462][G train loss: 0.239893]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.385651][G eval loss: 0.270825]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.384197][G train loss: 0.235967]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.387314][G eval loss: 0.272514]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.386113][G train loss: 0.237345]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.387862][G eval loss: 0.275164]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.387402][G train loss: 0.239397]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.387545][G eval loss: 0.276629]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.389929][G train loss: 0.239833]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.386511][G eval loss: 0.275223]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.390213][G train loss: 0.237010]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.384834][G eval loss: 0.270751]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.389309][G train loss: 0.231039]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.382573][G eval loss: 0.264236]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.387319][G train loss: 0.223121]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.379790][G eval loss: 0.257046]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.384599][G train loss: 0.214907]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.376520][G eval loss: 0.250375]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.381384][G train loss: 0.207657]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.372781][G eval loss: 0.245062]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.377491][G train loss: 0.201673]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.368598][G eval loss: 0.241030]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.372122][G train loss: 0.196810]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.364030][G eval loss: 0.237832]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.363926][G train loss: 0.192761]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.359046][G eval loss: 0.235029]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.358881][G train loss: 0.189335]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.353758][G eval loss: 0.231900]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.353635][G train loss: 0.186451]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.347969][G eval loss: 0.228549]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.342659][G train loss: 0.191475]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.341786][G eval loss: 0.227882]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.341323][G train loss: 0.185106]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.335289][G eval loss: 0.228326]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.334753][G train loss: 0.185859]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.328512][G eval loss: 0.229661]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.327623][G train loss: 0.187226]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.321400][G eval loss: 0.233462]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.320148][G train loss: 0.189564]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.314149][G eval loss: 0.237732]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.312818][G train loss: 0.193083]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.307754][G eval loss: 0.240511]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.306328][G train loss: 0.196252]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.301104][G eval loss: 0.243729]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.300101][G train loss: 0.199871]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.294527][G eval loss: 0.248180]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.293768][G train loss: 0.205141]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.288195][G eval loss: 0.254685]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.287519][G train loss: 0.211557]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.282133][G eval loss: 0.263037]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.281634][G train loss: 0.219016]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.276426][G eval loss: 0.272504]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.0012 -> score=0.548930\n",
      "\n",
      "----- 2330: lr_g=0.004, lr_d=0.0015 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250053][G eval loss: 0.486357]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250051][G train loss: 0.450664]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.480466]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249988][G train loss: 0.444881]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.482661]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249966][G train loss: 0.447229]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249992][G eval loss: 0.484208]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249979][G train loss: 0.448895]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.483420]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249947][G train loss: 0.448101]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249906][G eval loss: 0.482354]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249865][G train loss: 0.446954]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249816][G eval loss: 0.481525]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249749][G train loss: 0.446022]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249664][G eval loss: 0.480777]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249553][G train loss: 0.445174]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249426][G eval loss: 0.479889]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249247][G train loss: 0.444210]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249066][G eval loss: 0.478855]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248785][G train loss: 0.443143]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248515][G eval loss: 0.477918]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248071][G train loss: 0.442213]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247676][G eval loss: 0.477564]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.246980][G train loss: 0.441904]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246398][G eval loss: 0.478024]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245311][G train loss: 0.442429]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244469][G eval loss: 0.479712]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.242802][G train loss: 0.444183]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.242054][G eval loss: 0.482382]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239542][G train loss: 0.446916]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.238918][G eval loss: 0.485954]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235289][G train loss: 0.450499]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.234947][G eval loss: 0.490688]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.229857][G train loss: 0.455203]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.230249][G eval loss: 0.495887]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.223445][G train loss: 0.460349]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.223998][G eval loss: 0.501540]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.215604][G train loss: 0.465926]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.215616][G eval loss: 0.508089]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.206096][G train loss: 0.472414]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.204897][G eval loss: 0.516195]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.194636][G train loss: 0.480487]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.192031][G eval loss: 0.525875]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.181323][G train loss: 0.490150]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.177024][G eval loss: 0.537688]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.166355][G train loss: 0.501946]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.160059][G eval loss: 0.553489]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.149357][G train loss: 0.517656]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.142196][G eval loss: 0.572087]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.132129][G train loss: 0.536034]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.124038][G eval loss: 0.589683]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.117764][G train loss: 0.552973]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.113250][G eval loss: 0.592801]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.111973][G train loss: 0.552079]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.204176][G eval loss: 0.436860]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.229163][G train loss: 0.358379]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.323044][G eval loss: 0.304448]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.325682][G train loss: 0.268915]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.359985][G eval loss: 0.293978]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.359194][G train loss: 0.258347]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.371355][G eval loss: 0.291053]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.370165][G train loss: 0.255437]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.377741][G eval loss: 0.284081]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.376578][G train loss: 0.248454]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.381398][G eval loss: 0.275930]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.380376][G train loss: 0.240798]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.383095][G eval loss: 0.271036]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.382592][G train loss: 0.236329]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.383280][G eval loss: 0.270697]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.386850][G train loss: 0.235935]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.382344][G eval loss: 0.271719]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.387822][G train loss: 0.236632]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.380439][G eval loss: 0.271212]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.386262][G train loss: 0.235611]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.377676][G eval loss: 0.268627]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.383124][G train loss: 0.232350]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.374074][G eval loss: 0.264747]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.379277][G train loss: 0.227692]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.369676][G eval loss: 0.260829]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.372866][G train loss: 0.223044]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.364721][G eval loss: 0.257978]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.364567][G train loss: 0.219538]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.359002][G eval loss: 0.256791]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.358870][G train loss: 0.217770]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.352584][G eval loss: 0.257255]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.352487][G train loss: 0.217449]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.345549][G eval loss: 0.258539]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.345490][G train loss: 0.217601]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.338205][G eval loss: 0.259809]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.338171][G train loss: 0.217493]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.330296][G eval loss: 0.260816]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.330292][G train loss: 0.217131]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.322008][G eval loss: 0.261636]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.322028][G train loss: 0.216871]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.313513][G eval loss: 0.262010]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.313461][G train loss: 0.217475]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.305104][G eval loss: 0.262493]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.299945][G train loss: 0.229435]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.296754][G eval loss: 0.265101]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.294353][G train loss: 0.223942]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.288675][G eval loss: 0.270819]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.287294][G train loss: 0.228320]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.281095][G eval loss: 0.278441]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.278848][G train loss: 0.237264]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.274939][G eval loss: 0.286563]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.274166][G train loss: 0.242573]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.269342][G eval loss: 0.296677]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.004 lr_d=0.0015 -> score=0.566019\n",
      "\n",
      "----- 2330: lr_g=0.0045, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.490584]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249997][G train loss: 0.454893]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.480607]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.445046]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.479098]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.443735]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.478807]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.443537]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.478677]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.443348]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.478893]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249958][G train loss: 0.443443]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.479305]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249943][G train loss: 0.443725]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.479420]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249923][G train loss: 0.443735]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 0.478809]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249900][G train loss: 0.443077]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249899][G eval loss: 0.477537]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249871][G train loss: 0.441817]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249871][G eval loss: 0.476207]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249835][G train loss: 0.440542]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249835][G eval loss: 0.475197]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249791][G train loss: 0.439599]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249790][G eval loss: 0.474572]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249736][G train loss: 0.439026]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249735][G eval loss: 0.474294]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249668][G train loss: 0.438780]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249665][G eval loss: 0.474307]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.249581][G train loss: 0.438772]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.249580][G eval loss: 0.474691]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.249475][G train loss: 0.439097]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.249478][G eval loss: 0.475442]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.249349][G train loss: 0.439774]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.249367][G eval loss: 0.476297]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.249207][G train loss: 0.440571]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.249221][G eval loss: 0.476926]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.249027][G train loss: 0.441185]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.249063][G eval loss: 0.477370]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.248828][G train loss: 0.441652]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.248885][G eval loss: 0.477730]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.248601][G train loss: 0.442057]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.248677][G eval loss: 0.478092]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.248331][G train loss: 0.442460]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.248428][G eval loss: 0.478452]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.248009][G train loss: 0.442838]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.248138][G eval loss: 0.478776]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.247632][G train loss: 0.443169]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.247792][G eval loss: 0.479096]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.247182][G train loss: 0.443480]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.247396][G eval loss: 0.479495]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.246666][G train loss: 0.443879]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.246929][G eval loss: 0.480044]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.246066][G train loss: 0.444438]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.246405][G eval loss: 0.480437]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.245400][G train loss: 0.444822]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.245777][G eval loss: 0.480869]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244612][G train loss: 0.445241]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.245103][G eval loss: 0.481257]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243759][G train loss: 0.445628]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.244392][G eval loss: 0.481785]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.0045 lr_d=0.0005 -> score=0.726177\n",
      "\n",
      "----- 2330: lr_g=0.0045, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 0.488654]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250007][G train loss: 0.452963]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.480317]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.444755]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 0.480115]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 0.444753]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.480393]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249972][G train loss: 0.445123]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249968][G eval loss: 0.480029]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249956][G train loss: 0.444700]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249947][G eval loss: 0.479672]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249930][G train loss: 0.444221]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249916][G eval loss: 0.479444]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249891][G train loss: 0.443864]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249872][G eval loss: 0.479061]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249837][G train loss: 0.443377]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249814][G eval loss: 0.478097]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249766][G train loss: 0.442365]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249739][G eval loss: 0.476844]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249673][G train loss: 0.441123]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249637][G eval loss: 0.475610]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249547][G train loss: 0.439946]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249496][G eval loss: 0.474753]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249373][G train loss: 0.439155]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249309][G eval loss: 0.474343]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249133][G train loss: 0.438798]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249053][G eval loss: 0.474369]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248808][G train loss: 0.438858]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248745][G eval loss: 0.474683]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248420][G train loss: 0.439153]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248375][G eval loss: 0.475501]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247946][G train loss: 0.439912]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247892][G eval loss: 0.476789]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247334][G train loss: 0.441129]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247294][G eval loss: 0.478341]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246576][G train loss: 0.442622]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246686][G eval loss: 0.479657]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245767][G train loss: 0.443922]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245830][G eval loss: 0.480564]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244683][G train loss: 0.444851]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244729][G eval loss: 0.481513]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243306][G train loss: 0.445849]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243445][G eval loss: 0.482506]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241693][G train loss: 0.446891]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241821][G eval loss: 0.483600]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239698][G train loss: 0.448001]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239844][G eval loss: 0.484812]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.237302][G train loss: 0.449217]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.237495][G eval loss: 0.486197]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.234502][G train loss: 0.450597]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234901][G eval loss: 0.487708]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.0045 lr_d=0.0008 -> score=0.722608\n",
      "\n",
      "----- 2330: lr_g=0.0045, lr_d=0.001 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 0.487474]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250016][G train loss: 0.451783]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.480130]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249991][G train loss: 0.444569]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.480814]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249975][G train loss: 0.445451]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.481445]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249963][G train loss: 0.446175]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249954][G eval loss: 0.480958]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249937][G train loss: 0.445629]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 0.480286]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249890][G train loss: 0.444836]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249862][G eval loss: 0.479712]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249820][G train loss: 0.444132]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249779][G eval loss: 0.479315]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249718][G train loss: 0.443630]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249664][G eval loss: 0.478374]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249575][G train loss: 0.442644]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249507][G eval loss: 0.476965]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249380][G train loss: 0.441247]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249287][G eval loss: 0.475701]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249103][G train loss: 0.440039]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248987][G eval loss: 0.474913]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248716][G train loss: 0.439317]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248559][G eval loss: 0.474735]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248167][G train loss: 0.439194]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.248015][G eval loss: 0.475130]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247470][G train loss: 0.439625]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247286][G eval loss: 0.476100]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246546][G train loss: 0.440580]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246476][G eval loss: 0.477451]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245482][G train loss: 0.441876]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245284][G eval loss: 0.478880]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.243985][G train loss: 0.443231]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243696][G eval loss: 0.480673]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242020][G train loss: 0.444964]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241759][G eval loss: 0.482512]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239628][G train loss: 0.446787]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.239362][G eval loss: 0.484416]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236697][G train loss: 0.448712]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.236524][G eval loss: 0.486535]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.233240][G train loss: 0.450882]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.233202][G eval loss: 0.488782]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.229237][G train loss: 0.453179]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.229208][G eval loss: 0.491238]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.224514][G train loss: 0.455637]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.224428][G eval loss: 0.494069]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.218956][G train loss: 0.458457]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.218723][G eval loss: 0.497387]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.212531][G train loss: 0.461780]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.212003][G eval loss: 0.501425]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.205186][G train loss: 0.465842]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.204275][G eval loss: 0.506259]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.196830][G train loss: 0.470694]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.195498][G eval loss: 0.511693]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.187500][G train loss: 0.476115]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.186061][G eval loss: 0.517594]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.177358][G train loss: 0.482026]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.175888][G eval loss: 0.523689]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.166694][G train loss: 0.488090]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.165376][G eval loss: 0.529798]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.156413][G train loss: 0.494026]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.156395][G eval loss: 0.533155]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.148497][G train loss: 0.496870]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.157506][G eval loss: 0.517440]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.151404][G train loss: 0.479060]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.232164][G eval loss: 0.391521]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.231747][G train loss: 0.347500]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.301599][G eval loss: 0.333274]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.297995][G train loss: 0.295180]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.337479][G eval loss: 0.317379]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.331757][G train loss: 0.280467]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.353748][G eval loss: 0.299340]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.347641][G train loss: 0.263467]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.362668][G eval loss: 0.284709]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.357199][G train loss: 0.249229]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.367544][G eval loss: 0.273778]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.363034][G train loss: 0.238131]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.369908][G eval loss: 0.270752]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.366497][G train loss: 0.233881]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.370471][G eval loss: 0.274042]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.368302][G train loss: 0.234567]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.369885][G eval loss: 0.275546]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.367968][G train loss: 0.232543]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.368643][G eval loss: 0.271358]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.367569][G train loss: 0.225357]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.366992][G eval loss: 0.260448]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.368943][G train loss: 0.214115]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.348298][G eval loss: 0.253464]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.356043][G train loss: 0.219990]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.306436][G eval loss: 0.298624]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.350428][G train loss: 0.212928]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.288992][G eval loss: 0.319047]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.344229][G train loss: 0.208798]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.285324][G eval loss: 0.320711]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.342215][G train loss: 0.203370]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.282684][G eval loss: 0.319792]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.339503][G train loss: 0.200187]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.280877][G eval loss: 0.316259]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.336303][G train loss: 0.197652]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.281503][G eval loss: 0.306680]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.333381][G train loss: 0.194004]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.290731][G eval loss: 0.285461]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.330805][G train loss: 0.189968]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.300315][G eval loss: 0.265075]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.325367][G train loss: 0.187706]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.331310][G eval loss: 0.209196]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.327249][G train loss: 0.172873]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.327344][G eval loss: 0.209741]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.326566][G train loss: 0.167395]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.322151][G eval loss: 0.212010]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.321269][G train loss: 0.169878]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.316756][G eval loss: 0.215788]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.315863][G train loss: 0.174003]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.311237][G eval loss: 0.221761]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.310400][G train loss: 0.179114]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.305647][G eval loss: 0.229072]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.304909][G train loss: 0.185124]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.300059][G eval loss: 0.234080]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.299323][G train loss: 0.191763]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.294607][G eval loss: 0.239520]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.286147][G train loss: 0.214085]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.289293][G eval loss: 0.246759]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.281608][G train loss: 0.219936]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.284123][G eval loss: 0.255476]\n",
      "[Epoch 64/200][Batch 1/1][D train loss: 0.279870][G train loss: 0.223058]\n",
      "[Epoch 64/200][Batch 1/1][D eval loss: 0.279546][G eval loss: 0.263768]\n",
      "[Epoch 65/200][Batch 1/1][D train loss: 0.275477][G train loss: 0.230663]\n",
      "[Epoch 65/200][Batch 1/1][D eval loss: 0.275272][G eval loss: 0.271900]\n",
      "[Epoch 66/200][Batch 1/1][D train loss: 0.271397][G train loss: 0.238028]\n",
      "[Epoch 66/200][Batch 1/1][D eval loss: 0.271242][G eval loss: 0.280026]\n",
      "[Epoch 67/200][Batch 1/1][D train loss: 0.267585][G train loss: 0.245684]\n",
      "[Epoch 67/200][Batch 1/1][D eval loss: 0.267535][G eval loss: 0.288035]\n",
      "[Epoch 68/200][Batch 1/1][D train loss: 0.264074][G train loss: 0.253201]\n",
      "[Epoch 68/200][Batch 1/1][D eval loss: 0.264112][G eval loss: 0.296236]\n",
      "[Epoch 69/200][Batch 1/1][D train loss: 0.260844][G train loss: 0.260927]\n",
      "[Epoch 69/200][Batch 1/1][D eval loss: 0.260994][G eval loss: 0.304872]\n",
      "[Epoch 70/200][Batch 1/1][D train loss: 0.257923][G train loss: 0.269137]\n",
      "[Epoch 70/200][Batch 1/1][D eval loss: 0.258212][G eval loss: 0.313930]\n",
      "[Epoch 71/200][Batch 1/1][D train loss: 0.255335][G train loss: 0.277703]\n",
      "[Epoch 71/200][Batch 1/1][D eval loss: 0.255867][G eval loss: 0.323088]\n",
      "[Epoch 72/200][Batch 1/1][D train loss: 0.253157][G train loss: 0.286559]\n",
      "[Epoch 72/200][Batch 1/1][D eval loss: 0.253936][G eval loss: 0.332581]\n",
      "[Epoch 73/200][Batch 1/1][D train loss: 0.251379][G train loss: 0.296011]\n",
      "[Epoch 73/200][Batch 1/1][D eval loss: 0.252358][G eval loss: 0.342638]\n",
      "[Epoch 74/200][Batch 1/1][D train loss: 0.249970][G train loss: 0.305933]\n",
      "[Epoch 74/200][Batch 1/1][D eval loss: 0.251218][G eval loss: 0.352193]\n",
      "[Epoch 75/200][Batch 1/1][D train loss: 0.248965][G train loss: 0.315589]\n",
      "[Epoch 75/200][Batch 1/1][D eval loss: 0.250388][G eval loss: 0.361975]\n",
      "[Epoch 76/200][Batch 1/1][D train loss: 0.248265][G train loss: 0.325442]\n",
      "[Epoch 76/200][Batch 1/1][D eval loss: 0.249853][G eval loss: 0.372170]\n",
      "[Epoch 77/200][Batch 1/1][D train loss: 0.247861][G train loss: 0.335676]\n",
      "[Epoch 77/200][Batch 1/1][D eval loss: 0.249638][G eval loss: 0.382881]\n",
      "[Epoch 78/200][Batch 1/1][D train loss: 0.247769][G train loss: 0.346453]\n",
      "[Epoch 78/200][Batch 1/1][D eval loss: 0.249700][G eval loss: 0.393696]\n",
      "[Epoch 79/200][Batch 1/1][D train loss: 0.247943][G train loss: 0.357436]\n",
      "[Epoch 79/200][Batch 1/1][D eval loss: 0.250015][G eval loss: 0.404361]\n",
      "[Epoch 80/200][Batch 1/1][D train loss: 0.248353][G train loss: 0.368231]\n",
      "[Epoch 80/200][Batch 1/1][D eval loss: 0.250497][G eval loss: 0.413919]\n",
      "[Epoch 81/200][Batch 1/1][D train loss: 0.248902][G train loss: 0.378042]\n",
      "[Epoch 81/200][Batch 1/1][D eval loss: 0.251080][G eval loss: 0.422843]\n",
      "[Epoch 82/200][Batch 1/1][D train loss: 0.249556][G train loss: 0.387148]\n",
      "[Epoch 82/200][Batch 1/1][D eval loss: 0.251706][G eval loss: 0.430324]\n",
      "[Epoch 83/200][Batch 1/1][D train loss: 0.250210][G train loss: 0.394785]\n",
      "[Epoch 83/200][Batch 1/1][D eval loss: 0.252302][G eval loss: 0.436428]\n",
      "[Epoch 84/200][Batch 1/1][D train loss: 0.250809][G train loss: 0.401021]\n",
      "[Epoch 84/200][Batch 1/1][D eval loss: 0.252838][G eval loss: 0.441544]\n",
      "[Epoch 85/200][Batch 1/1][D train loss: 0.251352][G train loss: 0.406275]\n",
      "[Epoch 85/200][Batch 1/1][D eval loss: 0.253260][G eval loss: 0.445406]\n",
      "[Epoch 86/200][Batch 1/1][D train loss: 0.251757][G train loss: 0.410343]\n",
      "[Epoch 86/200][Batch 1/1][D eval loss: 0.253597][G eval loss: 0.448339]\n",
      "[Epoch 87/200][Batch 1/1][D train loss: 0.251953][G train loss: 0.413562]\n",
      "[Epoch 87/200][Batch 1/1][D eval loss: 0.253870][G eval loss: 0.450520]\n",
      "[Epoch 88/200][Batch 1/1][D train loss: 0.252008][G train loss: 0.415769]\n",
      "[Epoch 88/200][Batch 1/1][D eval loss: 0.254059][G eval loss: 0.451799]\n",
      "[Epoch 89/200][Batch 1/1][D train loss: 0.252747][G train loss: 0.416864]\n",
      "[Epoch 89/200][Batch 1/1][D eval loss: 0.254161][G eval loss: 0.452247]\n",
      "[Epoch 90/200][Batch 1/1][D train loss: 0.253180][G train loss: 0.417516]\n",
      "[Epoch 90/200][Batch 1/1][D eval loss: 0.254260][G eval loss: 0.452356]\n",
      "[Epoch 91/200][Batch 1/1][D train loss: 0.253031][G train loss: 0.417841]\n",
      "[Epoch 91/200][Batch 1/1][D eval loss: 0.254270][G eval loss: 0.451865]\n",
      "[Epoch 92/200][Batch 1/1][D train loss: 0.252556][G train loss: 0.416694]\n",
      "[Epoch 92/200][Batch 1/1][D eval loss: 0.254210][G eval loss: 0.450973]\n",
      "[Epoch 93/200][Batch 1/1][D train loss: 0.252172][G train loss: 0.415052]\n",
      "[Epoch 93/200][Batch 1/1][D eval loss: 0.254035][G eval loss: 0.449408]\n",
      "[Epoch 94/200][Batch 1/1][D train loss: 0.252145][G train loss: 0.413450]\n",
      "[Epoch 94/200][Batch 1/1][D eval loss: 0.253818][G eval loss: 0.447643]\n",
      "[Epoch 95/200][Batch 1/1][D train loss: 0.252030][G train loss: 0.411573]\n",
      "[Epoch 95/200][Batch 1/1][D eval loss: 0.253565][G eval loss: 0.445471]\n",
      "[Epoch 96/200][Batch 1/1][D train loss: 0.251819][G train loss: 0.409203]\n",
      "[Epoch 96/200][Batch 1/1][D eval loss: 0.253280][G eval loss: 0.442911]\n",
      "[Epoch 97/200][Batch 1/1][D train loss: 0.251581][G train loss: 0.406370]\n",
      "[Epoch 97/200][Batch 1/1][D eval loss: 0.252965][G eval loss: 0.440030]\n",
      "[Epoch 98/200][Batch 1/1][D train loss: 0.251280][G train loss: 0.403307]\n",
      "[Epoch 98/200][Batch 1/1][D eval loss: 0.252639][G eval loss: 0.436472]\n",
      "[Epoch 99/200][Batch 1/1][D train loss: 0.250941][G train loss: 0.399991]\n",
      "[Epoch 99/200][Batch 1/1][D eval loss: 0.252300][G eval loss: 0.432968]\n",
      "[Epoch 100/200][Batch 1/1][D train loss: 0.250579][G train loss: 0.396389]\n",
      "[Epoch 100/200][Batch 1/1][D eval loss: 0.251959][G eval loss: 0.429211]\n",
      "[Epoch 101/200][Batch 1/1][D train loss: 0.250211][G train loss: 0.392543]\n",
      "[Epoch 101/200][Batch 1/1][D eval loss: 0.251629][G eval loss: 0.425342]\n",
      "[Epoch 102/200][Batch 1/1][D train loss: 0.249847][G train loss: 0.388482]\n",
      "[Epoch 102/200][Batch 1/1][D eval loss: 0.251311][G eval loss: 0.421625]\n",
      "[Epoch 103/200][Batch 1/1][D train loss: 0.249493][G train loss: 0.384256]\n",
      "[Epoch 103/200][Batch 1/1][D eval loss: 0.251011][G eval loss: 0.417917]\n",
      "[Epoch 104/200][Batch 1/1][D train loss: 0.249158][G train loss: 0.379960]\n",
      "[Epoch 104/200][Batch 1/1][D eval loss: 0.250738][G eval loss: 0.414101]\n",
      "[Epoch 105/200][Batch 1/1][D train loss: 0.248846][G train loss: 0.375583]\n",
      "[Epoch 105/200][Batch 1/1][D eval loss: 0.250530][G eval loss: 0.411172]\n",
      "[Epoch 106/200][Batch 1/1][D train loss: 0.248597][G train loss: 0.371944]\n",
      "[Epoch 106/200][Batch 1/1][D eval loss: 0.250364][G eval loss: 0.408567]\n",
      "[Epoch 107/200][Batch 1/1][D train loss: 0.248390][G train loss: 0.368587]\n",
      "[Epoch 107/200][Batch 1/1][D eval loss: 0.250213][G eval loss: 0.405888]\n",
      "[Epoch 108/200][Batch 1/1][D train loss: 0.248205][G train loss: 0.365143]\n",
      "[Epoch 108/200][Batch 1/1][D eval loss: 0.250076][G eval loss: 0.403061]\n",
      "[Epoch 109/200][Batch 1/1][D train loss: 0.248043][G train loss: 0.361695]\n",
      "[Epoch 109/200][Batch 1/1][D eval loss: 0.249974][G eval loss: 0.400128]\n",
      "[Epoch 110/200][Batch 1/1][D train loss: 0.247906][G train loss: 0.358291]\n",
      "[Epoch 110/200][Batch 1/1][D eval loss: 0.249896][G eval loss: 0.397152]\n",
      "[Epoch 111/200][Batch 1/1][D train loss: 0.247792][G train loss: 0.354985]\n",
      "[Epoch 111/200][Batch 1/1][D eval loss: 0.249837][G eval loss: 0.394066]\n",
      "[Epoch 112/200][Batch 1/1][D train loss: 0.247699][G train loss: 0.351644]\n",
      "[Epoch 112/200][Batch 1/1][D eval loss: 0.249797][G eval loss: 0.390917]\n",
      "[Epoch 113/200][Batch 1/1][D train loss: 0.247628][G train loss: 0.348216]\n",
      "[Epoch 113/200][Batch 1/1][D eval loss: 0.249778][G eval loss: 0.387731]\n",
      "[Epoch 114/200][Batch 1/1][D train loss: 0.247579][G train loss: 0.344888]\n",
      "[Epoch 114/200][Batch 1/1][D eval loss: 0.249784][G eval loss: 0.384656]\n",
      "[Epoch 115/200][Batch 1/1][D train loss: 0.247552][G train loss: 0.341660]\n",
      "[Epoch 115/200][Batch 1/1][D eval loss: 0.249805][G eval loss: 0.381688]\n",
      "[Epoch 116/200][Batch 1/1][D train loss: 0.247544][G train loss: 0.338572]\n",
      "[Epoch 116/200][Batch 1/1][D eval loss: 0.249832][G eval loss: 0.378851]\n",
      "[Epoch 117/200][Batch 1/1][D train loss: 0.247554][G train loss: 0.335618]\n",
      "[Epoch 117/200][Batch 1/1][D eval loss: 0.249851][G eval loss: 0.376042]\n",
      "[Epoch 118/200][Batch 1/1][D train loss: 0.247576][G train loss: 0.332802]\n",
      "[Epoch 118/200][Batch 1/1][D eval loss: 0.249868][G eval loss: 0.373545]\n",
      "[Epoch 119/200][Batch 1/1][D train loss: 0.247601][G train loss: 0.330347]\n",
      "[Epoch 119/200][Batch 1/1][D eval loss: 0.249909][G eval loss: 0.371324]\n",
      "[Epoch 120/200][Batch 1/1][D train loss: 0.247637][G train loss: 0.328184]\n",
      "[Epoch 120/200][Batch 1/1][D eval loss: 0.249950][G eval loss: 0.369210]\n",
      "[Epoch 121/200][Batch 1/1][D train loss: 0.247675][G train loss: 0.326162]\n",
      "[Epoch 121/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.367344]\n",
      "[Epoch 122/200][Batch 1/1][D train loss: 0.247711][G train loss: 0.324482]\n",
      "[Epoch 122/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 0.366939]\n",
      "[Epoch 123/200][Batch 1/1][D train loss: 0.247713][G train loss: 0.324185]\n",
      "[Epoch 123/200][Batch 1/1][D eval loss: 0.250035][G eval loss: 0.366623]\n",
      "[Epoch 124/200][Batch 1/1][D train loss: 0.247704][G train loss: 0.324063]\n",
      "[Epoch 124/200][Batch 1/1][D eval loss: 0.250022][G eval loss: 0.366560]\n",
      "[Epoch 125/200][Batch 1/1][D train loss: 0.247682][G train loss: 0.324116]\n",
      "[Epoch 125/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.366639]\n",
      "[Epoch 126/200][Batch 1/1][D train loss: 0.247661][G train loss: 0.324240]\n",
      "[Epoch 126/200][Batch 1/1][D eval loss: 0.249903][G eval loss: 0.366961]\n",
      "[Epoch 127/200][Batch 1/1][D train loss: 0.247630][G train loss: 0.324556]\n",
      "[Epoch 127/200][Batch 1/1][D eval loss: 0.249827][G eval loss: 0.367342]\n",
      "[Epoch 128/200][Batch 1/1][D train loss: 0.247599][G train loss: 0.325008]\n",
      "[Epoch 128/200][Batch 1/1][D eval loss: 0.249716][G eval loss: 0.367786]\n",
      "[Epoch 129/200][Batch 1/1][D train loss: 0.247568][G train loss: 0.325446]\n",
      "[Epoch 129/200][Batch 1/1][D eval loss: 0.249617][G eval loss: 0.368859]\n",
      "[Epoch 130/200][Batch 1/1][D train loss: 0.247519][G train loss: 0.326316]\n",
      "[Epoch 130/200][Batch 1/1][D eval loss: 0.249538][G eval loss: 0.370349]\n",
      "[Epoch 131/200][Batch 1/1][D train loss: 0.247473][G train loss: 0.327497]\n",
      "[Epoch 131/200][Batch 1/1][D eval loss: 0.249468][G eval loss: 0.371283]\n",
      "[Epoch 132/200][Batch 1/1][D train loss: 0.247427][G train loss: 0.328879]\n",
      "[Epoch 132/200][Batch 1/1][D eval loss: 0.249394][G eval loss: 0.371945]\n",
      "[Epoch 133/200][Batch 1/1][D train loss: 0.247380][G train loss: 0.330342]\n",
      "[Epoch 133/200][Batch 1/1][D eval loss: 0.249317][G eval loss: 0.372550]\n",
      "[Epoch 134/200][Batch 1/1][D train loss: 0.247329][G train loss: 0.331750]\n",
      "[Epoch 134/200][Batch 1/1][D eval loss: 0.249209][G eval loss: 0.373332]\n",
      "[Epoch 135/200][Batch 1/1][D train loss: 0.247274][G train loss: 0.332931]\n",
      "[Epoch 135/200][Batch 1/1][D eval loss: 0.249106][G eval loss: 0.374430]\n",
      "[Epoch 136/200][Batch 1/1][D train loss: 0.247229][G train loss: 0.334114]\n",
      "[Epoch 136/200][Batch 1/1][D eval loss: 0.248985][G eval loss: 0.375811]\n",
      "[Epoch 137/200][Batch 1/1][D train loss: 0.247183][G train loss: 0.335243]\n",
      "[Epoch 137/200][Batch 1/1][D eval loss: 0.248836][G eval loss: 0.377297]\n",
      "[Epoch 138/200][Batch 1/1][D train loss: 0.247105][G train loss: 0.336369]\n",
      "[Epoch 138/200][Batch 1/1][D eval loss: 0.248670][G eval loss: 0.379016]\n",
      "[Epoch 139/200][Batch 1/1][D train loss: 0.247044][G train loss: 0.337653]\n",
      "[Epoch 139/200][Batch 1/1][D eval loss: 0.248455][G eval loss: 0.380525]\n",
      "[Epoch 140/200][Batch 1/1][D train loss: 0.246915][G train loss: 0.339226]\n",
      "[Epoch 140/200][Batch 1/1][D eval loss: 0.248310][G eval loss: 0.381877]\n",
      "[Epoch 141/200][Batch 1/1][D train loss: 0.246851][G train loss: 0.340541]\n",
      "[Epoch 141/200][Batch 1/1][D eval loss: 0.248095][G eval loss: 0.384051]\n",
      "[Epoch 142/200][Batch 1/1][D train loss: 0.246676][G train loss: 0.342272]\n",
      "[Epoch 142/200][Batch 1/1][D eval loss: 0.247882][G eval loss: 0.386396]\n",
      "[Epoch 143/200][Batch 1/1][D train loss: 0.246593][G train loss: 0.344223]\n",
      "[Epoch 143/200][Batch 1/1][D eval loss: 0.247737][G eval loss: 0.388331]\n",
      "[Epoch 144/200][Batch 1/1][D train loss: 0.246631][G train loss: 0.345926]\n",
      "[Epoch 144/200][Batch 1/1][D eval loss: 0.247647][G eval loss: 0.389771]\n",
      "[Epoch 145/200][Batch 1/1][D train loss: 0.246590][G train loss: 0.347531]\n",
      "[Epoch 145/200][Batch 1/1][D eval loss: 0.247413][G eval loss: 0.391276]\n",
      "[Epoch 146/200][Batch 1/1][D train loss: 0.246692][G train loss: 0.348432]\n",
      "[Epoch 146/200][Batch 1/1][D eval loss: 0.247346][G eval loss: 0.392587]\n",
      "[Epoch 147/200][Batch 1/1][D train loss: 0.246687][G train loss: 0.349903]\n",
      "[Epoch 147/200][Batch 1/1][D eval loss: 0.247361][G eval loss: 0.393939]\n",
      "[Epoch 148/200][Batch 1/1][D train loss: 0.246649][G train loss: 0.350706]\n",
      "[Epoch 148/200][Batch 1/1][D eval loss: 0.247181][G eval loss: 0.395810]\n",
      "[Epoch 149/200][Batch 1/1][D train loss: 0.246582][G train loss: 0.351709]\n",
      "[Epoch 149/200][Batch 1/1][D eval loss: 0.246682][G eval loss: 0.397806]\n",
      "[Epoch 150/200][Batch 1/1][D train loss: 0.246521][G train loss: 0.352589]\n",
      "[Epoch 150/200][Batch 1/1][D eval loss: 0.246229][G eval loss: 0.399321]\n",
      "[Epoch 151/200][Batch 1/1][D train loss: 0.246342][G train loss: 0.353408]\n",
      "[Epoch 151/200][Batch 1/1][D eval loss: 0.245499][G eval loss: 0.400962]\n",
      "[Epoch 152/200][Batch 1/1][D train loss: 0.246147][G train loss: 0.353973]\n",
      "[Epoch 152/200][Batch 1/1][D eval loss: 0.244807][G eval loss: 0.403215]\n",
      "[Epoch 153/200][Batch 1/1][D train loss: 0.245880][G train loss: 0.355053]\n",
      "[Epoch 153/200][Batch 1/1][D eval loss: 0.244252][G eval loss: 0.405163]\n",
      "[Epoch 154/200][Batch 1/1][D train loss: 0.245652][G train loss: 0.356129]\n",
      "[Epoch 154/200][Batch 1/1][D eval loss: 0.243572][G eval loss: 0.406967]\n",
      "[Epoch 155/200][Batch 1/1][D train loss: 0.245386][G train loss: 0.356471]\n",
      "[Epoch 155/200][Batch 1/1][D eval loss: 0.242784][G eval loss: 0.408374]\n",
      "[Epoch 156/200][Batch 1/1][D train loss: 0.244929][G train loss: 0.356642]\n",
      "[Epoch 156/200][Batch 1/1][D eval loss: 0.241832][G eval loss: 0.411921]\n",
      "[Epoch 157/200][Batch 1/1][D train loss: 0.244497][G train loss: 0.358643]\n",
      "[Epoch 157/200][Batch 1/1][D eval loss: 0.241294][G eval loss: 0.414070]\n",
      "[Epoch 158/200][Batch 1/1][D train loss: 0.244221][G train loss: 0.359444]\n",
      "[Epoch 158/200][Batch 1/1][D eval loss: 0.240672][G eval loss: 0.415436]\n",
      "[Epoch 159/200][Batch 1/1][D train loss: 0.243723][G train loss: 0.360467]\n",
      "[Epoch 159/200][Batch 1/1][D eval loss: 0.240017][G eval loss: 0.417177]\n",
      "[Epoch 160/200][Batch 1/1][D train loss: 0.243282][G train loss: 0.361345]\n",
      "[Epoch 160/200][Batch 1/1][D eval loss: 0.239031][G eval loss: 0.418216]\n",
      "[Epoch 161/200][Batch 1/1][D train loss: 0.242569][G train loss: 0.361890]\n",
      "[Epoch 161/200][Batch 1/1][D eval loss: 0.238033][G eval loss: 0.424316]\n",
      "[Epoch 162/200][Batch 1/1][D train loss: 0.241930][G train loss: 0.364442]\n",
      "[Epoch 162/200][Batch 1/1][D eval loss: 0.237466][G eval loss: 0.427914]\n",
      "[Epoch 163/200][Batch 1/1][D train loss: 0.241523][G train loss: 0.366965]\n",
      "[Epoch 163/200][Batch 1/1][D eval loss: 0.236399][G eval loss: 0.431361]\n",
      "[Epoch 164/200][Batch 1/1][D train loss: 0.241180][G train loss: 0.367701]\n",
      "[Epoch 164/200][Batch 1/1][D eval loss: 0.235747][G eval loss: 0.431144]\n",
      "[Epoch 165/200][Batch 1/1][D train loss: 0.240745][G train loss: 0.367132]\n",
      "[Epoch 165/200][Batch 1/1][D eval loss: 0.234604][G eval loss: 0.431697]\n",
      "[Epoch 166/200][Batch 1/1][D train loss: 0.240091][G train loss: 0.366831]\n",
      "[Epoch 166/200][Batch 1/1][D eval loss: 0.233325][G eval loss: 0.432766]\n",
      "[Epoch 167/200][Batch 1/1][D train loss: 0.239610][G train loss: 0.365519]\n",
      "[Epoch 167/200][Batch 1/1][D eval loss: 0.232395][G eval loss: 0.431385]\n",
      "[Epoch 168/200][Batch 1/1][D train loss: 0.238872][G train loss: 0.363847]\n",
      "[Epoch 168/200][Batch 1/1][D eval loss: 0.231512][G eval loss: 0.429229]\n",
      "[Epoch 169/200][Batch 1/1][D train loss: 0.237818][G train loss: 0.361755]\n",
      "[Epoch 169/200][Batch 1/1][D eval loss: 0.230690][G eval loss: 0.428703]\n",
      "[Epoch 170/200][Batch 1/1][D train loss: 0.237200][G train loss: 0.360283]\n",
      "[Epoch 170/200][Batch 1/1][D eval loss: 0.229340][G eval loss: 0.432157]\n",
      "[Epoch 171/200][Batch 1/1][D train loss: 0.236432][G train loss: 0.359974]\n",
      "[Epoch 171/200][Batch 1/1][D eval loss: 0.228558][G eval loss: 0.434020]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.0045 lr_d=0.001 -> score=0.662578\n",
      "\n",
      "----- 2330: lr_g=0.0045, lr_d=0.0012 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 0.486292]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250027][G train loss: 0.450600]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.479923]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 0.444362]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.481482]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.446120]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 0.482475]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249953][G train loss: 0.447205]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249937][G eval loss: 0.481927]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249912][G train loss: 0.446598]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249876][G eval loss: 0.481018]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249835][G train loss: 0.445568]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249785][G eval loss: 0.480284]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249723][G train loss: 0.444704]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249646][G eval loss: 0.480220]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249550][G train loss: 0.444537]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249458][G eval loss: 0.479573]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249315][G train loss: 0.443844]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249193][G eval loss: 0.478328]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248978][G train loss: 0.442610]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248807][G eval loss: 0.477089]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248471][G train loss: 0.441428]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248228][G eval loss: 0.476347]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247733][G train loss: 0.440754]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247507][G eval loss: 0.476152]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246800][G train loss: 0.440618]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246524][G eval loss: 0.475808]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245552][G train loss: 0.440313]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.245153][G eval loss: 0.476429]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243831][G train loss: 0.440921]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.243316][G eval loss: 0.478305]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.241534][G train loss: 0.442749]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.240931][G eval loss: 0.480987]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.238556][G train loss: 0.445365]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.238057][G eval loss: 0.483880]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.234955][G train loss: 0.448192]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.234699][G eval loss: 0.486707]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.230704][G train loss: 0.450998]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.230272][G eval loss: 0.490388]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.225316][G train loss: 0.454681]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.224907][G eval loss: 0.494166]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.218991][G train loss: 0.458498]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.218360][G eval loss: 0.498574]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.211545][G train loss: 0.462974]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.210325][G eval loss: 0.503805]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.202705][G train loss: 0.468228]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.200473][G eval loss: 0.510305]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.192220][G train loss: 0.474723]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.189452][G eval loss: 0.517455]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.180458][G train loss: 0.481873]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.177219][G eval loss: 0.525273]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.167460][G train loss: 0.489663]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.163555][G eval loss: 0.534419]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.154040][G train loss: 0.498758]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.148863][G eval loss: 0.545109]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.141038][G train loss: 0.509380]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.135718][G eval loss: 0.554413]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.130157][G train loss: 0.518363]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.130735][G eval loss: 0.549266]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.127296][G train loss: 0.511469]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.224962][G eval loss: 0.385034]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.233680][G train loss: 0.332835]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.323702][G eval loss: 0.307164]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.323383][G train loss: 0.270017]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.353376][G eval loss: 0.303166]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.351374][G train loss: 0.266807]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.365669][G eval loss: 0.294487]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.363509][G train loss: 0.258097]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.372865][G eval loss: 0.283133]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.370912][G train loss: 0.246987]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.377203][G eval loss: 0.273598]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.375619][G train loss: 0.237019]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.379643][G eval loss: 0.266727]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.378488][G train loss: 0.228800]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.380693][G eval loss: 0.262613]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.381451][G train loss: 0.222563]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.380809][G eval loss: 0.259467]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.383567][G train loss: 0.217323]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.379986][G eval loss: 0.253299]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.381912][G train loss: 0.211141]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.378717][G eval loss: 0.252664]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.382479][G train loss: 0.207974]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.376705][G eval loss: 0.249009]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.380605][G train loss: 0.202747]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.374023][G eval loss: 0.242573]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.377737][G train loss: 0.195526]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.370691][G eval loss: 0.234509]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.373895][G train loss: 0.187445]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.366952][G eval loss: 0.226386]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.368625][G train loss: 0.179756]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.362875][G eval loss: 0.218714]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.362781][G train loss: 0.173599]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.358369][G eval loss: 0.212533]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.358320][G train loss: 0.169914]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.353433][G eval loss: 0.208596]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.353138][G train loss: 0.168225]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.347832][G eval loss: 0.207256]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.346710][G train loss: 0.169180]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.341810][G eval loss: 0.208164]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.339302][G train loss: 0.172413]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.335417][G eval loss: 0.210681]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.331833][G train loss: 0.177041]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.328797][G eval loss: 0.214336]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.324647][G train loss: 0.180788]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.321925][G eval loss: 0.217915]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.317882][G train loss: 0.183787]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.314893][G eval loss: 0.221908]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.311574][G train loss: 0.186409]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.307778][G eval loss: 0.226386]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.305399][G train loss: 0.189210]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.300681][G eval loss: 0.232166]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.299433][G train loss: 0.193134]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.293688][G eval loss: 0.239575]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.293331][G train loss: 0.198927]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.287007][G eval loss: 0.249333]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.286893][G train loss: 0.207356]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.280681][G eval loss: 0.261383]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.0045 lr_d=0.0012 -> score=0.542064\n",
      "\n",
      "----- 2330: lr_g=0.0045, lr_d=0.0015 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.0045, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250054][G eval loss: 0.484514]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250052][G train loss: 0.448822]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.479501]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 0.443939]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249976][G eval loss: 0.482344]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249967][G train loss: 0.446981]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249993][G eval loss: 0.483870]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249981][G train loss: 0.448601]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249971][G eval loss: 0.483271]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249950][G train loss: 0.447942]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249909][G eval loss: 0.482588]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249871][G train loss: 0.447138]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249817][G eval loss: 0.482004]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249755][G train loss: 0.446425]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249662][G eval loss: 0.481180]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249557][G train loss: 0.445496]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249432][G eval loss: 0.479772]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249263][G train loss: 0.444043]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249089][G eval loss: 0.478030]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248821][G train loss: 0.442313]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248555][G eval loss: 0.476703]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248132][G train loss: 0.441044]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247733][G eval loss: 0.476372]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247067][G train loss: 0.440783]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246496][G eval loss: 0.477098]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245448][G train loss: 0.441572]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244611][G eval loss: 0.479057]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.0045 lr_d=0.0015 -> score=0.723668\n",
      "\n",
      "----- 2330: lr_g=0.005, lr_d=0.0005 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0005\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250000][G eval loss: 0.488970]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.249998][G train loss: 0.453278]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 0.480091]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.444559]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249989][G eval loss: 0.478667]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249985][G train loss: 0.443364]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249983][G eval loss: 0.478368]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249977][G train loss: 0.443093]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.478586]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249970][G train loss: 0.443204]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249969][G eval loss: 0.479261]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249959][G train loss: 0.443731]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.479844]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249943][G train loss: 0.444173]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.479557]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249924][G train loss: 0.443800]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249922][G eval loss: 0.478221]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249901][G train loss: 0.442470]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249900][G eval loss: 0.476488]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249872][G train loss: 0.440822]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249871][G eval loss: 0.475155]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249837][G train loss: 0.439593]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249835][G eval loss: 0.474289]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249792][G train loss: 0.438784]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249789][G eval loss: 0.473846]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249735][G train loss: 0.438334]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249730][G eval loss: 0.473887]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.249663][G train loss: 0.438320]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.249658][G eval loss: 0.474331]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.005 lr_d=0.0005 -> score=0.723989\n",
      "\n",
      "----- 2330: lr_g=0.005, lr_d=0.0008 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250010][G eval loss: 0.487040]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250007][G train loss: 0.451348]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249998][G eval loss: 0.479802]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249994][G train loss: 0.444269]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249987][G eval loss: 0.479689]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 0.444385]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.479956]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 0.444681]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.479936]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249958][G train loss: 0.444553]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 0.480033]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249932][G train loss: 0.444503]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249917][G eval loss: 0.479969]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249893][G train loss: 0.444298]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249874][G eval loss: 0.479174]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249840][G train loss: 0.443417]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249818][G eval loss: 0.477483]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249771][G train loss: 0.441733]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249746][G eval loss: 0.475736]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249681][G train loss: 0.440071]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249646][G eval loss: 0.474486]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249558][G train loss: 0.438925]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249507][G eval loss: 0.473775]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249387][G train loss: 0.438272]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249318][G eval loss: 0.473567]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249150][G train loss: 0.438057]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249064][G eval loss: 0.473936]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248825][G train loss: 0.438371]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248729][G eval loss: 0.474770]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248410][G train loss: 0.439148]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248362][G eval loss: 0.475934]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247941][G train loss: 0.440278]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247888][G eval loss: 0.477311]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247337][G train loss: 0.441616]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247296][G eval loss: 0.478561]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246585][G train loss: 0.442864]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246727][G eval loss: 0.479551]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245818][G train loss: 0.443867]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245888][G eval loss: 0.480336]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244753][G train loss: 0.444685]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244817][G eval loss: 0.481186]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243413][G train loss: 0.445572]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.243511][G eval loss: 0.482020]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.241793][G train loss: 0.446433]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241891][G eval loss: 0.482964]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.239818][G train loss: 0.447370]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239967][G eval loss: 0.484093]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.005 lr_d=0.0008 -> score=0.724060\n",
      "\n",
      "----- 2330: lr_g=0.005, lr_d=0.001 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250018][G eval loss: 0.485860]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250016][G train loss: 0.450168]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249996][G eval loss: 0.479617]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249992][G train loss: 0.444084]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249982][G eval loss: 0.480389]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249975][G train loss: 0.445085]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249975][G eval loss: 0.481009]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249964][G train loss: 0.445733]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249956][G eval loss: 0.480867]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249939][G train loss: 0.445484]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 0.480650]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249892][G train loss: 0.445120]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249859][G eval loss: 0.480244]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249819][G train loss: 0.444574]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249778][G eval loss: 0.479373]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249720][G train loss: 0.443617]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249668][G eval loss: 0.477643]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249584][G train loss: 0.441893]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249514][G eval loss: 0.475740]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249393][G train loss: 0.440076]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249296][G eval loss: 0.474470]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249117][G train loss: 0.438909]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249001][G eval loss: 0.473882]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248735][G train loss: 0.438382]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248563][G eval loss: 0.473972]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248177][G train loss: 0.438466]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247989][G eval loss: 0.474809]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247449][G train loss: 0.439249]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247267][G eval loss: 0.476247]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246531][G train loss: 0.440633]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246484][G eval loss: 0.477967]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245493][G train loss: 0.442316]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245293][G eval loss: 0.479648]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.243986][G train loss: 0.443958]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243755][G eval loss: 0.481246]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.242044][G train loss: 0.445559]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241895][G eval loss: 0.482793]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239711][G train loss: 0.447126]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.239577][G eval loss: 0.484614]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236830][G train loss: 0.448976]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.236819][G eval loss: 0.486549]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.233441][G train loss: 0.450943]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.233548][G eval loss: 0.488626]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.229474][G train loss: 0.453045]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.229583][G eval loss: 0.490961]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.224803][G train loss: 0.455375]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.224868][G eval loss: 0.493903]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.219322][G train loss: 0.458322]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.219419][G eval loss: 0.497231]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.213135][G train loss: 0.461670]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.212997][G eval loss: 0.501296]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.206034][G train loss: 0.465727]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.205648][G eval loss: 0.506074]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.198009][G train loss: 0.470462]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.197457][G eval loss: 0.511370]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.189253][G train loss: 0.475641]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.189266][G eval loss: 0.516114]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.180480][G train loss: 0.480083]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.185721][G eval loss: 0.511757]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.176821][G train loss: 0.474237]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.208441][G eval loss: 0.467015]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.206631][G train loss: 0.415230]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.243572][G eval loss: 0.420711]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.248228][G train loss: 0.361252]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.275235][G eval loss: 0.383985]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.283300][G train loss: 0.319280]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.311686][G eval loss: 0.328774]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.308891][G train loss: 0.282800]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.339386][G eval loss: 0.299544]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.328765][G train loss: 0.260052]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.349028][G eval loss: 0.303578]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.335946][G train loss: 0.265812]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.353426][G eval loss: 0.313568]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.339267][G train loss: 0.274417]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.355461][G eval loss: 0.316910]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.340963][G train loss: 0.275315]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.355913][G eval loss: 0.310692]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.341645][G train loss: 0.266869]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.355057][G eval loss: 0.298515]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.341661][G train loss: 0.252984]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.352793][G eval loss: 0.285006]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.341529][G train loss: 0.238277]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.348299][G eval loss: 0.275730]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.341390][G train loss: 0.228002]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.343332][G eval loss: 0.269700]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.339615][G train loss: 0.220529]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.338479][G eval loss: 0.262230]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.335719][G train loss: 0.212114]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.333618][G eval loss: 0.253367]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.331378][G train loss: 0.204384]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.328816][G eval loss: 0.244747]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.325975][G train loss: 0.200809]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.324034][G eval loss: 0.239174]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.320055][G train loss: 0.202410]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.318905][G eval loss: 0.237937]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.311143][G train loss: 0.211066]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.312801][G eval loss: 0.241890]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.308191][G train loss: 0.212265]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.304547][G eval loss: 0.249001]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.302272][G train loss: 0.216767]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.295496][G eval loss: 0.258607]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.298040][G train loss: 0.220149]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.287256][G eval loss: 0.269184]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.294333][G train loss: 0.222483]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.281281][G eval loss: 0.276962]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.005 lr_d=0.001 -> score=0.558243\n",
      "\n",
      "----- 2330: lr_g=0.005, lr_d=0.0012 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0012\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250030][G eval loss: 0.484677]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250028][G train loss: 0.448985]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.479410]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 0.443878]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.481058]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249969][G train loss: 0.445755]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.482037]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249956][G train loss: 0.446762]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249941][G eval loss: 0.481828]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249917][G train loss: 0.446445]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249880][G eval loss: 0.481366]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249840][G train loss: 0.445836]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249788][G eval loss: 0.480790]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249728][G train loss: 0.445121]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249653][G eval loss: 0.480314]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249561][G train loss: 0.444559]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249471][G eval loss: 0.478932]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249334][G train loss: 0.443184]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249216][G eval loss: 0.477219]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249009][G train loss: 0.441557]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248840][G eval loss: 0.475965]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248518][G train loss: 0.440406]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248274][G eval loss: 0.475351]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247798][G train loss: 0.439854]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.247529][G eval loss: 0.475349]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.246845][G train loss: 0.439850]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.246461][G eval loss: 0.475731]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.245510][G train loss: 0.440179]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.244977][G eval loss: 0.476677]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.243678][G train loss: 0.441069]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.243112][G eval loss: 0.478627]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.005 lr_d=0.0012 -> score=0.721739\n",
      "\n",
      "----- 2330: lr_g=0.005, lr_d=0.0015 -----\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0015\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.481051]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250054][G eval loss: 0.482899]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250052][G train loss: 0.447207]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.478983]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249989][G train loss: 0.443450]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249977][G eval loss: 0.481909]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249968][G train loss: 0.446606]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249995][G eval loss: 0.483425]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249984][G train loss: 0.448149]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249973][G eval loss: 0.483173]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249954][G train loss: 0.447790]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249912][G eval loss: 0.482945]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249875][G train loss: 0.447415]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249820][G eval loss: 0.482503]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249760][G train loss: 0.446833]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249665][G eval loss: 0.481253]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249565][G train loss: 0.445497]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249442][G eval loss: 0.479091]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249279][G train loss: 0.443343]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249109][G eval loss: 0.476861]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.248849][G train loss: 0.441199]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.248594][G eval loss: 0.475489]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.248181][G train loss: 0.439932]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.247791][G eval loss: 0.475317]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.247140][G train loss: 0.439825]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.246549][G eval loss: 0.476299]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.245525][G train loss: 0.440810]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.244617][G eval loss: 0.478692]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.243033][G train loss: 0.443154]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.242282][G eval loss: 0.481999]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.239874][G train loss: 0.446412]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.239216][G eval loss: 0.486303]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.235731][G train loss: 0.450663]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.235417][G eval loss: 0.491327]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.230519][G train loss: 0.455653]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.230880][G eval loss: 0.496531]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.224310][G train loss: 0.460860]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.224891][G eval loss: 0.501869]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.216753][G train loss: 0.466208]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.216786][G eval loss: 0.508042]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.207549][G train loss: 0.472395]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.206378][G eval loss: 0.515999]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.196435][G train loss: 0.480387]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.194045][G eval loss: 0.525583]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.183523][G train loss: 0.489998]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.179409][G eval loss: 0.537450]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.168846][G train loss: 0.501829]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.162520][G eval loss: 0.553305]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.151953][G train loss: 0.517450]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.145711][G eval loss: 0.569754]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.135585][G train loss: 0.533238]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.133824][G eval loss: 0.574418]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.126777][G train loss: 0.534898]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.208558][G eval loss: 0.440154]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.227024][G train loss: 0.364232]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.288189][G eval loss: 0.356146]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.309645][G train loss: 0.289544]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.344979][G eval loss: 0.298120]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.344955][G train loss: 0.259373]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.356045][G eval loss: 0.282529]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.353888][G train loss: 0.244295]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.360960][G eval loss: 0.272866]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.358890][G train loss: 0.234845]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.363212][G eval loss: 0.276581]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.361538][G train loss: 0.236867]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.363720][G eval loss: 0.280168]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.362477][G train loss: 0.237759]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.362898][G eval loss: 0.275543]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.362810][G train loss: 0.231046]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.361007][G eval loss: 0.264715]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.365168][G train loss: 0.219130]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.358322][G eval loss: 0.252966]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.363979][G train loss: 0.206607]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.354884][G eval loss: 0.244830]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.359782][G train loss: 0.197861]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.350714][G eval loss: 0.239862]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.353276][G train loss: 0.192382]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.346010][G eval loss: 0.233903]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.345858][G train loss: 0.187334]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.340548][G eval loss: 0.228356]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.340153][G train loss: 0.184962]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.334449][G eval loss: 0.225462]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.333670][G train loss: 0.186282]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.327712][G eval loss: 0.227177]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.326265][G train loss: 0.190926]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.320568][G eval loss: 0.236696]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.318884][G train loss: 0.197436]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.290933][G eval loss: 0.305552]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.311285][G train loss: 0.202869]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.284246][G eval loss: 0.312074]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1.pth\n",
      "Done training LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "[INFO] 2330 lr_g=0.005 lr_d=0.0015 -> score=0.596320\n",
      "\n",
      ">>> Best config for 2330: lr_g=0.0045, lr_d=0.0012, score=0.542064\n",
      "\n",
      "========== Grid search finished ==========\n",
      "0050: best lr_g=0.004, lr_d=0.0008, score=0.700129\n",
      "0056: best lr_g=0.005, lr_d=0.0008, score=0.697335\n",
      "2330: best lr_g=0.0045, lr_d=0.0012, score=0.542064\n"
     ]
    }
   ],
   "source": [
    "from LOB_GAN_training_raw import train_lob_gan\n",
    "\n",
    "# \n",
    "stock_list = [\"0050\", \"0056\", \"2330\"]\n",
    "\n",
    "# \n",
    "lr_g_candidates = [0.003 , 0.0035, 0.0040, 0.0045, 0.005]\n",
    "lr_d_candidates = [0.0005,0.0008, 0.0010, 0.0012,0.0015]\n",
    "\n",
    "batch_size = 50\n",
    "seed = 307\n",
    "\n",
    "#  (stock, lr_g, lr_d) -> dict(...)\n",
    "all_results = {}\n",
    "\n",
    "# \n",
    "best_config = {}\n",
    "best_score = {}\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"\\n========== Grid search for {stock} ==========\")\n",
    "    best_config[stock] = None\n",
    "    best_score[stock] = float(\"inf\")\n",
    "\n",
    "    for lr_g in lr_g_candidates:\n",
    "        for lr_d in lr_d_candidates:\n",
    "            print(f\"\\n----- {stock}: lr_g={lr_g}, lr_d={lr_d} -----\")\n",
    "            res = train_lob_gan(\n",
    "                stock=stock,\n",
    "                lr_g=lr_g,\n",
    "                lr_d=lr_d,\n",
    "                batch_size=batch_size,\n",
    "                seed=seed,\n",
    "            )\n",
    "\n",
    "            #  265  None\n",
    "            if res is None:\n",
    "                print(f\"[WARN] {stock} lr_g={lr_g} lr_d={lr_d}  None\")\n",
    "                continue\n",
    "\n",
    "            #  eval loss \n",
    "            eval_g = res[\"eval_g_loss\"]\n",
    "            eval_d = res[\"eval_d_loss\"]\n",
    "            if len(eval_g) == 0 or len(eval_d) == 0:\n",
    "                print(f\"[WARN] {stock} lr_g={lr_g} lr_d={lr_d}  eval loss\")\n",
    "                continue\n",
    "\n",
    "            score = eval_g[-1] + eval_d[-1]\n",
    "            print(f\"[INFO] {stock} lr_g={lr_g} lr_d={lr_d} -> score={score:.6f}\")\n",
    "\n",
    "            all_results[(stock, lr_g, lr_d)] = {\n",
    "                \"res\": res,\n",
    "                \"score\": score,\n",
    "            }\n",
    "\n",
    "            # \n",
    "            if score < best_score[stock]:\n",
    "                best_score[stock] = score\n",
    "                best_config[stock] = (lr_g, lr_d)\n",
    "\n",
    "    print(f\"\\n>>> Best config for {stock}: \"\n",
    "          f\"lr_g={best_config[stock][0]}, lr_d={best_config[stock][1]}, \"\n",
    "          f\"score={best_score[stock]:.6f}\")\n",
    "\n",
    "print(\"\\n========== Grid search finished ==========\")\n",
    "for stock in stock_list:\n",
    "    if best_config[stock] is not None:\n",
    "        print(f\"{stock}: best lr_g={best_config[stock][0]}, \"\n",
    "              f\"lr_d={best_config[stock][1]}, score={best_score[stock]:.6f}\")\n",
    "    else:\n",
    "        print(f\"{stock}: no valid config found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50415595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training ADJUSTED GAN for 0050 =====\n",
      "Raw data loading and processing 0050\n",
      "Data 1 for 0050 loaded.\n",
      "Data 2 for 0050 loaded.\n",
      "Data 3 for 0050 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0050: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.297082]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250009][G eval loss: 1.118483]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.270163]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249997][G eval loss: 1.108594]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.260239]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249986][G eval loss: 1.108031]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249982][G train loss: 1.259625]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249979][G eval loss: 1.107663]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249973][G train loss: 1.259283]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249967][G eval loss: 1.106205]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249957][G train loss: 1.257858]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249945][G eval loss: 1.105429]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249931][G train loss: 1.257097]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249918][G eval loss: 1.105595]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249897][G train loss: 1.257266]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249881][G eval loss: 1.105764]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249853][G train loss: 1.257418]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249828][G eval loss: 1.105285]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249790][G train loss: 1.256906]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249755][G eval loss: 1.104132]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249703][G train loss: 1.255709]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249655][G eval loss: 1.102789]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249586][G train loss: 1.254317]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249519][G eval loss: 1.101652]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249425][G train loss: 1.253123]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249342][G eval loss: 1.100688]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249220][G train loss: 1.252100]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249104][G eval loss: 1.100070]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248947][G train loss: 1.251417]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248789][G eval loss: 1.099955]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248589][G train loss: 1.251228]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248377][G eval loss: 1.100421]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.248125][G train loss: 1.251626]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247867][G eval loss: 1.100872]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247550][G train loss: 1.252012]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247307][G eval loss: 1.100510]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246920][G train loss: 1.251514]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246465][G eval loss: 1.098973]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245998][G train loss: 1.249774]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245433][G eval loss: 1.096361]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244867][G train loss: 1.246884]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244225][G eval loss: 1.092349]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243558][G train loss: 1.242497]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242773][G eval loss: 1.086392]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242006][G train loss: 1.236056]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241052][G eval loss: 1.077646]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240170][G train loss: 1.226677]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.239045][G eval loss: 1.065128]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.238014][G train loss: 1.213254]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.236814][G eval loss: 1.047958]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.235562][G train loss: 1.194925]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.234458][G eval loss: 1.025588]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.232879][G train loss: 1.171072]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.231960][G eval loss: 0.996933]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.230013][G train loss: 1.140806]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.229413][G eval loss: 0.961122]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.226988][G train loss: 1.103639]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.227096][G eval loss: 0.918504]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.224001][G train loss: 1.059771]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.225587][G eval loss: 0.871323]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.221428][G train loss: 1.010458]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.226078][G eval loss: 0.821702]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.219976][G train loss: 0.957120]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.229495][G eval loss: 0.771464]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.220964][G train loss: 0.899839]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.235891][G eval loss: 0.727185]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.225621][G train loss: 0.842304]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.244287][G eval loss: 0.696898]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.234947][G train loss: 0.791124]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.256125][G eval loss: 0.679493]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.248008][G train loss: 0.753258]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.269728][G eval loss: 0.672261]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.261403][G train loss: 0.730609]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.282324][G eval loss: 0.669270]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.273122][G train loss: 0.715017]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.292606][G eval loss: 0.660643]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.282797][G train loss: 0.697548]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.300592][G eval loss: 0.637728]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.290857][G train loss: 0.674207]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.306993][G eval loss: 0.604636]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.297357][G train loss: 0.643970]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.311947][G eval loss: 0.567230]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.302066][G train loss: 0.612784]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.315593][G eval loss: 0.531245]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.305331][G train loss: 0.587449]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.317739][G eval loss: 0.506741]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.307374][G train loss: 0.575154]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.318056][G eval loss: 0.495672]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.308173][G train loss: 0.572496]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.316369][G eval loss: 0.490602]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.307499][G train loss: 0.574058]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.313640][G eval loss: 0.488337]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.305870][G train loss: 0.573497]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.310144][G eval loss: 0.487382]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.303171][G train loss: 0.569973]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.306285][G eval loss: 0.486348]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.299788][G train loss: 0.562555]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.302282][G eval loss: 0.482679]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.296549][G train loss: 0.554732]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.297764][G eval loss: 0.481197]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.293635][G train loss: 0.549131]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.292555][G eval loss: 0.483055]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.290931][G train loss: 0.547108]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.287708][G eval loss: 0.483770]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.288009][G train loss: 0.545790]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.283412][G eval loss: 0.480921]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.284894][G train loss: 0.545219]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.279523][G eval loss: 0.479344]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.281514][G train loss: 0.545060]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.275876][G eval loss: 0.482349]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.277995][G train loss: 0.546494]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.273022][G eval loss: 0.486523]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_loss_curve_adjusted.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_generator1_adjusted.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0050_discriminator1_adjusted.pth\n",
      "Done training ADJUSTED LOB_GAN for stock 0050\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "===== Training ADJUSTED GAN for 0056 =====\n",
      "Raw data loading and processing 0056\n",
      "Data 1 for 0056 loaded.\n",
      "Data 2 for 0056 loaded.\n",
      "Data 3 for 0056 loaded.\n",
      "Minutely data generated.\n",
      "Start training 0056: epochs=200, batch_size=50, lr_g=0.005, lr_d=0.0008\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250004][G train loss: 1.115726]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250011][G eval loss: 0.982051]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250008][G train loss: 1.084601]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249999][G eval loss: 0.974798]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249995][G train loss: 1.077652]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249988][G eval loss: 0.973797]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249981][G train loss: 1.077052]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249981][G eval loss: 0.973372]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249971][G train loss: 1.076818]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249970][G eval loss: 0.973453]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249955][G train loss: 1.076891]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249949][G eval loss: 0.974351]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249929][G train loss: 1.077670]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249920][G eval loss: 0.974736]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249891][G train loss: 1.077893]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249879][G eval loss: 0.973566]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249841][G train loss: 1.076584]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249826][G eval loss: 0.971647]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249772][G train loss: 1.074571]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249753][G eval loss: 0.969643]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249681][G train loss: 1.072509]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249652][G eval loss: 0.968243]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249556][G train loss: 1.071088]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.249518][G eval loss: 0.967653]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.249388][G train loss: 1.070496]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.249334][G eval loss: 0.967436]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.249164][G train loss: 1.070271]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.249086][G eval loss: 0.967773]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.248866][G train loss: 1.070592]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.248756][G eval loss: 0.968649]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.248475][G train loss: 1.071451]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.248344][G eval loss: 0.969724]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.247991][G train loss: 1.072513]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.247833][G eval loss: 0.970702]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.247392][G train loss: 1.073487]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.247304][G eval loss: 0.971057]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.246757][G train loss: 1.073837]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.246477][G eval loss: 0.970524]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.245835][G train loss: 1.073246]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.245455][G eval loss: 0.969325]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.244770][G train loss: 1.071851]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.244285][G eval loss: 0.967316]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.243581][G train loss: 1.069534]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.242929][G eval loss: 0.963998]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.242249][G train loss: 1.065730]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.241453][G eval loss: 0.957732]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.240896][G train loss: 1.058641]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.240188][G eval loss: 0.945052]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.240057][G train loss: 1.044273]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.239643][G eval loss: 0.924399]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.240515][G train loss: 1.020186]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.239711][G eval loss: 0.901551]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.241636][G train loss: 0.993257]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.239965][G eval loss: 0.889364]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.242484][G train loss: 0.977410]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.240577][G eval loss: 0.890027]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.243608][G train loss: 0.974151]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.240541][G eval loss: 0.887268]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.244217][G train loss: 0.966670]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.239246][G eval loss: 0.872638]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.243573][G train loss: 0.946549]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.237163][G eval loss: 0.846330]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.241855][G train loss: 0.916926]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.234562][G eval loss: 0.819499]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.239653][G train loss: 0.888653]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.232140][G eval loss: 0.795496]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.237647][G train loss: 0.863679]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.230667][G eval loss: 0.765910]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.238020][G train loss: 0.830773]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.233376][G eval loss: 0.723543]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.243011][G train loss: 0.786873]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.240058][G eval loss: 0.678187]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.247225][G train loss: 0.746777]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.247930][G eval loss: 0.647713]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.251300][G train loss: 0.716583]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.252148][G eval loss: 0.629671]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.253240][G train loss: 0.697346]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.254902][G eval loss: 0.607230]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.253238][G train loss: 0.677625]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.255824][G eval loss: 0.575962]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.252019][G train loss: 0.650194]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.255065][G eval loss: 0.536606]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.250136][G train loss: 0.616347]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.252180][G eval loss: 0.502309]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.247723][G train loss: 0.583709]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.248359][G eval loss: 0.478841]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.244856][G train loss: 0.563705]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.246184][G eval loss: 0.467203]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.242609][G train loss: 0.560864]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.248613][G eval loss: 0.462005]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.241408][G train loss: 0.566813]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.251144][G eval loss: 0.464398]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.241168][G train loss: 0.571229]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.253212][G eval loss: 0.462546]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.243112][G train loss: 0.564274]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.259084][G eval loss: 0.448601]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.247055][G train loss: 0.545111]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.262336][G eval loss: 0.437555]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.251119][G train loss: 0.520577]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.271014][G eval loss: 0.415867]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.254177][G train loss: 0.502442]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.273275][G eval loss: 0.422459]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.255841][G train loss: 0.500938]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.273840][G eval loss: 0.434902]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_loss_curve_adjusted.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_generator1_adjusted.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\0056_discriminator1_adjusted.pth\n",
      "Done training ADJUSTED LOB_GAN for stock 0056\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n",
      "===== Training ADJUSTED GAN for 2330 =====\n",
      "Raw data loading and processing 2330\n",
      "Data 1 for 2330 loaded.\n",
      "Data 2 for 2330 loaded.\n",
      "Data 3 for 2330 loaded.\n",
      "Minutely data generated.\n",
      "Start training 2330: epochs=200, batch_size=50, lr_g=0.004, lr_d=0.001\n",
      "[Epoch 1/200][Batch 1/1][D train loss: 0.250005][G train loss: 0.483047]\n",
      "[Epoch 1/200][Batch 1/1][D eval loss: 0.250017][G eval loss: 0.490259]\n",
      "[Epoch 2/200][Batch 1/1][D train loss: 0.250015][G train loss: 0.454633]\n",
      "[Epoch 2/200][Batch 1/1][D eval loss: 0.249994][G eval loss: 0.481824]\n",
      "[Epoch 3/200][Batch 1/1][D train loss: 0.249990][G train loss: 0.446192]\n",
      "[Epoch 3/200][Batch 1/1][D eval loss: 0.249980][G eval loss: 0.481594]\n",
      "[Epoch 4/200][Batch 1/1][D train loss: 0.249974][G train loss: 0.446081]\n",
      "[Epoch 4/200][Batch 1/1][D eval loss: 0.249972][G eval loss: 0.481620]\n",
      "[Epoch 5/200][Batch 1/1][D train loss: 0.249960][G train loss: 0.446275]\n",
      "[Epoch 5/200][Batch 1/1][D eval loss: 0.249951][G eval loss: 0.480866]\n",
      "[Epoch 6/200][Batch 1/1][D train loss: 0.249932][G train loss: 0.445596]\n",
      "[Epoch 6/200][Batch 1/1][D eval loss: 0.249915][G eval loss: 0.480150]\n",
      "[Epoch 7/200][Batch 1/1][D train loss: 0.249885][G train loss: 0.444856]\n",
      "[Epoch 7/200][Batch 1/1][D eval loss: 0.249860][G eval loss: 0.479595]\n",
      "[Epoch 8/200][Batch 1/1][D train loss: 0.249815][G train loss: 0.444210]\n",
      "[Epoch 8/200][Batch 1/1][D eval loss: 0.249780][G eval loss: 0.479392]\n",
      "[Epoch 9/200][Batch 1/1][D train loss: 0.249714][G train loss: 0.443886]\n",
      "[Epoch 9/200][Batch 1/1][D eval loss: 0.249660][G eval loss: 0.478979]\n",
      "[Epoch 10/200][Batch 1/1][D train loss: 0.249564][G train loss: 0.443355]\n",
      "[Epoch 10/200][Batch 1/1][D eval loss: 0.249485][G eval loss: 0.478289]\n",
      "[Epoch 11/200][Batch 1/1][D train loss: 0.249347][G train loss: 0.442574]\n",
      "[Epoch 11/200][Batch 1/1][D eval loss: 0.249241][G eval loss: 0.477352]\n",
      "[Epoch 12/200][Batch 1/1][D train loss: 0.249037][G train loss: 0.441600]\n",
      "[Epoch 12/200][Batch 1/1][D eval loss: 0.248906][G eval loss: 0.476340]\n",
      "[Epoch 13/200][Batch 1/1][D train loss: 0.248611][G train loss: 0.440609]\n",
      "[Epoch 13/200][Batch 1/1][D eval loss: 0.248506][G eval loss: 0.475599]\n",
      "[Epoch 14/200][Batch 1/1][D train loss: 0.248100][G train loss: 0.439931]\n",
      "[Epoch 14/200][Batch 1/1][D eval loss: 0.247970][G eval loss: 0.475526]\n",
      "[Epoch 15/200][Batch 1/1][D train loss: 0.247414][G train loss: 0.439933]\n",
      "[Epoch 15/200][Batch 1/1][D eval loss: 0.247248][G eval loss: 0.476345]\n",
      "[Epoch 16/200][Batch 1/1][D train loss: 0.246497][G train loss: 0.440814]\n",
      "[Epoch 16/200][Batch 1/1][D eval loss: 0.246435][G eval loss: 0.477777]\n",
      "[Epoch 17/200][Batch 1/1][D train loss: 0.245427][G train loss: 0.442285]\n",
      "[Epoch 17/200][Batch 1/1][D eval loss: 0.245278][G eval loss: 0.479037]\n",
      "[Epoch 18/200][Batch 1/1][D train loss: 0.243957][G train loss: 0.443540]\n",
      "[Epoch 18/200][Batch 1/1][D eval loss: 0.243697][G eval loss: 0.480757]\n",
      "[Epoch 19/200][Batch 1/1][D train loss: 0.241976][G train loss: 0.445219]\n",
      "[Epoch 19/200][Batch 1/1][D eval loss: 0.241736][G eval loss: 0.482803]\n",
      "[Epoch 20/200][Batch 1/1][D train loss: 0.239538][G train loss: 0.447210]\n",
      "[Epoch 20/200][Batch 1/1][D eval loss: 0.239269][G eval loss: 0.485101]\n",
      "[Epoch 21/200][Batch 1/1][D train loss: 0.236507][G train loss: 0.449455]\n",
      "[Epoch 21/200][Batch 1/1][D eval loss: 0.236400][G eval loss: 0.487468]\n",
      "[Epoch 22/200][Batch 1/1][D train loss: 0.232968][G train loss: 0.451778]\n",
      "[Epoch 22/200][Batch 1/1][D eval loss: 0.232974][G eval loss: 0.489835]\n",
      "[Epoch 23/200][Batch 1/1][D train loss: 0.228839][G train loss: 0.454112]\n",
      "[Epoch 23/200][Batch 1/1][D eval loss: 0.228846][G eval loss: 0.492326]\n",
      "[Epoch 24/200][Batch 1/1][D train loss: 0.223970][G train loss: 0.456598]\n",
      "[Epoch 24/200][Batch 1/1][D eval loss: 0.223999][G eval loss: 0.495004]\n",
      "[Epoch 25/200][Batch 1/1][D train loss: 0.218367][G train loss: 0.459303]\n",
      "[Epoch 25/200][Batch 1/1][D eval loss: 0.218227][G eval loss: 0.498214]\n",
      "[Epoch 26/200][Batch 1/1][D train loss: 0.211891][G train loss: 0.462557]\n",
      "[Epoch 26/200][Batch 1/1][D eval loss: 0.211379][G eval loss: 0.502236]\n",
      "[Epoch 27/200][Batch 1/1][D train loss: 0.204433][G train loss: 0.466636]\n",
      "[Epoch 27/200][Batch 1/1][D eval loss: 0.203566][G eval loss: 0.507017]\n",
      "[Epoch 28/200][Batch 1/1][D train loss: 0.195974][G train loss: 0.471467]\n",
      "[Epoch 28/200][Batch 1/1][D eval loss: 0.194684][G eval loss: 0.512489]\n",
      "[Epoch 29/200][Batch 1/1][D train loss: 0.186506][G train loss: 0.476973]\n",
      "[Epoch 29/200][Batch 1/1][D eval loss: 0.185121][G eval loss: 0.518362]\n",
      "[Epoch 30/200][Batch 1/1][D train loss: 0.176216][G train loss: 0.482854]\n",
      "[Epoch 30/200][Batch 1/1][D eval loss: 0.174858][G eval loss: 0.524283]\n",
      "[Epoch 31/200][Batch 1/1][D train loss: 0.165520][G train loss: 0.488754]\n",
      "[Epoch 31/200][Batch 1/1][D eval loss: 0.164269][G eval loss: 0.530238]\n",
      "[Epoch 32/200][Batch 1/1][D train loss: 0.155289][G train loss: 0.494603]\n",
      "[Epoch 32/200][Batch 1/1][D eval loss: 0.155540][G eval loss: 0.532837]\n",
      "[Epoch 33/200][Batch 1/1][D train loss: 0.147848][G train loss: 0.496519]\n",
      "[Epoch 33/200][Batch 1/1][D eval loss: 0.161818][G eval loss: 0.506688]\n",
      "[Epoch 34/200][Batch 1/1][D train loss: 0.156904][G train loss: 0.466142]\n",
      "[Epoch 34/200][Batch 1/1][D eval loss: 0.229567][G eval loss: 0.396628]\n",
      "[Epoch 35/200][Batch 1/1][D train loss: 0.235273][G train loss: 0.343430]\n",
      "[Epoch 35/200][Batch 1/1][D eval loss: 0.300560][G eval loss: 0.332686]\n",
      "[Epoch 36/200][Batch 1/1][D train loss: 0.299231][G train loss: 0.293046]\n",
      "[Epoch 36/200][Batch 1/1][D eval loss: 0.337617][G eval loss: 0.318761]\n",
      "[Epoch 37/200][Batch 1/1][D train loss: 0.332837][G train loss: 0.281334]\n",
      "[Epoch 37/200][Batch 1/1][D eval loss: 0.354322][G eval loss: 0.306387]\n",
      "[Epoch 38/200][Batch 1/1][D train loss: 0.348725][G train loss: 0.269750]\n",
      "[Epoch 38/200][Batch 1/1][D eval loss: 0.363422][G eval loss: 0.290903]\n",
      "[Epoch 39/200][Batch 1/1][D train loss: 0.358365][G train loss: 0.254540]\n",
      "[Epoch 39/200][Batch 1/1][D eval loss: 0.368513][G eval loss: 0.278937]\n",
      "[Epoch 40/200][Batch 1/1][D train loss: 0.364452][G train loss: 0.242651]\n",
      "[Epoch 40/200][Batch 1/1][D eval loss: 0.370923][G eval loss: 0.273470]\n",
      "[Epoch 41/200][Batch 1/1][D train loss: 0.367951][G train loss: 0.236304]\n",
      "[Epoch 41/200][Batch 1/1][D eval loss: 0.371561][G eval loss: 0.274851]\n",
      "[Epoch 42/200][Batch 1/1][D train loss: 0.369536][G train loss: 0.235778]\n",
      "[Epoch 42/200][Batch 1/1][D eval loss: 0.371115][G eval loss: 0.277551]\n",
      "[Epoch 43/200][Batch 1/1][D train loss: 0.368921][G train loss: 0.235872]\n",
      "[Epoch 43/200][Batch 1/1][D eval loss: 0.370163][G eval loss: 0.276507]\n",
      "[Epoch 44/200][Batch 1/1][D train loss: 0.368685][G train loss: 0.232582]\n",
      "[Epoch 44/200][Batch 1/1][D eval loss: 0.368793][G eval loss: 0.270486]\n",
      "[Epoch 45/200][Batch 1/1][D train loss: 0.369575][G train loss: 0.225045]\n",
      "[Epoch 45/200][Batch 1/1][D eval loss: 0.366930][G eval loss: 0.259106]\n",
      "[Epoch 46/200][Batch 1/1][D train loss: 0.367291][G train loss: 0.214607]\n",
      "[Epoch 46/200][Batch 1/1][D eval loss: 0.364774][G eval loss: 0.253210]\n",
      "[Epoch 47/200][Batch 1/1][D train loss: 0.366939][G train loss: 0.205712]\n",
      "[Epoch 47/200][Batch 1/1][D eval loss: 0.362152][G eval loss: 0.245640]\n",
      "[Epoch 48/200][Batch 1/1][D train loss: 0.364781][G train loss: 0.195822]\n",
      "[Epoch 48/200][Batch 1/1][D eval loss: 0.359127][G eval loss: 0.237129]\n",
      "[Epoch 49/200][Batch 1/1][D train loss: 0.361932][G train loss: 0.185719]\n",
      "[Epoch 49/200][Batch 1/1][D eval loss: 0.355723][G eval loss: 0.228763]\n",
      "[Epoch 50/200][Batch 1/1][D train loss: 0.358597][G train loss: 0.177123]\n",
      "[Epoch 50/200][Batch 1/1][D eval loss: 0.351995][G eval loss: 0.222006]\n",
      "[Epoch 51/200][Batch 1/1][D train loss: 0.354748][G train loss: 0.171885]\n",
      "[Epoch 51/200][Batch 1/1][D eval loss: 0.347966][G eval loss: 0.217490]\n",
      "[Epoch 52/200][Batch 1/1][D train loss: 0.349848][G train loss: 0.170378]\n",
      "[Epoch 52/200][Batch 1/1][D eval loss: 0.343733][G eval loss: 0.214763]\n",
      "[Epoch 53/200][Batch 1/1][D train loss: 0.343308][G train loss: 0.170556]\n",
      "[Epoch 53/200][Batch 1/1][D eval loss: 0.339177][G eval loss: 0.213821]\n",
      "[Epoch 54/200][Batch 1/1][D train loss: 0.338507][G train loss: 0.172320]\n",
      "[Epoch 54/200][Batch 1/1][D eval loss: 0.334255][G eval loss: 0.214572]\n",
      "[Epoch 55/200][Batch 1/1][D train loss: 0.333560][G train loss: 0.175035]\n",
      "[Epoch 55/200][Batch 1/1][D eval loss: 0.329170][G eval loss: 0.216597]\n",
      "[Epoch 56/200][Batch 1/1][D train loss: 0.328369][G train loss: 0.178294]\n",
      "[Epoch 56/200][Batch 1/1][D eval loss: 0.323968][G eval loss: 0.219828]\n",
      "[Epoch 57/200][Batch 1/1][D train loss: 0.322977][G train loss: 0.180683]\n",
      "[Epoch 57/200][Batch 1/1][D eval loss: 0.318613][G eval loss: 0.223165]\n",
      "[Epoch 58/200][Batch 1/1][D train loss: 0.317366][G train loss: 0.182774]\n",
      "[Epoch 58/200][Batch 1/1][D eval loss: 0.313127][G eval loss: 0.224998]\n",
      "[Epoch 59/200][Batch 1/1][D train loss: 0.311530][G train loss: 0.186030]\n",
      "[Epoch 59/200][Batch 1/1][D eval loss: 0.307567][G eval loss: 0.228738]\n",
      "[Epoch 60/200][Batch 1/1][D train loss: 0.305663][G train loss: 0.190571]\n",
      "[Epoch 60/200][Batch 1/1][D eval loss: 0.301984][G eval loss: 0.233860]\n",
      "[Epoch 61/200][Batch 1/1][D train loss: 0.300078][G train loss: 0.196229]\n",
      "[Epoch 61/200][Batch 1/1][D eval loss: 0.296539][G eval loss: 0.240247]\n",
      "[Epoch 62/200][Batch 1/1][D train loss: 0.294755][G train loss: 0.202592]\n",
      "[Epoch 62/200][Batch 1/1][D eval loss: 0.291194][G eval loss: 0.247724]\n",
      "[Epoch 63/200][Batch 1/1][D train loss: 0.289815][G train loss: 0.209302]\n",
      "[Epoch 63/200][Batch 1/1][D eval loss: 0.285984][G eval loss: 0.256051]\n",
      "Early stopping.\n",
      "Saved loss curve to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_loss_curve_adjusted.png\n",
      "Saved models to c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_generator1_adjusted.pth and c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\\2330_discriminator1_adjusted.pth\n",
      "Done training ADJUSTED LOB_GAN for stock 2330\n",
      "Results saved to: c:\\Users\\Lenovo\\Desktop\\MMM_HW4\\Results\n"
     ]
    }
   ],
   "source": [
    "from LOB_GAN_training_adjusted import train_lob_gan\n",
    "\n",
    "stock_list = [\"0050\", \"0056\", \"2330\"]\n",
    "\n",
    "lr_config = {\n",
    "    \"0050\": {\"lr_g\": 0.004, \"lr_d\": 0.0008},\n",
    "    \"0056\": {\"lr_g\": 0.005, \"lr_d\": 0.0008},\n",
    "    \"2330\": {\"lr_g\": 0.004, \"lr_d\": 0.001},\n",
    "}\n",
    "\n",
    "\n",
    "batch_config = {\n",
    "    \"0050\": 50,\n",
    "    \"0056\": 50,\n",
    "    \"2330\": 50,\n",
    "}\n",
    "\n",
    "results_adjusted = {}\n",
    "\n",
    "for stock in stock_list:\n",
    "    print(f\"===== Training ADJUSTED GAN for {stock} =====\")\n",
    "    cfg = lr_config[stock]\n",
    "    bs = batch_config[stock]\n",
    "    res = train_lob_gan(\n",
    "        stock=stock,\n",
    "        lr_g=cfg[\"lr_g\"],\n",
    "        lr_d=cfg[\"lr_d\"],\n",
    "        batch_size=bs,\n",
    "        seed=307,\n",
    "    )\n",
    "    results_adjusted[stock] = res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616cab6",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
